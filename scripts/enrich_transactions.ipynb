{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Azure OpenAI (chat) not fully set; AI summaries will fall back to deterministic base.\n",
      "⚠️ Azure OpenAI (embeddings) not set; embeddings cache will be skipped.\n",
      "✅ Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Robust setup + centralized Azure client factory ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure OpenAI SDK is available (Azure OpenAI compatible)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# --- Paths (prefer GITHUB_WORKSPACE, never walk above repo) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW       = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR     = REPO / \"config\"\n",
    "STATE_DIR      = REPO / \".state\"\n",
    "VECTOR_DIR     = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH  = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH    = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH  = DATA_RAW / \"latest.csv\"                # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH        = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH          = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH    = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# Ensure dirs\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config flags\n",
    "MAP_ALL        = True        # label unmapped merchants via Azure (if enabled)\n",
    "GOAL_SAVINGS   = 1000.0      # monthly savings target for nudges\n",
    "ANOMALY_Z      = 2.5         # z-score threshold for anomalies\n",
    "\n",
    "# --- Azure OpenAI env ---\n",
    "AZURE_OPENAI_ENDPOINT   = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "AZURE_OPENAI_API_KEY    = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\")   # chat model (deployment name)\n",
    "AZURE_OPENAI_EMBEDDINGS = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")   # embeddings deployment name\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "def _have_azure(deploy: str) -> bool:\n",
    "    return bool(AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and deploy)\n",
    "\n",
    "def make_azure_client(deployment: str) -> OpenAI | None:\n",
    "    \"\"\"Factory for Azure OpenAI client bound to a specific deployment.\"\"\"\n",
    "    if not _have_azure(deployment):\n",
    "        return None\n",
    "    # For Azure, base_url points at the deployment; api-version goes on every request\n",
    "    return OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{deployment}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "\n",
    "# Shared clients (None if not configured)\n",
    "chat_client  = make_azure_client(AZURE_OPENAI_DEPLOYMENT) if AZURE_OPENAI_DEPLOYMENT else None\n",
    "embed_client = make_azure_client(AZURE_OPENAI_EMBEDDINGS) if AZURE_OPENAI_EMBEDDINGS else None\n",
    "azure_enabled = chat_client is not None\n",
    "\n",
    "if not azure_enabled:\n",
    "    print(\"⚠️ Azure OpenAI (chat) not fully set; AI summaries will fall back to deterministic base.\")\n",
    "if embed_client is None:\n",
    "    print(\"⚠️ Azure OpenAI (embeddings) not set; embeddings cache will be skipped.\")\n",
    "\n",
    "print(\"✅ Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b64fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 148 transactions. expenses_are_negative=False\n"
     ]
    }
   ],
   "source": [
    "# Load latest.csv (from build_latest.ipynb), robust path resolution\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "# Global sign convention: True if expenses are negative numbers\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "print(f\"Loaded {len(df)} transactions. expenses_are_negative={EXPENSES_ARE_NEGATIVE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1397b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized (consistent with build_latest).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Normalize merchant_key consistently with build_latest ---\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    s = (name or \"\").upper()\n",
    "    s = re.sub(r\"APPLE PAY ENDING IN \\d{4}\", \"\", s)\n",
    "    s = re.sub(r\"#\\d{2,}\", \"\", s)              # strip store numbers like #1234\n",
    "    s = re.sub(r\"\\d+\", \"\", s)                  # kill stray digits\n",
    "    s = re.sub(r\"[^A-Z&\\s]\", \" \", s)           # keep letters, ampersand, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or \"UNKNOWN\"\n",
    "\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    df[\"merchant_name\"].map(merchant_key_from),\n",
    "    df[\"name\"].map(merchant_key_from)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized (consistent with build_latest).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize merchant dimension table\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db494c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE-MERCHANT LABELING (robust)\n",
    "import re, json, ast\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a financial data labeling assistant.\\n\"\n",
    "    \"Given ONE merchant_key, output a single JSON object with fields:\\n\"\n",
    "    \"merchant_key (echo EXACTLY), display_name (string), category (string), subcategory (string), tags (array of 1-5 short strings).\\n\"\n",
    "    \"Categories: Dining, Groceries, Gas, Utilities, Subscriptions, Shopping, Travel, Health, Entertainment, Education, Income, Transfers, Fees, Misc.\\n\"\n",
    "    \"display_name should be human-friendly (e.g., 'ARCO', 'APPLEBEE'S').\\n\"\n",
    "    \"Return ONLY JSON. No code fences, no commentary.\"\n",
    ")\n",
    "\n",
    "def _salvage_json_object(txt: str):\n",
    "    \"\"\"Try hard to recover a single JSON object from a messy string.\"\"\"\n",
    "    t = txt.strip()\n",
    "    # strip code fences if present\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    # direct parse\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # find largest {...} block\n",
    "    start = t.find(\"{\")\n",
    "    end = t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = t[start:end+1]\n",
    "        try:\n",
    "            obj = json.loads(candidate)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last resort: python-ish literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise RuntimeError(f\"Failed to parse single-object JSON:\\n{t[:400]}\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=12))\n",
    "def azure_label_one(mk: str):\n",
    "    \"\"\"Label exactly one merchant_key with strict JSON, resilient to noise.\"\"\"\n",
    "    if client is None:\n",
    "        return None\n",
    "    user = (\n",
    "        \"Label this merchant_key and return ONLY a single JSON object:\\n\"\n",
    "        '{ \"merchant_key\": \"...\", \"display_name\":\"...\", \"category\":\"...\", \"subcategory\":\"...\", \"tags\":[...] }\\n\\n'\n",
    "        f'merchant_key: \"{mk}\"'\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[{\"role\":\"system\",\"content\": SYSTEM}, {\"role\":\"user\",\"content\": user}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        response_format={\"type\": \"json_object\"},   # strongly nudges valid JSON\n",
    "    )\n",
    "    raw = resp.choices[0].message.content\n",
    "    obj = _salvage_json_object(raw)\n",
    "    # Coerce + fill\n",
    "    out = {\n",
    "        \"merchant_key\": mk,  # echo exactly\n",
    "        \"display_name\": str(obj.get(\"display_name\", mk)).upper().strip(),\n",
    "        \"category\": str(obj.get(\"category\", \"\")),\n",
    "        \"subcategory\": str(obj.get(\"subcategory\", \"\")),\n",
    "        \"tags\": obj.get(\"tags\", []),\n",
    "    }\n",
    "    # normalize tags into CSV (safe)\n",
    "    if not isinstance(out[\"tags\"], list):\n",
    "        out[\"tags\"] = []\n",
    "    out[\"tags\"] = [str(t).strip() for t in out[\"tags\"] if str(t).strip()]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b712b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new mappings needed or AI disabled.\n"
     ]
    }
   ],
   "source": [
    "new_rows = []\n",
    "if len(unmapped_keys) and client is not None and MAP_ALL:\n",
    "    print(f\"Labeling {len(unmapped_keys)} merchants (single-call mode)...\")\n",
    "    for idx, mk in enumerate(unmapped_keys, 1):\n",
    "        try:\n",
    "            item = azure_label_one(mk)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Label fail for '{mk}': {e}\")\n",
    "            continue\n",
    "\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if item:\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": item[\"display_name\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"subcategory\": item[\"subcategory\"],\n",
    "                \"tags\": \",\".join(item[\"tags\"]),\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        dim_new = pd.DataFrame(new_rows)\n",
    "        dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "        dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "        MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        dim = dim_all\n",
    "        print(f\"✅ Added {len(new_rows)} merchant mappings (single-call).\")\n",
    "    else:\n",
    "        print(\"No new mappings added (single-call).\")\n",
    "else:\n",
    "    print(\"No new mappings needed or AI disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aef6c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 merchants_dim.csv saved (64 rows) → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\merchants_dim.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6B: Persist merchants_dim.csv (idempotent) ---\n",
    "\n",
    "# Toggle if you ever want to skip writing on runs with no changes\n",
    "PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "# dim_cols defined in Cell 4; dim may be updated in Cell 6\n",
    "if not isinstance(PERSIST_MERCHANT_DIM, bool):\n",
    "    PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "if PERSIST_MERCHANT_DIM:\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'dim' in globals() and isinstance(dim, pd.DataFrame) and len(dim):\n",
    "        # ensure expected columns/order exist before save\n",
    "        for c in dim_cols:\n",
    "            if c not in dim.columns:\n",
    "                dim[c] = np.nan\n",
    "        dim = dim[dim_cols]\n",
    "\n",
    "        dim.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        print(f\"📝 merchants_dim.csv saved ({len(dim)} rows) → {MERCHANT_DIM_PATH}\")\n",
    "    else:\n",
    "        # either no new mappings this run or dim was empty; ensure file exists\n",
    "        if not MERCHANT_DIM_PATH.exists():\n",
    "            pd.DataFrame(columns=dim_cols).to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            print(f\"📝 Created headers-only merchants_dim.csv → {MERCHANT_DIM_PATH}\")\n",
    "        else:\n",
    "            print(\"ℹ️ merchants_dim.csv already exists; no changes to sync.\")\n",
    "else:\n",
    "    print(\"PERSIST_MERCHANT_DIM=False → skipping merchants_dim.csv persistence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6eba14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels joined.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"], errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Final output columns (feel free to adjust ordering)\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory\", \"tags\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "# Ensure existence even if null\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Canonical display name fallback\n",
    "df[\"display_name\"] = df[\"display_name\"].fillna(df[\"merchant_key\"])\n",
    "\n",
    "print(\"Labels joined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be3dff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_25252\\4184111192.py:47: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Subscription detection (sign-aware, idempotent) ---\n",
    "\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # use absolute spend magnitudes for stability\n",
    "    amounts = g[\"amount\"].abs().to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # gaps in days\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9)) if mu > 0 else 1.0\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Clean any leftover artifacts from previous runs (e.g., is_subscription_x from merges)\n",
    "for col in [c for c in df.columns if c.startswith(\"is_subscription\") and c != \"is_subscription\"]:\n",
    "    df.drop(columns=col, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Respect your sign convention\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "if EXPENSES_ARE_NEGATIVE:\n",
    "    outflows = df.loc[(df[\"amount\"] < 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "    outflows[\"amount\"] = outflows[\"amount\"].abs()\n",
    "else:\n",
    "    outflows = df.loc[(df[\"amount\"] > 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "\n",
    "subs_map = {}\n",
    "for disp, g in outflows.groupby(\"display_name\", dropna=False):\n",
    "    try:\n",
    "        subs_map[disp] = bool(detect_subscription(g[[\"date\", \"amount\"]]))\n",
    "    except Exception:\n",
    "        subs_map[disp] = False\n",
    "\n",
    "df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e514f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 1\n"
     ]
    }
   ],
   "source": [
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6675fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,542.88 (+2,059.58 vs prior)\n",
      "Top 3 merchants: WITHDRAWAL ALLY TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ACH ECC WEB ACH TRACE ($1,494.22), WITHDRAWAL AMEX EPAYMENT TYPE ACH PMT ID DATA ER AM CO AMEX EPAYMENT NAME KOSISONNA UGOCHUKWU ACH ECC WEB ACH TRACE ($777.78), PETAL ($738.96)\n",
      "Biggest category driver: nan ($2,868.17)\n",
      "\n",
      "Saved digest → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest → {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f17a1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut nan by 35%\n",
      "\n",
      "Saved goal nudges → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Suggest % cuts in top categories to reach GOAL_SAVINGS over next 30 days\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges → {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d67e86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new embeddings added (none missing or embeddings disabled).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 12: Build text and cache embeddings (reuse shared embed_client) ---\n",
    "def build_search_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"display_name\") or \"\"),\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        str(row.get(\"category\") or \"\"),\n",
    "        str(row.get(\"subcategory\") or \"\"),\n",
    "        str(row.get(\"tags\") or \"\"),\n",
    "    ]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Limit to recent rows for cost control\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df[~embed_df[\"txn_uid\"].isin(existing)][[\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "def get_embeddings(texts: list[str]) -> list | None:\n",
    "    if embed_client is None:\n",
    "        return None\n",
    "    # The model name is the deployment name on Azure\n",
    "    res = embed_client.embeddings.create(model=AZURE_OPENAI_EMBEDDINGS, input=list(texts))\n",
    "    # Return raw vectors (list[float]) as provided\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed) and embed_client is not None:\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        if vecs is None:\n",
    "            break\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} → total {len(merged)}\")\n",
    "else:\n",
    "    print(\"No new embeddings added (none missing or embeddings disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7420962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched CSV saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "📄 Copy saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Reorder and save\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    \"category\",\"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\"is_subscription\",\"is_anomaly\",\"z_by_merchant\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Enriched CSV saved → {ENRICHED_OUT_PATH}\")\n",
    "print(f\"📄 Copy saved → {ENRICHED_COPY_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9d447e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Weekly executive digest written (WoW):\n",
      "- JSON: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.json\n",
      "- MD:   C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.md\n",
      "- CSV:  C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_flat.csv\n",
      "Window: 2025-09-01 → 2025-09-07 | Prev: 2025-08-25 → 2025-08-31\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 14: Weekly Executive Digest (WoW) — Azure overlay + flat CSV for Power BI ---\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIGEST_JSON = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "DIGEST_MD   = INSIGHTS_DIR / \"digest_latest.md\"\n",
    "DIGEST_FLAT = INSIGHTS_DIR / \"digest_latest_flat.csv\"   # <-- PBI-friendly\n",
    "\n",
    "# -------- 1) Last COMPLETED week (Mon–Sun), compare WoW --------\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wd = int(now.weekday())                               # Mon=0 ... Sun=6\n",
    "days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()    # prior Monday\n",
    "prev_end = (pd.Timestamp(wk_end) - pd.Timedelta(days=7)).date()\n",
    "prev_start = (pd.Timestamp(prev_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "df_w = df.copy()\n",
    "df_w[\"date_only\"] = df_w[\"date\"].dt.date\n",
    "cur  = df_w[(df_w[\"date_only\"] >= wk_start) & (df_w[\"date_only\"] <= wk_end)]\n",
    "prev = df_w[(df_w[\"date_only\"] >= prev_start) & (df_w[\"date_only\"] <= prev_end)]\n",
    "\n",
    "# -------- 2) Robust sign detection --------\n",
    "amt_all = df_w[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "def spend_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a < 0].abs().sum()) if expenses_are_negative else float(a[a > 0].sum())\n",
    "\n",
    "def income_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a > 0].sum()) if expenses_are_negative else float(a[a < 0].abs().sum())\n",
    "\n",
    "cur_spend  = round(spend_sum(cur), 2)\n",
    "prev_spend = round(spend_sum(prev), 2)\n",
    "cur_income  = round(income_sum(cur), 2)\n",
    "prev_income = round(income_sum(prev), 2)\n",
    "\n",
    "spend_delta     = round(cur_spend - prev_spend, 2)\n",
    "spend_delta_pct = round((spend_delta / prev_spend), 4) if prev_spend else (1.0 if cur_spend else 0.0)\n",
    "\n",
    "# Top drivers this week based on spend direction\n",
    "if expenses_are_negative:\n",
    "    cur_exp = cur[cur[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp = cur[cur[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "top_merchants_cur = (\n",
    "    cur_exp.groupby(\"display_name\", dropna=False)[\"spend\"].sum()\n",
    "          .sort_values(ascending=False).head(5)\n",
    "          .reset_index()\n",
    ")\n",
    "top_cats_cur = (\n",
    "    cur_exp.groupby(\"category\", dropna=False)[\"spend\"].sum()\n",
    "          .sort_values(ascending=False).head(5)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "subs_w  = cur.loc[cur.get(\"is_subscription\", False) == True]\n",
    "anoms_w = cur.loc[cur.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "summary_payload = {\n",
    "    \"as_of_date\": pd.Timestamp(wk_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(wk_start), \"end\": str(wk_end), \"label\": \"Last completed week (Mon–Sun)\"},\n",
    "        \"previous\": {\"start\": str(prev_start), \"end\": str(prev_end)}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        \"spend_current\": cur_spend,\n",
    "        \"spend_previous\": prev_spend,\n",
    "        \"spend_delta\": spend_delta,\n",
    "        \"spend_delta_pct\": spend_delta_pct,\n",
    "        \"income_current\": cur_income,\n",
    "        \"income_previous\": prev_income,\n",
    "    },\n",
    "    \"top_merchants\": [\n",
    "        {\"display_name\": str(r[\"display_name\"]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_merchants_cur.iterrows()\n",
    "    ],\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[\"category\"]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_cur.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_w[\"display_name\"].nunique()) if len(subs_w) else 0,\n",
    "    \"anomalies_count\": int(anoms_w.shape[0]) if len(anoms_w) else 0,\n",
    "}\n",
    "\n",
    "# -------- 3) Azure summarizer (overlay JSON, never empty) --------\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided aggregates for the last completed week and the previous week, \"\n",
    "    \"produce an executive digest in STRICT JSON. Do not invent numbers. Keep it concise.\"\n",
    ")\n",
    "USER_INSTRUCTIONS = (\n",
    "    \"Compare the current week vs previous week (WoW). \"\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ]\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct for improvements if spend fell.\\n\"\n",
    "    \"- impact_usd is a rough **weekly** savings estimate.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call(payload_json: str) -> str:\n",
    "    assert chat_client is not None\n",
    "    resp = chat_client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,   # deployment name as model\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=600,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Deterministic base digest\n",
    "base_digest = {\n",
    "    \"insights_version\": 2,\n",
    "    \"window\": summary_payload[\"window\"],\n",
    "    \"totals\": summary_payload[\"totals\"],\n",
    "    \"headline\": f\"Weekly digest {wk_start}–{wk_end}\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Spend (week)\",  \"value\": cur_spend,  \"delta_pct\": spend_delta_pct},\n",
    "        {\"name\":\"Income (week)\", \"value\": cur_income, \"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": t[\"category\"], \"spend\": float(t[\"spend\"])} for t in summary_payload[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload['subscriptions_count']} active subs this week\"}] if summary_payload[\"subscriptions_count\"] else [])\n",
    "        + ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload['anomalies_count']} anomalies this week\"}] if summary_payload[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": []\n",
    "}\n",
    "\n",
    "azure_digest = None\n",
    "if chat_client is not None:\n",
    "    try:\n",
    "        raw = _azure_digest_call(json.dumps(summary_payload))\n",
    "        azure_digest = _salvage_json_object(raw)\n",
    "    except Exception:\n",
    "        azure_digest = None\n",
    "\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "digest = _overlay(base_digest, azure_digest)\n",
    "\n",
    "# -------- 4) Persist JSON + Markdown + flat CSV --------\n",
    "with open(DIGEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def render_md(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    lines = [f\"## Weekly Digest: {ws}–{we}\", f\"{d.get('headline','Executive digest')}\"]\n",
    "    km = d.get(\"key_metrics\", [])[:5]\n",
    "    if km:\n",
    "        lines.append(\"\\n**Key metrics (WoW)**\")\n",
    "        for m in km:\n",
    "            dp = m.get(\"delta_pct\", None)\n",
    "            dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "            lines.append(f\"- {m['name']}: ${m['value']:,.2f}{dp_txt}\")\n",
    "    td = d.get(\"top_drivers\", [])[:5]\n",
    "    if td:\n",
    "        lines.append(\"\\n**Top drivers (this week)**\")\n",
    "        for t in td:\n",
    "            label = t.get(\"label\") or t.get(\"display_name\") or \"\"\n",
    "            lines.append(f\"- {label}: ${float(t.get('spend',0)):,.2f}\")\n",
    "    rk = d.get(\"risks\", [])[:5]\n",
    "    if rk:\n",
    "        lines.append(\"\\n**Risks**\")\n",
    "        for r in rk:\n",
    "            lines.append(f\"- {r.get('type','note')}: {r.get('note','')}\")\n",
    "    ai = d.get(\"action_items\", [])[:5]\n",
    "    if ai:\n",
    "        lines.append(\"\\n**Action items**\")\n",
    "        for a in ai:\n",
    "            lines.append(f\"- {a['title']} — est. weekly impact ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(DIGEST_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md(digest))\n",
    "\n",
    "# Flat table for Power BI ingestion (one row per element with type)\n",
    "flat_rows = []\n",
    "\n",
    "# Header row (window + totals)\n",
    "flat_rows.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(wk_end),\n",
    "    \"cur_start\": str(wk_start),\n",
    "    \"cur_end\": str(wk_end),\n",
    "    \"prev_start\": str(prev_start),\n",
    "    \"prev_end\": str(prev_end),\n",
    "    \"headline\": digest.get(\"headline\", \"\"),\n",
    "    \"name\": \"Spend (week)\",\n",
    "    \"value\": cur_spend,\n",
    "    \"delta_pct\": spend_delta_pct,\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "\n",
    "# Key metrics\n",
    "for m in digest.get(\"key_metrics\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Top drivers\n",
    "for t in digest.get(\"top_drivers\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": t.get(\"label\",\"\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Risks\n",
    "for r in digest.get(\"risks\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Action items\n",
    "for a in digest.get(\"action_items\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"action\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": a.get(\"title\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": a.get(\"rationale\",\"\"),\n",
    "        \"impact_usd\": float(a.get(\"impact_usd\",0) or 0.0),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(flat_rows).to_csv(DIGEST_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"🧠 Weekly executive digest written (WoW):\\n\"\n",
    "    f\"- JSON: {DIGEST_JSON}\\n- MD:   {DIGEST_MD}\\n- CSV:  {DIGEST_FLAT}\\n\"\n",
    "    f\"Window: {wk_start} → {wk_end} | Prev: {prev_start} → {prev_end}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
