{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Azure OpenAI env not fully set. AI labeling will be skipped.\n",
      "âœ… Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to ensure OpenAI SDK is available (for Azure OpenAI)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# --- Paths ---\n",
    "REPO = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parents[0]\n",
    "DATA_RAW = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR = REPO / \"config\"\n",
    "STATE_DIR = REPO / \".state\"\n",
    "VECTOR_DIR = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH = DATA_RAW / \"latest.csv\"               # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# --- Ensure dirs ---\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Config flags ---\n",
    "MAP_ALL = True              # map any merchant missing from dimension\n",
    "GOAL_SAVINGS = 1000.0       # target monthly savings for \"goal nudges\"\n",
    "ANOMALY_Z = 2.5             # z-score threshold for anomalies\n",
    "\n",
    "# --- Azure OpenAI env ---\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\")  # chat model\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "if not (AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT):\n",
    "    print(\"âš ï¸ Azure OpenAI env not fully set. AI labeling will be skipped.\")\n",
    "\n",
    "# Build OpenAI (Azure) client if possible\n",
    "client = None\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT:\n",
    "    client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "\n",
    "print(\"âœ… Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b64fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 147 transactions.\n"
     ]
    }
   ],
   "source": [
    "# Load latest.csv (from build_latest.ipynb)\n",
    "df = pd.read_csv(LATEST_CSV_PATH)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "print(f\"Loaded {len(df)} transactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1397b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized.\n"
     ]
    }
   ],
   "source": [
    "# Normalize noisy merchant strings into a stable 'merchant_key'\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "def normalize_merchant_key(txt: str) -> str:\n",
    "    t = (txt or \"\").upper().strip()\n",
    "    # Remove common noise: excessive spaces, digits, #, store ids, etc.\n",
    "    t = re.sub(r\"\\d{2,}\", \"\", t)              # drop long digit runs\n",
    "    t = re.sub(r\"[-_/#*]+\", \" \", t)           # separators -> space\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    # Drop locale suffixes like \"NV\", \"CA\" at the end if present\n",
    "    t = re.sub(r\"\\b([A-Z]{2})\\b$\", \"\", t).strip()\n",
    "    # Collapse APPLE PAY / GOOGLE PAY hints\n",
    "    t = t.replace(\"APPLE PAY\", \"\").replace(\"GOOGLE PAY\", \"\").strip()\n",
    "    # Fallback\n",
    "    return t or \"UNKNOWN\"\n",
    "\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].str.len() > 0,\n",
    "    df[\"merchant_name\"].apply(normalize_merchant_key),\n",
    "    df[\"name\"].apply(normalize_merchant_key)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize merchant dimension table\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db494c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a financial data labeling assistant. \"\n",
    "    \"For each merchant_key, produce concise JSON objects with fields: \"\n",
    "    \"display_name (string), category (string), subcategory (string), tags (list of short strings). \"\n",
    "    \"Use US personal finance categories like Dining, Groceries, Gas, Utilities, Subscriptions, Travel, Health, Shopping, Income, Transfers. \"\n",
    "    \"Keep display_name human-friendly (e.g., 'APPLEBEE'S', 'PANDA EXPRESS'). \"\n",
    "    \"When uncertain, make your best guess.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(merchant_keys):\n",
    "    # Keep prompt compact; model can handle ~50-80 at once easily; weâ€™ll batch anyway.\n",
    "    examples = \"\\n\".join(f'- \"{k}\"' for k in merchant_keys)\n",
    "    return (\n",
    "        \"Label the following merchant keys. Return ONLY a valid JSON array where each item is:\\n\"\n",
    "        \"{merchant_key, display_name, category, subcategory, tags}\\n\\n\"\n",
    "        f\"MERCHANT_KEYS:\\n{examples}\"\n",
    "    )\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=8))\n",
    "def azure_label_batch(keys_batch):\n",
    "    if client is None:\n",
    "        return []\n",
    "    msg = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM},\n",
    "        {\"role\":\"user\", \"content\": build_user_prompt(keys_batch)}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=msg,\n",
    "        temperature=0.2,\n",
    "        max_tokens=1200\n",
    "    )\n",
    "    txt = resp.choices[0].message.content.strip()\n",
    "    # Sometimes models wrap in code fencesâ€”strip them\n",
    "    if txt.startswith(\"```\"):\n",
    "        txt = re.sub(r\"^```(json)?\", \"\", txt, flags=re.IGNORECASE).strip()\n",
    "        txt = re.sub(r\"```$\", \"\", txt).strip()\n",
    "    try:\n",
    "        parsed = json.loads(txt)\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed = [parsed]\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        # Last resort: try to eval if it's accidentally Python-ish\n",
    "        try:\n",
    "            parsed = ast.literal_eval(txt)\n",
    "            if isinstance(parsed, dict):\n",
    "                parsed = [parsed]\n",
    "            return parsed\n",
    "        except Exception:\n",
    "            raise RuntimeError(f\"Failed to parse AI JSON:\\n{txt}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b712b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new mappings added (either none missing or AI disabled).\n"
     ]
    }
   ],
   "source": [
    "new_rows = []\n",
    "if len(unmapped_keys) and client is not None and MAP_ALL:\n",
    "    BATCH = 40\n",
    "    for i in range(0, len(unmapped_keys), BATCH):\n",
    "        batch = unmapped_keys[i:i+BATCH]\n",
    "        labeled = azure_label_batch(batch)\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        for item in labeled:\n",
    "            mk = item.get(\"merchant_key\")\n",
    "            if not mk:\n",
    "                # attempt to align by matching display_name back to the requested mk if missing\n",
    "                # but prefer exact merchant_key if present\n",
    "                continue\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": str(item.get(\"display_name\", mk)).upper().strip(),\n",
    "                \"category\": str(item.get(\"category\", \"\")),\n",
    "                \"subcategory\": str(item.get(\"subcategory\", \"\")),\n",
    "                \"tags\": \",\".join(item.get(\"tags\", []) if isinstance(item.get(\"tags\", []), list) else []),\n",
    "                \"source\": \"ai\",\n",
    "                \"confidence\": 0.85,  # heuristic; could ask model to return confidence later\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "# Append & dedupe (latest wins)\n",
    "if new_rows:\n",
    "    dim_new = pd.DataFrame(new_rows)\n",
    "    dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "    dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "    dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "    dim = dim_all\n",
    "    print(f\"Added {len(new_rows)} new merchant mappings.\")\n",
    "else:\n",
    "    print(\"No new mappings added (either none missing or AI disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eba14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels joined.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"], errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Final output columns (feel free to adjust ordering)\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory\", \"tags\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "# Ensure existence even if null\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Canonical display name fallback\n",
    "df[\"display_name\"] = df[\"display_name\"].fillna(df[\"merchant_key\"])\n",
    "\n",
    "print(\"Labels joined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3dff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_53236\\3844477478.py:37: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(detect_subscription)\n",
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_53236\\3844477478.py:43: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_subscription\"] = df[\"is_subscription\"].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Subscription detection (robust, version-proof) ---\n",
    "\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    # Sort and keep only rows with valid date & amount\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # Expenses only\n",
    "    amounts = g[\"amount\"].to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # Inter-payment gaps in DAYS using int64 ns -> days\n",
    "    # (Avoids .dt on object-dtype and works across pandas versions)\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()  # datetime64[ns] -> int ns\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000  # ns per day\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    # Monthly-ish cadence and amount consistency\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    if mu <= 0:\n",
    "        return False\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9))  # coefficient of variation\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Group by display_name; we already set a fallback to merchant_key so NaNs should not exist.\n",
    "subs_flags = (\n",
    "    df[df[\"amount\"] > 0]\n",
    "      .groupby(\"display_name\")               # <- no dropna kwarg to avoid pandas version issues\n",
    "      .apply(detect_subscription)\n",
    "      .rename(\"is_subscription\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(subs_flags, on=\"display_name\", how=\"left\")\n",
    "df[\"is_subscription\"] = df[\"is_subscription\"].fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e514f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 1\n"
     ]
    }
   ],
   "source": [
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6675fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,746.32 (+2,470.06 vs prior)\n",
      "Top 3 merchants: WITHDRAWAL ALLY TYPE: ALLY PAYMT ID: CO: ALLY NAME: KOSISONNA UGOCHUKW %% ACH ECC WEB %% ACH TRACE ($1,494.22), WITHDRAWAL AMEX EPAYMENT TYPE: ACH PMT ID: DATA: ER AM CO: AMEX EPAYMENT NAME: KOSISONNA UGOCHUKWU %% ACH ECC WEB %% ACH TRACE ($777.78), PETAL ($738.96)\n",
      "Biggest category driver: nan ($5,746.32)\n",
      "\n",
      "Saved digest â†’ c:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest â†’ {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f17a1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut nan by 17%\n",
      "\n",
      "Saved goal nudges â†’ c:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Suggest % cuts in top categories to reach GOAL_SAVINGS over next 30 days\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges â†’ {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67e86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new embeddings added (either none missing or AI disabled).\n"
     ]
    }
   ],
   "source": [
    "# Build text field and store embeddings for semantic search\n",
    "def build_search_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"display_name\") or \"\"),\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        str(row.get(\"category\") or \"\"),\n",
    "        str(row.get(\"subcategory\") or \"\"),\n",
    "        str(row.get(\"tags\") or \"\"),\n",
    "    ]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Prepare rows (limit to recent for cost-control)\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df[~embed_df[\"txn_uid\"].isin(existing)][[\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    if client is None:\n",
    "        return [None for _ in texts]\n",
    "    # Use the Azure embeddings deployment name from env\n",
    "    emb_deploy = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")\n",
    "    if not emb_deploy:\n",
    "        return [None for _ in texts]\n",
    "\n",
    "    # New OpenAI client pattern for embeddings under Azure:\n",
    "    # base_url should be resource; we temporarily create a fresh client pointing to embeddings deployment\n",
    "    emb_client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{emb_deploy}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "    res = emb_client.embeddings.create(model=emb_deploy, input=list(texts))\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed):\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} â†’ total {len(merged)}\")\n",
    "else:\n",
    "    print(\"No new embeddings added (either none missing or AI disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7420962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched CSV saved â†’ c:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "ðŸ“„ Copy saved â†’ c:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Reorder and save\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    \"category\",\"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\"is_subscription\",\"is_anomaly\",\"z_by_merchant\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… Enriched CSV saved â†’ {ENRICHED_OUT_PATH}\")\n",
    "print(f\"ðŸ“„ Copy saved â†’ {ENRICHED_COPY_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
