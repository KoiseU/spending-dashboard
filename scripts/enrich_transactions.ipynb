{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to ensure OpenAI SDK is available (for Azure OpenAI)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# --- Paths (robust: prefer GITHUB_WORKSPACE, never walk above repo) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "# find the repo root by locating the first directory that has a .git\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR = REPO / \"config\"\n",
    "STATE_DIR = REPO / \".state\"\n",
    "VECTOR_DIR = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH   = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH = DATA_RAW / \"latest.csv\"               # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH   = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# --- Ensure dirs ---\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Config flags ---\n",
    "MAP_ALL = True              # map any merchant missing from dimension\n",
    "GOAL_SAVINGS = 1000.0       # target monthly savings for \"goal nudges\"\n",
    "ANOMALY_Z = 2.5             # z-score threshold for anomalies\n",
    "\n",
    "# --- Azure OpenAI env ---\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\")  # chat model\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "if not (AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT):\n",
    "    print(\"âš ï¸ Azure OpenAI env not fully set. AI labeling will be skipped.\")\n",
    "\n",
    "# Build OpenAI (Azure) client if possible\n",
    "client = None\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT:\n",
    "    client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "\n",
    "print(\"âœ… Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b64fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 149 transactions.\n"
     ]
    }
   ],
   "source": [
    "# Load latest.csv (from build_latest.ipynb), robust path resolution\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "print(f\"Loaded {len(df)} transactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1397b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized.\n"
     ]
    }
   ],
   "source": [
    "# Normalize noisy merchant strings into a stable 'merchant_key'\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "def normalize_merchant_key(txt: str) -> str:\n",
    "    t = (txt or \"\").upper().strip()\n",
    "    # Remove common noise: excessive spaces, digits, #, store ids, etc.\n",
    "    t = re.sub(r\"\\d{2,}\", \"\", t)              # drop long digit runs\n",
    "    t = re.sub(r\"[-_/#*]+\", \" \", t)           # separators -> space\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    # Drop locale suffixes like \"NV\", \"CA\" at the end if present\n",
    "    t = re.sub(r\"\\b([A-Z]{2})\\b$\", \"\", t).strip()\n",
    "    # Collapse APPLE PAY / GOOGLE PAY hints\n",
    "    t = t.replace(\"APPLE PAY\", \"\").replace(\"GOOGLE PAY\", \"\").strip()\n",
    "    # Fallback\n",
    "    return t or \"UNKNOWN\"\n",
    "\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].str.len() > 0,\n",
    "    df[\"merchant_name\"].apply(normalize_merchant_key),\n",
    "    df[\"name\"].apply(normalize_merchant_key)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize merchant dimension table\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db494c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE-MERCHANT LABELING (robust)\n",
    "import re, json, ast\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a financial data labeling assistant.\\n\"\n",
    "    \"Given ONE merchant_key, output a single JSON object with fields:\\n\"\n",
    "    \"merchant_key (echo EXACTLY), display_name (string), category (string), subcategory (string), tags (array of 1-5 short strings).\\n\"\n",
    "    \"Categories: Dining, Groceries, Gas, Utilities, Subscriptions, Shopping, Travel, Health, Entertainment, Education, Income, Transfers, Fees, Misc.\\n\"\n",
    "    \"display_name should be human-friendly (e.g., 'ARCO', 'APPLEBEE'S').\\n\"\n",
    "    \"Return ONLY JSON. No code fences, no commentary.\"\n",
    ")\n",
    "\n",
    "def _salvage_json_object(txt: str):\n",
    "    \"\"\"Try hard to recover a single JSON object from a messy string.\"\"\"\n",
    "    t = txt.strip()\n",
    "    # strip code fences if present\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    # direct parse\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # find largest {...} block\n",
    "    start = t.find(\"{\")\n",
    "    end = t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = t[start:end+1]\n",
    "        try:\n",
    "            obj = json.loads(candidate)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    # last resort: python-ish literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise RuntimeError(f\"Failed to parse single-object JSON:\\n{t[:400]}\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=12))\n",
    "def azure_label_one(mk: str):\n",
    "    \"\"\"Label exactly one merchant_key with strict JSON, resilient to noise.\"\"\"\n",
    "    if client is None:\n",
    "        return None\n",
    "    user = (\n",
    "        \"Label this merchant_key and return ONLY a single JSON object:\\n\"\n",
    "        '{ \"merchant_key\": \"...\", \"display_name\":\"...\", \"category\":\"...\", \"subcategory\":\"...\", \"tags\":[...] }\\n\\n'\n",
    "        f'merchant_key: \"{mk}\"'\n",
    "    )\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[{\"role\":\"system\",\"content\": SYSTEM}, {\"role\":\"user\",\"content\": user}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        response_format={\"type\": \"json_object\"},   # strongly nudges valid JSON\n",
    "    )\n",
    "    raw = resp.choices[0].message.content\n",
    "    obj = _salvage_json_object(raw)\n",
    "    # Coerce + fill\n",
    "    out = {\n",
    "        \"merchant_key\": mk,  # echo exactly\n",
    "        \"display_name\": str(obj.get(\"display_name\", mk)).upper().strip(),\n",
    "        \"category\": str(obj.get(\"category\", \"\")),\n",
    "        \"subcategory\": str(obj.get(\"subcategory\", \"\")),\n",
    "        \"tags\": obj.get(\"tags\", []),\n",
    "    }\n",
    "    # normalize tags into CSV (safe)\n",
    "    if not isinstance(out[\"tags\"], list):\n",
    "        out[\"tags\"] = []\n",
    "    out[\"tags\"] = [str(t).strip() for t in out[\"tags\"] if str(t).strip()]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b712b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new mappings needed or AI disabled.\n"
     ]
    }
   ],
   "source": [
    "new_rows = []\n",
    "if len(unmapped_keys) and client is not None and MAP_ALL:\n",
    "    print(f\"Labeling {len(unmapped_keys)} merchants (single-call mode)...\")\n",
    "    for idx, mk in enumerate(unmapped_keys, 1):\n",
    "        try:\n",
    "            item = azure_label_one(mk)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Label fail for '{mk}': {e}\")\n",
    "            continue\n",
    "\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if item:\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": item[\"display_name\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"subcategory\": item[\"subcategory\"],\n",
    "                \"tags\": \",\".join(item[\"tags\"]),\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        dim_new = pd.DataFrame(new_rows)\n",
    "        dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "        dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "        MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        dim = dim_all\n",
    "        print(f\"âœ… Added {len(new_rows)} merchant mappings (single-call).\")\n",
    "    else:\n",
    "        print(\"No new mappings added (single-call).\")\n",
    "else:\n",
    "    print(\"No new mappings needed or AI disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aef6c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Created headers-only merchants_dim.csv â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\merchants_dim.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6B: Persist merchants_dim.csv (idempotent) ---\n",
    "\n",
    "# Toggle if you ever want to skip writing on runs with no changes\n",
    "PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "# dim_cols defined in Cell 4; dim may be updated in Cell 6\n",
    "if not isinstance(PERSIST_MERCHANT_DIM, bool):\n",
    "    PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "if PERSIST_MERCHANT_DIM:\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'dim' in globals() and isinstance(dim, pd.DataFrame) and len(dim):\n",
    "        # ensure expected columns/order exist before save\n",
    "        for c in dim_cols:\n",
    "            if c not in dim.columns:\n",
    "                dim[c] = np.nan\n",
    "        dim = dim[dim_cols]\n",
    "\n",
    "        dim.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        print(f\"ðŸ“ merchants_dim.csv saved ({len(dim)} rows) â†’ {MERCHANT_DIM_PATH}\")\n",
    "    else:\n",
    "        # either no new mappings this run or dim was empty; ensure file exists\n",
    "        if not MERCHANT_DIM_PATH.exists():\n",
    "            pd.DataFrame(columns=dim_cols).to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            print(f\"ðŸ“ Created headers-only merchants_dim.csv â†’ {MERCHANT_DIM_PATH}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ merchants_dim.csv already exists; no changes to sync.\")\n",
    "else:\n",
    "    print(\"PERSIST_MERCHANT_DIM=False â†’ skipping merchants_dim.csv persistence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels joined.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"], errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Final output columns (feel free to adjust ordering)\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory\", \"tags\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "# Ensure existence even if null\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Canonical display name fallback\n",
    "df[\"display_name\"] = df[\"display_name\"].fillna(df[\"merchant_key\"])\n",
    "\n",
    "print(\"Labels joined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be3dff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_27924\\3478818825.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8 (fixed): Subscription detection (idempotent, no merge/apply warnings) ---\n",
    "\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # positive outflows only\n",
    "    amounts = g[\"amount\"].to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # gaps in days (datetime64[ns] -> int ns -> days)\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    if mu <= 0:\n",
    "        return False\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9))\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Clean any leftover artifacts from previous runs (e.g., is_subscription_x from merges)\n",
    "for col in [c for c in df.columns if c.startswith(\"is_subscription\") and c != \"is_subscription\"]:\n",
    "    df.drop(columns=col, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Compute a per-display_name subscription flag without merge/apply warnings\n",
    "subs_map = {}\n",
    "pos = df.loc[(df[\"amount\"] > 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]]\n",
    "for disp, g in pos.groupby(\"display_name\", dropna=False):\n",
    "    try:\n",
    "        subs_map[disp] = bool(detect_subscription(g[[\"date\", \"amount\"]]))\n",
    "    except Exception:\n",
    "        subs_map[disp] = False\n",
    "\n",
    "# Assign deterministically; idempotent across runs\n",
    "df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e514f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 1\n"
     ]
    }
   ],
   "source": [
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6675fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,822.81 (+2,546.55 vs prior)\n",
      "Top 3 merchants: WITHDRAWAL ALLY TYPE: ALLY PAYMT ID: CO: ALLY NAME: KOSISONNA UGOCHUKW %% ACH ECC WEB %% ACH TRACE ($1,494.22), WITHDRAWAL AMEX EPAYMENT TYPE: ACH PMT ID: DATA: ER AM CO: AMEX EPAYMENT NAME: KOSISONNA UGOCHUKWU %% ACH ECC WEB %% ACH TRACE ($777.78), PETAL ($738.96)\n",
      "Biggest category driver: nan ($5,822.81)\n",
      "\n",
      "Saved digest â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest â†’ {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f17a1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut nan by 17%\n",
      "\n",
      "Saved goal nudges â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Suggest % cuts in top categories to reach GOAL_SAVINGS over next 30 days\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges â†’ {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d67e86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cached: +1 â†’ total 146\n"
     ]
    }
   ],
   "source": [
    "# Build text field and store embeddings for semantic search\n",
    "def build_search_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"display_name\") or \"\"),\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        str(row.get(\"category\") or \"\"),\n",
    "        str(row.get(\"subcategory\") or \"\"),\n",
    "        str(row.get(\"tags\") or \"\"),\n",
    "    ]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Prepare rows (limit to recent for cost-control)\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df[~embed_df[\"txn_uid\"].isin(existing)][[\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    if client is None:\n",
    "        return [None for _ in texts]\n",
    "    # Use the Azure embeddings deployment name from env\n",
    "    emb_deploy = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")\n",
    "    if not emb_deploy:\n",
    "        return [None for _ in texts]\n",
    "\n",
    "    # New OpenAI client pattern for embeddings under Azure:\n",
    "    # base_url should be resource; we temporarily create a fresh client pointing to embeddings deployment\n",
    "    emb_client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{emb_deploy}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "    res = emb_client.embeddings.create(model=emb_deploy, input=list(texts))\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed):\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} â†’ total {len(merged)}\")\n",
    "else:\n",
    "    print(\"No new embeddings added (either none missing or AI disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c7420962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched CSV saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "ðŸ“„ Copy saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Reorder and save\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    \"category\",\"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\"is_subscription\",\"is_anomaly\",\"z_by_merchant\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… Enriched CSV saved â†’ {ENRICHED_OUT_PATH}\")\n",
    "print(f\"ðŸ“„ Copy saved â†’ {ENRICHED_COPY_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
