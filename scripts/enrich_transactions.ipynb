{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKoise — Blue Lantern: Enrichment pipeline\\n-----------------------------------------\\n- Reads raw statements in data/raw/\\n- Normalizes -> [date, description, amount, account]\\n- Creates merchant_key\\n- Applies categories via (1) overrides in config/categories.yaml and (2) data/processed/merchant_map.csv\\n- Optionally classifies unknown merchants with GPT (once) and appends to merchant_map\\n- Flags non-spend flows and is_necessity\\n- Writes enriched CSV for Power BI\\n\\nRun:\\n  uv run python scripts/enrich_transactions.py\\n  # or: python scripts/enrich_transactions.py\\n\\nRequires:\\n  pip install pandas pyyaml python-dateutil openpyxl (for .xlsx)\\n  # pdfplumber is optional for PDFs: pip install pdfplumber\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Koise — Blue Lantern: Enrichment pipeline\n",
    "-----------------------------------------\n",
    "- Reads raw statements in data/raw/\n",
    "- Normalizes -> [date, description, amount, account]\n",
    "- Creates merchant_key\n",
    "- Applies categories via (1) overrides in config/categories.yaml and (2) data/processed/merchant_map.csv\n",
    "- Optionally classifies unknown merchants with GPT (once) and appends to merchant_map\n",
    "- Flags non-spend flows and is_necessity\n",
    "- Writes enriched CSV for Power BI\n",
    "\n",
    "Run:\n",
    "  uv run python scripts/enrich_transactions.py\n",
    "  # or: python scripts/enrich_transactions.py\n",
    "\n",
    "Requires:\n",
    "  pip install pandas pyyaml python-dateutil openpyxl (for .xlsx)\n",
    "  # pdfplumber is optional for PDFs: pip install pdfplumber\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d4ffde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1397b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, csv, json, glob\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============== SETTINGS ==============\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "PATH_RAW       = os.path.join(PROJECT_ROOT, \"data\", \"raw\")\n",
    "PATH_INTERIM   = os.path.join(PROJECT_ROOT, \"data\", \"interim\")\n",
    "PATH_PROCESSED = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "PATH_DOCS      = os.path.join(PROJECT_ROOT, \"docs\")\n",
    "PATH_CONFIG    = os.path.join(PROJECT_ROOT, \"config\", \"categories.yaml\")\n",
    "PATH_MMAP      = os.path.join(PATH_PROCESSED, \"merchant_map.csv\")\n",
    "\n",
    "USE_GPT = False  # <-- flip to True when you want to call GPT for unknown merchants\n",
    "\n",
    "# If you turn on GPT, set your env var OPENAI_API_KEY before running.\n",
    "# Model & batch size:\n",
    "GPT_MODEL = \"gpt-4o-mini\"  # light, cheap; swap to gpt-4o for extra quality\n",
    "GPT_BATCH = 20\n",
    "\n",
    "# ======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db494c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dirs():\n",
    "    for p in [PATH_INTERIM, PATH_PROCESSED, PATH_DOCS]:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def read_yaml(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def try_read_csv_like(path: str) -> pd.DataFrame | None:\n",
    "    # 1) normal read\n",
    "    for enc in (\"utf-8\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) python engine w/ sep=None\n",
    "    for enc in (\"utf-8\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=None, engine=\"python\", encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 3) sniff\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
    "            sample = f.read(4096)\n",
    "            dialect = csv.Sniffer().sniff(sample)\n",
    "            f.seek(0)\n",
    "            return pd.read_csv(f, dialect.delimiter)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def try_read_xlsx(path: str) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    except Exception:\n",
    "        return None\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize to columns: date, description, amount\n",
    "    Handles messy cases:\n",
    "      - 'Amount Debit'/'Amount Credit' pairs\n",
    "      - 'Credit'/'Debit' pairs\n",
    "      - various date column names (Posting/Transaction/etc.)\n",
    "      - date recovery from Memo/Description if no explicit date column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    # ---------- Amount ----------\n",
    "    # First: unified money cleaner\n",
    "    def to_num(s):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "        s = s.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "        if \"(\" in s and \")\" in s:\n",
    "            s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            try: return -float(s)\n",
    "            except: return pd.NA\n",
    "        try: return float(s)\n",
    "        except: return pd.NA\n",
    "\n",
    "    amount_col = None\n",
    "\n",
    "    # Case A: explicit Amount column\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"\\bamount\\b\", c, re.I) and c.lower() not in (\"amount debit\",\"amount credit\"):\n",
    "            amount_col = c\n",
    "            break\n",
    "\n",
    "    # Case B: Amount Debit / Amount Credit pair (credit positive)\n",
    "    if amount_col is None:\n",
    "        has_debit  = any(re.fullmatch(r\"(?i)amount debit\", c) for c in df.columns)\n",
    "        has_credit = any(re.fullmatch(r\"(?i)amount credit\", c) for c in df.columns)\n",
    "        if has_debit or has_credit:\n",
    "            debit_col  = next((c for c in df.columns if re.fullmatch(r\"(?i)amount debit\", c)), None)\n",
    "            credit_col = next((c for c in df.columns if re.fullmatch(r\"(?i)amount credit\", c)), None)\n",
    "            debit  = df[debit_col].map(to_num)  if debit_col  else 0\n",
    "            credit = df[credit_col].map(to_num) if credit_col else 0\n",
    "            df[\"__amount\"] = (credit.fillna(0) - debit.fillna(0))\n",
    "            amount_col = \"__amount\"\n",
    "\n",
    "    # Case C: generic Debit / Credit pair\n",
    "    if amount_col is None:\n",
    "        debit_col  = next((c for c in df.columns if re.search(r\"\\bdebit\\b\", c, re.I)), None)\n",
    "        credit_col = next((c for c in df.columns if re.search(r\"\\bcredit\\b\", c, re.I)), None)\n",
    "        if debit_col or credit_col:\n",
    "            debit  = df[debit_col].map(to_num)  if debit_col  else 0\n",
    "            credit = df[credit_col].map(to_num) if credit_col else 0\n",
    "            df[\"__amount\"] = (credit.fillna(0) - debit.fillna(0))\n",
    "            amount_col = \"__amount\"\n",
    "\n",
    "    # If still nothing, try any money-like column\n",
    "    if amount_col is None:\n",
    "        money_like = [c for c in df.columns if re.search(r\"amount|amt|\\$\\s*\", c, re.I)]\n",
    "        if money_like:\n",
    "            amount_col = money_like[0]\n",
    "\n",
    "    # ---------- Description ----------\n",
    "    desc_col = None\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"description|details|transaction description|payee|memo\", c, re.I):\n",
    "            desc_col = c\n",
    "            break\n",
    "    if desc_col is None:\n",
    "        # fallback: pick the widest text column\n",
    "        text_cols = [c for c in df.columns if df[c].astype(str).str.len().mean() > 6]\n",
    "        desc_col = text_cols[0] if text_cols else df.columns[0]\n",
    "\n",
    "    # ---------- Date ----------\n",
    "    date_col = None\n",
    "    # Strong candidates first\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(r\"(?i)(date|transaction date|posting date|posted date|trans date)\", c):\n",
    "            date_col = c\n",
    "            break\n",
    "    # Any column with 'date' in the name\n",
    "    if date_col is None:\n",
    "        dateish = [c for c in df.columns if re.search(r\"date\", c, re.I)]\n",
    "        if dateish:\n",
    "            date_col = dateish[0]\n",
    "\n",
    "    # Build output\n",
    "    out = pd.DataFrame()\n",
    "    out[\"description\"] = df[desc_col].astype(str)\n",
    "\n",
    "    # Amount\n",
    "    if amount_col is not None:\n",
    "        out[\"amount\"] = df[amount_col].map(to_num)\n",
    "    else:\n",
    "        out[\"amount\"] = pd.NA\n",
    "\n",
    "    # Date: direct if present, else extract from text (Memo/Description)\n",
    "    if date_col is not None:\n",
    "        out[\"date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        # Try pull from memo/description like: \"Date 07/19/25\" or \"07/19/2025\"\n",
    "        pat = re.compile(r\"(?:Date\\s*)?(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\")\n",
    "        def extract_date(s):\n",
    "            m = pat.search(str(s))\n",
    "            if m:\n",
    "                return pd.to_datetime(m.group(1), errors=\"coerce\")\n",
    "            return pd.NaT\n",
    "        out[\"date\"] = df.get(\"Memo\", pd.Series([None]*len(df))).apply(extract_date)\n",
    "        # if still NaT, try description\n",
    "        mask = out[\"date\"].isna()\n",
    "        out.loc[mask, \"date\"] = out.loc[mask, \"description\"].apply(extract_date)\n",
    "\n",
    "    # Final coercion & tidy\n",
    "    try:\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], format=\"mixed\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"date\"] = out[\"date\"].dt.date\n",
    "\n",
    "    # Clean description\n",
    "    out[\"description\"] = out[\"description\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    # Drop rows with no useful info\n",
    "    out = out.dropna(subset=[\"description\"], how=\"all\")\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "    # Clean amount\n",
    "    def clean_amt(v):\n",
    "        s = str(v) if pd.notna(v) else \"\"\n",
    "        s = s.replace(\"$\",\"\").replace(\",\",\"\").strip()\n",
    "        neg = \"(\" in s and \")\" in s\n",
    "        s = s.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "        try:\n",
    "            val = float(s)\n",
    "            return -val if neg else val\n",
    "        except:\n",
    "            return pd.NA\n",
    "    out[\"amount\"] = out[\"amount\"].apply(clean_amt)\n",
    "\n",
    "    # Coerce date\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "    # Description as string\n",
    "    out[\"description\"] = out[\"description\"].astype(str)\n",
    "\n",
    "    return out.dropna(subset=[\"description\"]).reset_index(drop=True)\n",
    "\n",
    "def merchant_key_from_description(s: str) -> str:\n",
    "    s = (s or \"\").upper().strip()\n",
    "\n",
    "    # remove common noise phrases first\n",
    "    s = re.sub(r\"\\bAPPLE PAY(MENT)?(?:\\s+ENDING\\s+IN\\s+\\d+)?\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\bGOOGLE PAY\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\bWALLET\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\bAPPLE CASH\\b\", \" \", s)\n",
    "\n",
    "    # remove digits & keep core letters and a few separators\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "    s = re.sub(r\"[^A-Z &'\\-]\", \" \", s)\n",
    "\n",
    "    # drop generic tokens that don’t help merchant identity\n",
    "    s = re.sub(r\"\\b(ONLINE|USA|US|STORE|CARD|VISA|MC|AMEX|DISCOVER|AUTOMATIC|PAYMENT|THANK|SEE|DETAILS|FULL|BALANCE)\\b\", \" \", s)\n",
    "\n",
    "    # squeeze spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_raw_transactions() -> pd.DataFrame:\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for path in glob.glob(os.path.join(PATH_RAW, \"*\")):\n",
    "        base = os.path.basename(path)\n",
    "        account = os.path.splitext(base)[0].lower()\n",
    "        df = None\n",
    "        if base.lower().endswith(\".csv\"):\n",
    "            df = try_read_csv_like(path)\n",
    "        elif base.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            df = try_read_xlsx(path)\n",
    "        else:\n",
    "            print(f\"[skip] unsupported (not CSV/XLSX): {base}\")\n",
    "            continue\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            print(f\"[skip] could not parse: {base}\")\n",
    "            continue\n",
    "\n",
    "        norm = normalize_columns(df)\n",
    "        missing = [c for c in [\"date\",\"description\",\"amount\"] if c not in norm.columns or norm[c].isna().all()]\n",
    "        if missing:\n",
    "            print(f\"[warn] {base} missing/empty columns: {missing} — keeping what we have.\")\n",
    "        norm[\"account\"] = account\n",
    "        frames.append(norm)\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No readable files in data/raw/.\")\n",
    "    all_tx = pd.concat(frames, ignore_index=True)\n",
    "    all_tx[\"merchant_key\"] = all_tx[\"description\"].apply(merchant_key_from_description)\n",
    "    return all_tx\n",
    "\n",
    "\n",
    "def load_or_init_merchant_map() -> pd.DataFrame:\n",
    "    # canonical columns\n",
    "    cols = [\n",
    "        \"merchant_key\",\n",
    "        \"display_name\",\n",
    "        \"category\",\n",
    "        \"subcategory\",\n",
    "        \"tags\",\n",
    "        \"confidence\",\n",
    "        \"source\",\n",
    "        \"first_seen\",\n",
    "        \"last_seen\",\n",
    "    ]\n",
    "    if os.path.exists(PATH_MMAP):\n",
    "        mm = pd.read_csv(PATH_MMAP)\n",
    "        # add any missing columns with NA defaults\n",
    "        for c in cols:\n",
    "            if c not in mm.columns:\n",
    "                mm[c] = pd.NA\n",
    "        # order columns consistently\n",
    "        mm = mm[cols].copy()\n",
    "    else:\n",
    "        mm = pd.DataFrame(columns=cols)\n",
    "    return mm\n",
    "\n",
    "\n",
    "def yaml_contains_category(tx: pd.DataFrame, cfg: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a Series with YAML-derived categories by substring match on merchant_key.\n",
    "    More specific/longer patterns win. First match wins.\n",
    "    \"\"\"\n",
    "    mapping = cfg.get(\"mapping\", {}) or {}\n",
    "    # flatten to [(pattern_key, category, display_name, length)]\n",
    "    rules = []\n",
    "    for cat, items in mapping.items():\n",
    "        if not isinstance(items, list):\n",
    "            continue\n",
    "        for raw in items:\n",
    "            pat = merchant_key_from_description(str(raw))\n",
    "            if pat:\n",
    "                rules.append((pat, cat, str(raw), len(pat)))\n",
    "    # longest patterns first so 'AMAZON PRIME' beats 'AMAZON'\n",
    "    rules.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    cat = pd.Series(index=tx.index, dtype=\"object\")\n",
    "    disp = pd.Series(index=tx.index, dtype=\"object\")\n",
    "\n",
    "    for pat, c, dispname, _ in rules:\n",
    "        # cheap substring check\n",
    "        mask = tx[\"merchant_key\"].str.contains(re.escape(pat), na=False)\n",
    "        # only fill where still empty\n",
    "        fill = mask & cat.isna()\n",
    "        cat.loc[fill] = c\n",
    "        disp.loc[fill] = dispname\n",
    "\n",
    "    tx[\"_yaml_category\"] = cat\n",
    "    tx[\"_yaml_display\"]  = disp\n",
    "    return cat\n",
    "\n",
    "\n",
    "def apply_rules_and_cache(tx, cfg, mm):\n",
    "    # 1) YAML contains\n",
    "    _ = yaml_contains_category(tx, cfg)  # fills tx['_yaml_category'], tx['_yaml_display']\n",
    "    has_yaml = tx[\"_yaml_category\"].notna()\n",
    "\n",
    "    # 2) merchant_map exact (cache)\n",
    "    mm_required = [\"merchant_key\",\"category\",\"display_name\",\"subcategory\",\"tags\",\"confidence\",\"source\"]\n",
    "    for c in mm_required:\n",
    "        if c not in mm.columns:\n",
    "            mm[c] = pd.NA\n",
    "    mm_slim = mm[mm_required].drop_duplicates()\n",
    "\n",
    "    merged = tx.merge(mm_slim, on=\"merchant_key\", how=\"left\", suffixes=(\"\",\"_mm\"))\n",
    "\n",
    "    # ensure suffix cols exist\n",
    "    for c in [\"category_mm\",\"display_name_mm\",\"subcategory_mm\",\"tags_mm\",\"confidence_mm\",\"source_mm\"]:\n",
    "        if c not in merged.columns:\n",
    "            merged[c] = pd.NA\n",
    "\n",
    "    # final picks: YAML wins; else merchant_map\n",
    "    merged[\"category_final\"] = merged[\"_yaml_category\"].where(has_yaml, merged[\"category_mm\"])\n",
    "    merged[\"display_name_final\"] = merged[\"_yaml_display\"].where(has_yaml, merged[\"display_name_mm\"])\n",
    "    merged[\"subcategory_final\"]  = merged[\"subcategory_mm\"]\n",
    "    merged[\"tags_final\"]         = merged[\"tags_mm\"]\n",
    "    merged[\"confidence_final\"]   = merged[\"confidence_mm\"]\n",
    "    merged[\"source_final\"]       = merged[\"source_mm\"].where(~has_yaml, \"yaml\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def mark_non_spend(enriched: pd.DataFrame) -> pd.DataFrame:\n",
    "    enriched = enriched.copy()\n",
    "    non_spend_cats = {\n",
    "        \"credit_card_payment\", \"transfer\", \"loan_payment\",\n",
    "        \"income\", \"refund\", \"internal_move\"\n",
    "    }\n",
    "    enriched[\"is_non_spend_flow\"] = (\n",
    "        enriched[\"is_non_spend_flow\"]\n",
    "        | enriched[\"category_final\"].isin(non_spend_cats)\n",
    "    ).fillna(False)\n",
    "\n",
    "    pay_phrases = enriched[\"description\"].str.contains(\n",
    "        r\"AUTOMATIC PAYMENT|DIRECTPAY|PAYMENT\\s*(THANK|YOU)|FULL BALANCE|ACCTVERIFY|TRANSFER\",\n",
    "        case=False, na=False\n",
    "    )\n",
    "    enriched.loc[pay_phrases, \"is_non_spend_flow\"] = True\n",
    "    return enriched\n",
    "\n",
    "\n",
    "# -------- GPT section (optional) --------\n",
    "def gpt_classify_merchants(merchant_rows: List[Dict[str,Any]], category_set: List[str]) -> List[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    merchant_rows: [{\"merchant_key\": \"...\", \"samples\": [\"raw desc 1\", \"raw desc 2\", ...]}]\n",
    "    Returns list of dicts with: merchant_key, display_name, category, subcategory, tags, confidence, source\n",
    "    \"\"\"\n",
    "    # Placeholder — no external calls when USE_GPT=False\n",
    "    if not USE_GPT:\n",
    "        return []\n",
    "\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    out: List[Dict[str,Any]] = []\n",
    "    SYSTEM = (\n",
    "        \"You are a meticulous financial transaction classifier. \"\n",
    "        \"Use only the provided CATEGORY_SET. Return JSONL, one object per line. \"\n",
    "        \"Fields: merchant_key, display_name, category, subcategory, tags (comma list), confidence (0-1).\"\n",
    "    )\n",
    "\n",
    "    # Chunk in batches\n",
    "    for i in range(0, len(merchant_rows), GPT_BATCH):\n",
    "        batch = merchant_rows[i:i+GPT_BATCH]\n",
    "        rows_text = []\n",
    "        for r in batch:\n",
    "            mk = r[\"merchant_key\"]\n",
    "            samples = \"; \".join(r.get(\"samples\", [])[:3])\n",
    "            rows_text.append(f'merchant_key=\"{mk}\", raw=\"{samples}\"')\n",
    "\n",
    "        USER = \"CATEGORY_SET = \" + json.dumps(category_set) + \"\\nClassify:\\n\" + \"\\n\".join(rows_text)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\":USER}]\n",
    "        )\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        # Parse JSONL lines\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                obj[\"source\"] = \"gpt\"\n",
    "                out.append(obj)\n",
    "            except Exception:\n",
    "                # best-effort parsing; skip bad lines\n",
    "                pass\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6ca08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] unsupported (not CSV/XLSX): apple_credit_1m.pdf\n",
      "[warn] chase_checking_6m.CSV missing/empty columns: ['date', 'amount'] — keeping what we have.\n",
      "[skip] unsupported (not CSV/XLSX): petal_credit_1m.pdf\n",
      "[skip] could not parse: ssscu_checking_6m.CSV\n",
      "[skip] could not parse: ssscu_credit_6m.CSV\n",
      "✅ Enrichment complete.\n",
      " - Normalized: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\interim\\all_transactions_normalized.csv\n",
      " - Merchant map: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\merchant_map.csv\n",
      " - Enriched: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\transactions_enriched.csv\n",
      " - Review unknowns: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\docs\\review_unknowns.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_50000\\2834387649.py:133: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  out[\"date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_50000\\2834387649.py:230: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_tx = pd.concat(frames, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------- MAIN ---------------\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    # 1) Load config + merchant_map\n",
    "    cfg = read_yaml(PATH_CONFIG)\n",
    "    mm = load_or_init_merchant_map()\n",
    "\n",
    "    # 2) Read and normalize all raw statements\n",
    "    tx = load_raw_transactions()\n",
    "    tx.to_csv(os.path.join(PATH_INTERIM, \"all_transactions_normalized.csv\"), index=False)\n",
    "\n",
    "    # 3) Apply rules + cache\n",
    "    applied = apply_rules_and_cache(tx, cfg, mm)\n",
    "\n",
    "    # 4) Build unknown merchant list (for GPT or review)\n",
    "    unknown = applied[applied[\"category_final\"].isna()].copy()\n",
    "    unknown_keys = unknown[\"merchant_key\"].dropna().unique().tolist()\n",
    "\n",
    "    # Group samples per merchant_key for better GPT context\n",
    "    samples = (unknown\n",
    "               .groupby(\"merchant_key\")[\"description\"]\n",
    "               .apply(lambda s: list(pd.Series(s).dropna().astype(str).head(3)))\n",
    "               .reset_index()\n",
    "               .rename(columns={\"description\":\"samples\"}))\n",
    "    to_classify = samples.to_dict(orient=\"records\")\n",
    "\n",
    "    # 5) Optional GPT classify\n",
    "    category_set = list({*cfg.get(\"necessities\", []),\n",
    "                         *cfg.get(\"discretionary\", []),\n",
    "                         *cfg.get(\"non_spend_flows\", []),\n",
    "                         *list((cfg.get(\"mapping\") or {}).keys())})\n",
    "    gpt_results = gpt_classify_merchants(to_classify, category_set)\n",
    "\n",
    "    # 6) Update merchant_map with GPT results\n",
    "    if gpt_results:\n",
    "        gpt_df = pd.DataFrame(gpt_results)\n",
    "        now = datetime.utcnow().date().isoformat()\n",
    "        gpt_df[\"first_seen\"] = now\n",
    "        gpt_df[\"last_seen\"]  = now\n",
    "\n",
    "        # merge/update existing records\n",
    "        mm_existing = mm.set_index(\"merchant_key\")\n",
    "        for _, row in gpt_df.iterrows():\n",
    "            mk = row[\"merchant_key\"]\n",
    "            if mk in mm_existing.index:\n",
    "                # update fields conservatively\n",
    "                for col in [\"display_name\",\"category\",\"subcategory\",\"tags\",\"confidence\",\"source\",\"last_seen\"]:\n",
    "                    mm_existing.at[mk, col] = row.get(col, mm_existing.at[mk, col])\n",
    "            else:\n",
    "                mm_existing.loc[mk] = row\n",
    "        mm = mm_existing.reset_index()\n",
    "\n",
    "        # Re-apply rules + cache after GPT\n",
    "        applied = apply_rules_and_cache(tx, cfg, mm)\n",
    "\n",
    "    # 7) Flags\n",
    "    applied = compute_flags(applied, cfg)\n",
    "\n",
    "    # 8) Output: merchant_map (updated), transactions_enriched\n",
    "    mm.to_csv(PATH_MMAP, index=False)\n",
    "\n",
    "    enriched_cols = [\n",
    "        \"date\",\"account\",\"description\",\"merchant_key\",\n",
    "        \"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\"confidence_final\",\"source_final\",\n",
    "        \"amount\",\"is_necessity\",\"is_non_spend_flow\"\n",
    "    ]\n",
    "    for c in enriched_cols:\n",
    "        if c not in applied.columns:\n",
    "            applied[c] = pd.NA\n",
    "    enriched = applied[enriched_cols].copy()\n",
    "    enriched = enriched.sort_values([\"date\",\"account\"], na_position=\"last\")\n",
    "\n",
    "    enriched_path = os.path.join(PATH_PROCESSED, \"transactions_enriched.csv\")\n",
    "    enriched.to_csv(enriched_path, index=False)\n",
    "\n",
    "    # 9) Review queue for any still-unknown items\n",
    "    still_unknown = enriched[enriched[\"category_final\"].isna()].copy()\n",
    "    if not still_unknown.empty:\n",
    "        still_unknown.to_csv(os.path.join(PATH_DOCS, \"review_unknowns.csv\"), index=False)\n",
    "\n",
    "    print(\"✅ Enrichment complete.\")\n",
    "    print(f\" - Normalized: {os.path.join(PATH_INTERIM, 'all_transactions_normalized.csv')}\")\n",
    "    print(f\" - Merchant map: {PATH_MMAP}\")\n",
    "    print(f\" - Enriched: {enriched_path}\")\n",
    "    if not still_unknown.empty:\n",
    "        print(f\" - Review unknowns: {os.path.join(PATH_DOCS, 'review_unknowns.csv')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293805f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Tools)",
   "language": "python",
   "name": "ai-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
