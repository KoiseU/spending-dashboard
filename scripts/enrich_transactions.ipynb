{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Loaded utils from C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\scripts\\ai_utils.ipynb\n",
      "âœ… Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to ensure OpenAI SDK is available (for Azure OpenAI)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# dotenv for local runs\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\"])\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "# --- Paths (robust: prefer GITHUB_WORKSPACE, never walk above repo) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "# find the repo root by locating the first directory that has a .git\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR = REPO / \"config\"\n",
    "STATE_DIR = REPO / \".state\"\n",
    "VECTOR_DIR = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH   = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH = DATA_RAW / \"latest.csv\"               # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH   = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# --- Ensure dirs ---\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Config flags ---\n",
    "MAP_ALL = True              # map any merchant missing from dimension\n",
    "GOAL_SAVINGS = 1000.0       # target monthly savings for \"goal nudges\"\n",
    "ANOMALY_Z = 2.5             # z-score threshold for anomalies\n",
    "\n",
    "# --- Load local envs if present (for dev runs) ---\n",
    "for p in [REPO / \"scripts\" / \".env\", REPO / \".env\"]:\n",
    "    if p.exists():\n",
    "        load_dotenv(p, override=True)\n",
    "\n",
    "# --- Azure OpenAI env ---\n",
    "AZURE_OPENAI_ENDPOINT   = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "AZURE_OPENAI_API_KEY    = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\")  # chat model deployment name\n",
    "AZURE_OPENAI_EMBEDDINGS = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")  # embeddings deployment name\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "if not (AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT):\n",
    "    print(\"âš ï¸ Azure OpenAI env not fully set. AI labeling will be skipped.\")\n",
    "\n",
    "# Build OpenAI (Azure) client if possible (used only if this notebook calls it directly)\n",
    "client = None\n",
    "if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and AZURE_OPENAI_DEPLOYMENT:\n",
    "    client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "\n",
    "# --- Load shared utils notebook (keeps everything .ipynb) ---\n",
    "UTILS_NOTEBOOK = REPO / \"scripts\" / \"ai_utils.ipynb\"\n",
    "if UTILS_NOTEBOOK.exists():\n",
    "    try:\n",
    "        import IPython\n",
    "        IPython.get_ipython().run_line_magic(\"run\", str(UTILS_NOTEBOOK))\n",
    "        print(f\"ðŸ“š Loaded utils from {UTILS_NOTEBOOK}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not %run {UTILS_NOTEBOOK}: {e}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Utils notebook not found at {UTILS_NOTEBOOK}. Skipping.\")\n",
    "\n",
    "print(\"âœ… Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6173ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merchant labeling (robust JSON mode, stable schema, timezone-aware) ---\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import json, re, time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "MERCHANT_DIM_PATH = Path(\"config/merchants_dim.csv\")\n",
    "BATCH = 20\n",
    "MAP_ALL = True\n",
    "\n",
    "# Stable schema prevents concat FutureWarnings\n",
    "SCHEMA = {\n",
    "    \"merchant_key\": \"string\",\n",
    "    \"display_name\": \"string\",\n",
    "    \"category\": \"string\",\n",
    "    \"subcategory\": \"string\",\n",
    "    \"tags\": \"string\",\n",
    "    \"source\": \"string\",\n",
    "    \"confidence\": \"float64\",\n",
    "    \"last_updated\": \"string\",\n",
    "}\n",
    "\n",
    "def _ensure_merchants_dim():\n",
    "    if MERCHANT_DIM_PATH.exists():\n",
    "        md = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "        for col, dt in SCHEMA.items():\n",
    "            if col not in md.columns:\n",
    "                md[col] = pd.Series(dtype=dt)\n",
    "        return md.astype(SCHEMA, copy=False)\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return pd.DataFrame({c: pd.Series(dtype=dt) for c, dt in SCHEMA.items()})\n",
    "\n",
    "def _parse_labels_strict_or_salvage(txt: str):\n",
    "    txt = txt.strip()\n",
    "    # Accept {\"items\":[...]} or [...]\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        if isinstance(obj, dict) and \"items\" in obj and isinstance(obj[\"items\"], list):\n",
    "            return obj[\"items\"]\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Salvage: try longest [...] block\n",
    "    m = re.findall(r\"\\[[\\s\\S]*\\]\", txt)\n",
    "    if m:\n",
    "        for cand in reversed(m):\n",
    "            try:\n",
    "                return json.loads(cand)\n",
    "            except Exception:\n",
    "                continue\n",
    "    raise RuntimeError(f\"Failed to parse AI JSON (first 400 chars):\\n{txt[:400]}\")\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=1, max=20), stop=stop_after_attempt(5))\n",
    "def azure_label_batch(keys_batch):\n",
    "    compact = [str(k)[:100] for k in keys_batch]  # shrink to avoid truncation\n",
    "\n",
    "    sys_prompt = (\n",
    "        \"You are labeling merchant identifiers for a personal finance dashboard.\\n\"\n",
    "        \"For each merchant_key, produce fields: merchant_key, display_name, category, subcategory, tags.\\n\"\n",
    "        \"- display_name: short human-friendly name (e.g., 'ARCO', 'Apple Card').\\n\"\n",
    "        \"- category: one of Dining, Groceries, Gas, Shopping, Utilities, Subscriptions, Transfers, \"\n",
    "        \"Income, Health, Travel, Entertainment, Education, Fees, Misc.\\n\"\n",
    "        \"- subcategory: specific subtype (e.g., 'Gas Station', 'Fast Casual', 'Internet Service').\\n\"\n",
    "        \"- tags: array of 1â€“5 lowercase keywords (e.g., ['gas','fuel']).\\n\"\n",
    "        'Return ONLY JSON in this exact shape: {\"items\":[{...}]} with no extra commentary.'\n",
    "    )\n",
    "    usr_payload = {\"merchant_keys\": compact}\n",
    "\n",
    "    c = azure_chat_client()\n",
    "    r = c.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": sys_prompt},\n",
    "            {\"role\":\"user\",\"content\": json.dumps(usr_payload)}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1400,\n",
    "        response_format={\"type\": \"json_object\"}  # strict JSON\n",
    "    )\n",
    "    return _parse_labels_strict_or_salvage(r.choices[0].message.content)\n",
    "\n",
    "def label_new_merchants(df, merchant_key_col=\"merchant_key\"):\n",
    "    md = _ensure_merchants_dim()\n",
    "    if merchant_key_col not in df.columns:\n",
    "        print(f\"Column '{merchant_key_col}' not in dataframe; skipping labeling.\")\n",
    "        return 0\n",
    "\n",
    "    known = set(md[\"merchant_key\"].astype(str)) if len(md) > 0 else set()\n",
    "    candidates = sorted(set(df[merchant_key_col].astype(str)) - known)\n",
    "    if not MAP_ALL or not candidates:\n",
    "        print(\"No new merchants to label.\"); return 0\n",
    "\n",
    "    added = 0\n",
    "    for i in range(0, len(candidates), BATCH):\n",
    "        batch = candidates[i:i+BATCH]\n",
    "        items = azure_label_batch(batch)\n",
    "        now = datetime.now(timezone.utc).isoformat()  # timezone-aware\n",
    "\n",
    "        rows = []\n",
    "        for it in items:\n",
    "            mk = str(it.get(\"merchant_key\") or \"\").strip()\n",
    "            if not mk:\n",
    "                continue\n",
    "            display = str(it.get(\"display_name\", mk)).upper().strip()\n",
    "            category = str(it.get(\"category\",\"\")).strip()\n",
    "            subcat   = str(it.get(\"subcategory\",\"\")).strip()\n",
    "            tags_val = it.get(\"tags\", [])\n",
    "            tags_csv = \",\".join([str(t).strip() for t in tags_val]) if isinstance(tags_val, list) else \"\"\n",
    "            rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": display,\n",
    "                \"category\": category,\n",
    "                \"subcategory\": subcat,\n",
    "                \"tags\": tags_csv,\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "        if rows:\n",
    "            chunk = pd.DataFrame(rows)\n",
    "            # ensure schema for both frames\n",
    "            for col, dt in SCHEMA.items():\n",
    "                if col not in chunk.columns:\n",
    "                    chunk[col] = pd.Series(dtype=dt)\n",
    "            chunk = chunk.astype(SCHEMA, copy=False)\n",
    "            md = md.astype(SCHEMA, copy=False)\n",
    "\n",
    "            md = pd.concat([md, chunk], ignore_index=True)\n",
    "            md = md.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "            md.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            added += len(chunk)\n",
    "            print(f\"Added {len(chunk)} merchant mappings (running total {added}).\")\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    return added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b64fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 147 transactions.\n"
     ]
    }
   ],
   "source": [
    "# Load latest.csv (from build_latest.ipynb), robust path resolution\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "print(f\"Loaded {len(df)} transactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1397b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized.\n"
     ]
    }
   ],
   "source": [
    "# Normalize noisy merchant strings into a stable 'merchant_key'\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "def normalize_merchant_key(txt: str) -> str:\n",
    "    t = (txt or \"\").upper().strip()\n",
    "    # Remove common noise: excessive spaces, digits, #, store ids, etc.\n",
    "    t = re.sub(r\"\\d{2,}\", \"\", t)              # drop long digit runs\n",
    "    t = re.sub(r\"[-_/#*]+\", \" \", t)           # separators -> space\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    # Drop locale suffixes like \"NV\", \"CA\" at the end if present\n",
    "    t = re.sub(r\"\\b([A-Z]{2})\\b$\", \"\", t).strip()\n",
    "    # Collapse APPLE PAY / GOOGLE PAY hints\n",
    "    t = t.replace(\"APPLE PAY\", \"\").replace(\"GOOGLE PAY\", \"\").strip()\n",
    "    # Fallback\n",
    "    return t or \"UNKNOWN\"\n",
    "\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].str.len() > 0,\n",
    "    df[\"merchant_name\"].apply(normalize_merchant_key),\n",
    "    df[\"name\"].apply(normalize_merchant_key)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize merchant dimension table\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db494c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a financial data labeling assistant. \"\n",
    "    \"For each merchant_key, produce concise JSON objects with fields: \"\n",
    "    \"display_name (string), category (string), subcategory (string), tags (list of short strings). \"\n",
    "    \"Use US personal finance categories like Dining, Groceries, Gas, Utilities, Subscriptions, Travel, Health, Shopping, Income, Transfers. \"\n",
    "    \"Keep display_name human-friendly (e.g., 'APPLEBEE'S', 'PANDA EXPRESS'). \"\n",
    "    \"When uncertain, make your best guess.\"\n",
    ")\n",
    "\n",
    "def build_user_prompt(merchant_keys):\n",
    "    # Keep prompt compact; model can handle ~50-80 at once easily; weâ€™ll batch anyway.\n",
    "    examples = \"\\n\".join(f'- \"{k}\"' for k in merchant_keys)\n",
    "    return (\n",
    "        \"Label the following merchant keys. Return ONLY a valid JSON array where each item is:\\n\"\n",
    "        \"{merchant_key, display_name, category, subcategory, tags}\\n\\n\"\n",
    "        f\"MERCHANT_KEYS:\\n{examples}\"\n",
    "    )\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=8))\n",
    "def azure_label_batch(keys_batch):\n",
    "    if client is None:\n",
    "        return []\n",
    "    msg = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM},\n",
    "        {\"role\":\"user\", \"content\": build_user_prompt(keys_batch)}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=msg,\n",
    "        temperature=0.2,\n",
    "        max_tokens=1200\n",
    "    )\n",
    "    txt = resp.choices[0].message.content.strip()\n",
    "    # Sometimes models wrap in code fencesâ€”strip them\n",
    "    if txt.startswith(\"```\"):\n",
    "        txt = re.sub(r\"^```(json)?\", \"\", txt, flags=re.IGNORECASE).strip()\n",
    "        txt = re.sub(r\"```$\", \"\", txt).strip()\n",
    "    try:\n",
    "        parsed = json.loads(txt)\n",
    "        if isinstance(parsed, dict):\n",
    "            parsed = [parsed]\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        # Last resort: try to eval if it's accidentally Python-ish\n",
    "        try:\n",
    "            parsed = ast.literal_eval(txt)\n",
    "            if isinstance(parsed, dict):\n",
    "                parsed = [parsed]\n",
    "            return parsed\n",
    "        except Exception:\n",
    "            raise RuntimeError(f\"Failed to parse AI JSON:\\n{txt}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b712b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new merchants to label.\n",
      "AI labeling done. New merchants added: 0\n",
      "Wrote 147 embeddings (dim 3072) â†’ vectorstore\\embeddings.parquet\n",
      "Embeddings built: 147\n"
     ]
    }
   ],
   "source": [
    "USE_AZURE = all(os.getenv(k) for k in [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\",\"AZURE_OPENAI_EMBEDDINGS\"\n",
    "])\n",
    "\n",
    "if USE_AZURE and 'df' in globals():\n",
    "    try:\n",
    "        added = label_new_merchants(df)\n",
    "        print(f\"AI labeling done. New merchants added: {added}\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Labeling failed (final):\", e)\n",
    "\n",
    "    try:\n",
    "        embedded = build_embeddings(df)\n",
    "        print(f\"Embeddings built: {embedded}\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Embeddings failed (final):\", e)\n",
    "else:\n",
    "    print(\"Azure not configured or df missing; skipping AI steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6eba14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels joined.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"], errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Final output columns (feel free to adjust ordering)\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory\", \"tags\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "# Ensure existence even if null\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Canonical display name fallback\n",
    "df[\"display_name\"] = df[\"display_name\"].fillna(df[\"merchant_key\"])\n",
    "\n",
    "print(\"Labels joined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be3dff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Subscription detection (robust, version-proof) ---\n",
    "\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    # Sort and keep only rows with valid date & amount\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # Expenses only\n",
    "    amounts = g[\"amount\"].to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # Inter-payment gaps in DAYS using int64 ns -> days\n",
    "    # (Avoids .dt on object-dtype and works across pandas versions)\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()  # datetime64[ns] -> int ns\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000  # ns per day\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    # Monthly-ish cadence and amount consistency\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    if mu <= 0:\n",
    "        return False\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9))  # coefficient of variation\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Group by display_name; avoid future warning by applying on selected columns only\n",
    "subs_series = (\n",
    "    df.loc[df[\"amount\"] > 0, [\"display_name\", \"date\", \"amount\"]]\n",
    "      .groupby(\"display_name\")[[\"date\", \"amount\"]]\n",
    "      .apply(detect_subscription)            # returns scalar per group\n",
    "      .rename(\"is_subscription\")             # Series: index=display_name\n",
    ")\n",
    "\n",
    "# Map back to the frame (no merge â†’ no duplicate columns)\n",
    "df[\"is_subscription\"] = (\n",
    "    df[\"display_name\"]\n",
    "      .map(subs_series)                      # align by display_name\n",
    "      .astype(\"boolean\")                     # nullable boolean to avoid downcast warning\n",
    "      .fillna(False)\n",
    "      .astype(bool)                          # plain bool for CSV/Power BI\n",
    ")\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e514f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 1\n"
     ]
    }
   ],
   "source": [
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6675fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,746.32 (+2,470.06 vs prior)\n",
      "Top 3 merchants: ALLY PAYMENT ($1,494.22), KOSISONNA UGOCHUKWU ($777.78), PETAL CARD ($738.96)\n",
      "Biggest category driver: Transfers ($2,839.17)\n",
      "\n",
      "Saved digest â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest â†’ {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f17a1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut Transfers by 35%\n",
      "\n",
      "Saved goal nudges â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Suggest % cuts in top categories to reach GOAL_SAVINGS over next 30 days\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges â†’ {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d67e86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new embeddings added (either none missing or AI disabled).\n"
     ]
    }
   ],
   "source": [
    "# Build text field and store embeddings for semantic search\n",
    "def build_search_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"display_name\") or \"\"),\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        str(row.get(\"category\") or \"\"),\n",
    "        str(row.get(\"subcategory\") or \"\"),\n",
    "        str(row.get(\"tags\") or \"\"),\n",
    "    ]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Prepare rows (limit to recent for cost-control)\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df[~embed_df[\"txn_uid\"].isin(existing)][[\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    if client is None:\n",
    "        return [None for _ in texts]\n",
    "    # Use the Azure embeddings deployment name from env\n",
    "    emb_deploy = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")\n",
    "    if not emb_deploy:\n",
    "        return [None for _ in texts]\n",
    "\n",
    "    # New OpenAI client pattern for embeddings under Azure:\n",
    "    # base_url should be resource; we temporarily create a fresh client pointing to embeddings deployment\n",
    "    emb_client = OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{emb_deploy}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "    res = emb_client.embeddings.create(model=emb_deploy, input=list(texts))\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed):\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} â†’ total {len(merged)}\")\n",
    "else:\n",
    "    print(\"No new embeddings added (either none missing or AI disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7420962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched CSV saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "ðŸ“„ Copy saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Reorder and save\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    \"category\",\"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\"is_subscription\",\"is_anomaly\",\"z_by_merchant\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… Enriched CSV saved â†’ {ENRICHED_OUT_PATH}\")\n",
    "print(f\"ðŸ“„ Copy saved â†’ {ENRICHED_COPY_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
