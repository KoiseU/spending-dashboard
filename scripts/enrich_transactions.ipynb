{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 3.283808,
     "end_time": "2025-09-13T23:16:14.042970",
     "exception": false,
     "start_time": "2025-09-13T23:16:10.759162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 1: Imports + robust Azure OpenAI init (original) ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure OpenAI SDK\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# Optional dotenv to load local secrets\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "    find_dotenv = None\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:4] + \"…\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# --- Paths (robust) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW       = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR     = REPO / \"config\"\n",
    "STATE_DIR      = REPO / \".state\"\n",
    "VECTOR_DIR     = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH  = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH    = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH  = DATA_RAW / \"latest.csv\"               # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH        = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH          = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH    = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# Ensure dirs\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config flags\n",
    "MAP_ALL    = True\n",
    "GOAL_SAVINGS = 1000.0\n",
    "ANOMALY_Z  = 2.5\n",
    "\n",
    "# --- Load .envs (mirror the build notebook behavior) ---\n",
    "def load_envs():\n",
    "    if load_dotenv is None:\n",
    "        return\n",
    "    # Explicit override\n",
    "    abs_override = os.getenv(\"ENV_PATH\", str(REPO / \"scripts\" / \".env\"))\n",
    "    if abs_override and Path(abs_override).exists():\n",
    "        try:\n",
    "            load_dotenv(abs_override, override=False, encoding=\"utf-8\")\n",
    "        except TypeError:\n",
    "            load_dotenv(abs_override, override=False)\n",
    "    # Common locations\n",
    "    for p in [\n",
    "        REPO / \"scripts\" / \".env\",\n",
    "        REPO / \".env\",\n",
    "        REPO / \"config\" / \".env\",\n",
    "        cwd / \".env\",\n",
    "    ]:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                load_dotenv(str(p), override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(str(p), override=False)\n",
    "    # Last-ditch: auto-find\n",
    "    if find_dotenv:\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        if found:\n",
    "            try:\n",
    "                load_dotenv(found, override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(found, override=False)\n",
    "\n",
    "load_envs()\n",
    "\n",
    "# --- Azure OpenAI env with fallbacks & normalization (AZURE CLIENT) ---\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT    = (os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"\").strip().rstrip(\"/\")\n",
    "AZURE_OPENAI_API_KEY     = (os.getenv(\"AZURE_OPENAI_API_KEY\")  or \"\").strip()\n",
    "AZURE_OPENAI_DEPLOYMENT  = (os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or \"\").strip()  # chat/completions deployment name\n",
    "AZURE_OPENAI_API_VERSION = (os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-02-15-preview\").strip()\n",
    "\n",
    "def _mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:4] + \"…\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "print(\n",
    "    \"Azure config →\",\n",
    "    \"endpoint:\", _mask(AZURE_OPENAI_ENDPOINT),\n",
    "    \"| key:\", _mask(AZURE_OPENAI_API_KEY),\n",
    "    \"| chat deployment:\", AZURE_OPENAI_DEPLOYMENT or \"<missing>\",\n",
    "    \"| version:\", AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "_missing = [k for k,v in {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": AZURE_OPENAI_ENDPOINT,\n",
    "    \"AZURE_OPENAI_API_KEY\": AZURE_OPENAI_API_KEY,\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": AZURE_OPENAI_DEPLOYMENT,\n",
    "}.items() if not v]\n",
    "if _missing:\n",
    "    raise RuntimeError(\"Azure OpenAI configuration missing: \" + \", \".join(_missing))\n",
    "\n",
    "# ✅ Use AzureOpenAI so the library builds /openai/deployments/{deployment}/... paths\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "print(\"✅ AzureOpenAI client initialized (deployment-aware).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.056261,
     "end_time": "2025-09-13T23:16:14.110812",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.054551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 2: Load latest.csv (original) ---\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# --- Stable txn_uid (prefer Plaid transaction_id) ---\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def make_txn_uid(row):\n",
    "    tid = str(row.get(\"transaction_id\") or \"\").strip()\n",
    "    if tid and tid.lower() != \"nan\":\n",
    "        return tid  # canonical Plaid ID\n",
    "    # Fallback hash when no Plaid ID (include account + normalized date + amount)\n",
    "    dt = pd.to_datetime(row.get(\"date\"), errors=\"coerce\")\n",
    "    parts = [\n",
    "        str(dt.date()) if pd.notna(dt) else \"\",\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        f\"{float(row.get('amount') or 0):.2f}\",\n",
    "        str(row.get(\"bank_name\") or \"\"),\n",
    "        str(row.get(\"account_id\") or \"\"),\n",
    "    ]\n",
    "    return hashlib.sha1(\"|\".join(parts).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "# --- Dedupe by transaction_id, then by txn_uid (prefer posted, then latest date) ---\n",
    "def _dedupe_transactions(df0: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = df0.copy()\n",
    "    g[\"_pending_rank\"] = g.get(\"pending\", True)\n",
    "    g[\"_pending_rank\"] = g[\"_pending_rank\"].fillna(True).astype(bool).astype(int)  # 0=posted, 1=pending\n",
    "    g[\"_date_rank\"] = pd.to_datetime(g.get(\"date\"), errors=\"coerce\")\n",
    "\n",
    "    # 1) If transaction_id exists, dedupe on that first\n",
    "    if \"transaction_id\" in g.columns and g[\"transaction_id\"].notna().any():\n",
    "        g = (\n",
    "            g.sort_values(by=[\"transaction_id\",\"_pending_rank\",\"_date_rank\"],\n",
    "                          ascending=[True, True, True])\n",
    "             .drop_duplicates(subset=[\"transaction_id\"], keep=\"first\")\n",
    "        )\n",
    "\n",
    "    # 2) Safety net: dedupe on txn_uid\n",
    "    g = (\n",
    "        g.sort_values(by=[\"_pending_rank\",\"_date_rank\"], ascending=[True, True])\n",
    "         .drop_duplicates(subset=[\"txn_uid\"], keep=\"first\")\n",
    "    )\n",
    "\n",
    "    return g.drop(columns=[\"_pending_rank\",\"_date_rank\"], errors=\"ignore\")\n",
    "\n",
    "before = len(df)\n",
    "df = _dedupe_transactions(df)\n",
    "print(f\"Dedupe trimmed {before - len(df)} row(s).\")\n",
    "\n",
    "# Global sign convention: True if expenses are negative numbers\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "print(f\"Loaded {len(df)} transactions. expenses_are_negative={EXPENSES_ARE_NEGATIVE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[ENRICH DIAG] category non-null count:\", df[\"category\"].notna().sum())\n",
    "print(df[\"category\"].fillna(\"<<NULL>>\").value_counts().head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.037493,
     "end_time": "2025-09-13T23:16:14.160311",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.122818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 3: Normalize merchant_key (original) ---\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalization for merchant identity:\n",
    "    - Canonicalize brand patterns (AMZN/AMAZON, PAYPAL, SQUARE, APPLE.COM/BILL, GOOGLE*)\n",
    "    - Strip bank noise (POS/DEBIT/CHECK CRD/ACH/ZELLE/TRANSFER/etc.)\n",
    "    - Remove store numbers/digits/punctuation; keep letters, &, spaces, and '/' '.' for brand URLs\n",
    "    - Collapse whitespace; fallback to 'UNKNOWN'\n",
    "    \"\"\"\n",
    "    u = (name or \"\").upper()\n",
    "\n",
    "    # Canonical brand replacements (before stripping)\n",
    "    canon = [\n",
    "        (r\"AMZN\\s+MKTPL?C?E?|AMAZON\\.?\\s*COM\", \"AMAZON\"),\n",
    "        (r\"APPLE\\.?\\s*COM/?BILL\", \"APPLE.COM/BILL\"),\n",
    "        (r\"\\bGOOGLE\\*\", \"GOOGLE \"),\n",
    "        (r\"\\bSQC?\\*\", \"SQUARE \"),\n",
    "        (r\"\\bPAYPAL\\*?\", \"PAYPAL \"),\n",
    "    ]\n",
    "    for pat, repl in canon:\n",
    "        u = re.sub(pat, repl, u)\n",
    "\n",
    "    # Strip common bank/payments noise tokens\n",
    "    noise = [\n",
    "        r\"APPLE PAY ENDING IN \\d{4}\",\n",
    "        r\"POS(?:\\s+PURCHASE)?\",\n",
    "        r\"DEBIT(?:\\s+CARD)?(?:\\s+PURCHASE)?\",\n",
    "        r\"CHECK ?CRD\",\n",
    "        r\"VISA(?:\\s+POS)?\", r\"MASTERCARD\", r\"DISCOVER\", r\"AMEX\",\n",
    "        r\"ACH(?:\\s+(CREDIT|DEBIT))?\", r\"WEB AUTHORIZED PMT\", r\"ONLINE PMT\",\n",
    "        r\"ZELLE(?:\\s+PAYMENT)?\", r\"VENMO(?:\\s+PAYMENT)?\",\n",
    "        r\"XFER\", r\"TRANSFER\",\n",
    "        r\"PURCHASE\", r\"PENDING\", r\"REVERSAL\", r\"ADJ(?:USTMENT)?\",\n",
    "        r\"ID[: ]?\\d+\",\n",
    "    ]\n",
    "    for pat in noise:\n",
    "        u = re.sub(rf\"\\b{pat}\\b\", \" \", u)\n",
    "\n",
    "    # Remove store numbers & digits\n",
    "    u = re.sub(r\"#\\d{2,}\", \" \", u)\n",
    "    u = re.sub(r\"\\d+\", \" \", u)\n",
    "\n",
    "    # Keep letters, '&', spaces, plus '/' '.' for URLish brands; collapse spaces\n",
    "    u = re.sub(r\"[^A-Z&\\s\\./]\", \" \", u)\n",
    "    u = re.sub(r\"\\s+\", \" \", u).strip()\n",
    "\n",
    "    # Post-canon tidy\n",
    "    u = u.replace(\"APPLE COM BILL\", \"APPLE.COM/BILL\").strip()\n",
    "    return u or \"UNKNOWN\"\n",
    "\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    df[\"merchant_name\"].map(merchant_key_from),\n",
    "    df[\"name\"].map(merchant_key_from)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized (consistent with build_latest).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.039783,
     "end_time": "2025-09-13T23:16:14.216681",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.176898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 4: Load or initialize merchant dimension table (original) ---\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.035064,
     "end_time": "2025-09-13T23:16:14.269573",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.234509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 5: Label unmapped merchants via Azure (original) ---\n",
    "new_rows = []\n",
    "if len(unmapped_keys) and ('chat_client' in globals()) and (chat_client is not None) and MAP_ALL:\n",
    "    print(f\"Labeling {len(unmapped_keys)} merchants (single-call mode)...\")\n",
    "    for idx, mk in enumerate(unmapped_keys, 1):\n",
    "        try:\n",
    "            item = azure_label_one(mk)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Label fail for '{mk}': {e}\")\n",
    "            continue\n",
    "\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if item:\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": item[\"display_name\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"subcategory\": item[\"subcategory\"],\n",
    "                \"tags\": \",\".join(item[\"tags\"]),\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        dim_new = pd.DataFrame(new_rows)\n",
    "        dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "        dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "        MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        dim = dim_all\n",
    "        print(f\"✅ Added {len(new_rows)} merchant mappings (single-call).\")\n",
    "    else:\n",
    "        print(\"No new mappings added (single-call).\")\n",
    "else:\n",
    "    print(\"No new mappings needed or AI disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.055568,
     "end_time": "2025-09-13T23:16:14.345195",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.289627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 6: Persist merchants_dim.csv (original) ---\n",
    "# Toggle if you ever want to skip writing on runs with no changes\n",
    "PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "# dim_cols defined in Cell 4; dim may be updated in Cell 6\n",
    "if not isinstance(PERSIST_MERCHANT_DIM, bool):\n",
    "    PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "if PERSIST_MERCHANT_DIM:\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'dim' in globals() and isinstance(dim, pd.DataFrame) and len(dim):\n",
    "        # ensure expected columns/order exist before save\n",
    "        for c in dim_cols:\n",
    "            if c not in dim.columns:\n",
    "                dim[c] = np.nan\n",
    "        dim = dim[dim_cols]\n",
    "\n",
    "        dim.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        print(f\"📝 merchants_dim.csv saved ({len(dim)} rows) → {MERCHANT_DIM_PATH}\")\n",
    "    else:\n",
    "        # either no new mappings this run or dim was empty; ensure file exists\n",
    "        if not MERCHANT_DIM_PATH.exists():\n",
    "            pd.DataFrame(columns=dim_cols).to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            print(f\"📝 Created headers-only merchants_dim.csv → {MERCHANT_DIM_PATH}\")\n",
    "        else:\n",
    "            print(\"ℹ️ merchants_dim.csv already exists; no changes to sync.\")\n",
    "else:\n",
    "    print(\"PERSIST_MERCHANT_DIM=False → skipping merchants_dim.csv persistence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.073115,
     "end_time": "2025-09-13T23:16:14.442487",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.369372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 7 (FIXED): Join labels + build display & category ---\n",
    "\n",
    "# 0) Cache Plaid's original category BEFORE any drops/merges\n",
    "if \"category\" in df.columns:\n",
    "    plaid_category_cached = df[\"category\"].copy()\n",
    "else:\n",
    "    plaid_category_cached = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "\n",
    "# 1) Remove prior label cols from earlier merges (do NOT drop our cached var)\n",
    "df = df.drop(\n",
    "    columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# 2) Merge merchants_dim (right has display_name/category/subcategory/tags)\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# 3) Restore Plaid category safely\n",
    "df[\"category_plaid\"] = plaid_category_cached\n",
    "\n",
    "# 4) Ensure YAML-final columns exist (from build_latest)\n",
    "for m in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\"]:\n",
    "    if m not in df.columns:\n",
    "        df[m] = pd.NA\n",
    "\n",
    "# 5) Rename dim category fields to explicit names (if present)\n",
    "if \"category\" in df.columns and \"category_dim\" not in df.columns:\n",
    "    df = df.rename(columns={\"category\": \"category_dim\"})\n",
    "if \"subcategory\" in df.columns and \"subcategory_dim\" not in df.columns:\n",
    "    df = df.rename(columns={\"subcategory\": \"subcategory_dim\"})\n",
    "\n",
    "# Helper: pick first non-empty/non-NaN string\n",
    "def pick_first_nonblank(row, cols):\n",
    "    for c in cols:\n",
    "        if c in row.index:\n",
    "            v = row[c]\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s and s.lower() not in {\"nan\", \"none\"}:\n",
    "                return s\n",
    "    return \"\"\n",
    "\n",
    "# 6) DISPLAY NAME → dim.display_name → YAML display_name_final → merchant_key\n",
    "df[\"display_name\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"display_name\", \"display_name_final\", \"merchant_key\"]),\n",
    "    axis=1\n",
    ")\n",
    "df.loc[df[\"display_name\"].str.strip().eq(\"\"), \"display_name\"] = df[\"merchant_key\"]\n",
    "\n",
    "# 7) CATEGORY (primary) → dim.category_dim → YAML category_final → Plaid category_plaid\n",
    "df[\"category_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"category_dim\", \"category_final\", \"category_plaid\"]),\n",
    "    axis=1\n",
    ").astype(str).str.strip()\n",
    "df.loc[df[\"category_display\"].eq(\"\") | df[\"category_display\"].str.lower().eq(\"none\"), \"category_display\"] = \"Uncategorized\"\n",
    "\n",
    "# Keep legacy 'category' column in sync for downstream code/Power BI\n",
    "df[\"category\"] = df[\"category_display\"]\n",
    "\n",
    "# 8) SUBCATEGORY/TAGS (same precedence)\n",
    "df[\"subcategory_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"subcategory_dim\", \"subcategory_final\"]),\n",
    "    axis=1\n",
    ")\n",
    "df[\"tags_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"tags\", \"tags_final\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep legacy columns populated for downstream\n",
    "df[\"subcategory\"] = df.get(\"subcategory_display\")\n",
    "df[\"tags\"] = df.get(\"tags_display\")\n",
    "\n",
    "# 9) Ensure required columns exist for save step\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory_display\", \"tags_display\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "print(\"✅ Labels joined. category_display built with precedence: dim → YAML → Plaid.\")\n",
    "print(\"Category sample:\", df[\"category_display\"].value_counts(dropna=False).head(10).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-merge guesses to current df and rebuild display\n",
    "df = df.drop(columns=[\"display_name\",\"category_dim\",\"subcategory_dim\",\"tags\",\"source\",\"confidence\",\"last_updated\"],\n",
    "             errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Helper\n",
    "def pick_first_nonblank(row, cols):\n",
    "    for c in cols:\n",
    "        if c in row.index:\n",
    "            v = row[c]\n",
    "            if pd.isna(v): \n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s and s.lower() not in {\"nan\",\"none\"}:\n",
    "                return s\n",
    "    return \"\"\n",
    "\n",
    "# --- FIX: build display_name then fill empties from merchant_key (no .replace here)\n",
    "df[\"display_name\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"display_name\", \"display_name_final\", \"merchant_key\"]),\n",
    "    axis=1\n",
    ")\n",
    "empty_mask = df[\"display_name\"].astype(str).str.strip().eq(\"\")\n",
    "df.loc[empty_mask, \"display_name\"] = df.loc[empty_mask, \"merchant_key\"]\n",
    "\n",
    "# Explicitly name dim columns if present\n",
    "if \"category\" in df.columns and \"category_dim\" not in df.columns:\n",
    "    df.rename(columns={\"category\":\"category_dim\"}, inplace=True)\n",
    "if \"subcategory\" in df.columns and \"subcategory_dim\" not in df.columns:\n",
    "    df.rename(columns={\"subcategory\":\"subcategory_dim\"}, inplace=True)\n",
    "\n",
    "# category_display precedence: dim → YAML → Plaid, then fallback\n",
    "df[\"category_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"category_dim\",\"category_final\",\"category_plaid\"]),\n",
    "    axis=1\n",
    ").astype(str).str.strip()\n",
    "df.loc[df[\"category_display\"].eq(\"\") | df[\"category_display\"].str.lower().eq(\"none\"), \"category_display\"] = \"Uncategorized\"\n",
    "\n",
    "# Keep legacy 'category' in sync\n",
    "df[\"category\"] = df[\"category_display\"]\n",
    "\n",
    "print(\"[AI Guess] category_display refreshed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.241299,
     "end_time": "2025-09-13T23:16:14.707109",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.465810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 8: Subscription detection (original) ---\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # use absolute spend magnitudes for stability\n",
    "    amounts = g[\"amount\"].abs().to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # gaps in days\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9)) if mu > 0 else 1.0\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Clean any leftover artifacts from previous runs (e.g., is_subscription_x from merges)\n",
    "for col in [c for c in df.columns if c.startswith(\"is_subscription\") and c != \"is_subscription\"]:\n",
    "    df.drop(columns=col, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Respect your sign convention\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "if EXPENSES_ARE_NEGATIVE:\n",
    "    outflows = df.loc[(df[\"amount\"] < 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "    outflows[\"amount\"] = outflows[\"amount\"].abs()\n",
    "else:\n",
    "    outflows = df.loc[(df[\"amount\"] > 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "\n",
    "subs_map = {}\n",
    "for disp, g in outflows.groupby(\"display_name\", dropna=False):\n",
    "    try:\n",
    "        subs_map[disp] = bool(detect_subscription(g[[\"date\", \"amount\"]]))\n",
    "    except Exception:\n",
    "        subs_map[disp] = False\n",
    "\n",
    "df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.065294,
     "end_time": "2025-09-13T23:16:14.794753",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.729459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 9: Anomaly detection (original) ---\n",
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 0.08206,
     "end_time": "2025-09-13T23:16:14.898637",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.816577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 10: 30-day digest (original) ---\n",
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest → {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.059493,
     "end_time": "2025-09-13T23:16:14.989894",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.930401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 11: Goal nudges (original) ---\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges → {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": 0.073422,
     "end_time": "2025-09-13T23:16:15.088652",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.015230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 12 (UPDATED): Embeddings cache (NA-safe) ---\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "EMBED_MODEL = (os.getenv(\"AZURE_OPENAI_EMBEDDINGS\") or os.getenv(\"OPENAI_EMBEDDINGS_DEPLOYMENT\") or \"\").strip()\n",
    "EMBED_ENABLED = bool(EMBED_MODEL)\n",
    "print(\"EMBED_MODEL:\", EMBED_MODEL or \"<disabled>\")\n",
    "\n",
    "def safe_str(v) -> str:\n",
    "    \"\"\"Return '' for None/NaN/pd.NA/'nan'/'None', else a clean string.\"\"\"\n",
    "    try:\n",
    "        if pd.isna(v):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(v)\n",
    "    return \"\" if s.strip().lower() in {\"nan\", \"none\"} else s\n",
    "\n",
    "def build_search_text(row: pd.Series) -> str:\n",
    "    # Prefer 'category_display' if you created it; fallback to 'category'\n",
    "    cat_col = \"category_display\" if \"category_display\" in row.index else \"category\"\n",
    "    fields = [\"display_name\", \"name\", \"merchant_name\", cat_col, \"subcategory\", \"tags\"]\n",
    "    parts = [safe_str(row.get(f)) for f in fields]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Limit to recent rows for cost control\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache (parquet with list column is fine under pyarrow)\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    try:\n",
    "        old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "        if \"txn_uid\" not in old.columns or \"embedding\" not in old.columns:\n",
    "            old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "    except Exception:\n",
    "        old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df.loc[~embed_df[\"txn_uid\"].isin(existing), [\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "# Azure embeddings config (enabled only if a deployment name is set)\n",
    "EMBED_MODEL = (os.getenv(\"AZURE_OPENAI_EMBEDDINGS\") or os.getenv(\"OPENAI_EMBEDDINGS_DEPLOYMENT\") or \"\").strip()\n",
    "EMBED_ENABLED = bool(EMBED_MODEL)\n",
    "\n",
    "def get_embeddings(texts: list[str]):\n",
    "    if not EMBED_ENABLED:\n",
    "        return None\n",
    "    # Use the same Azure OpenAI client; model is your embeddings deployment name\n",
    "    res = client.embeddings.create(model=EMBED_MODEL, input=list(texts))\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed) and EMBED_ENABLED:\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        if vecs is None:\n",
    "            break\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} → total {len(merged)}\")\n",
    "else:\n",
    "    msg = \"Embeddings disabled (no AZURE_OPENAI_EMBEDDINGS)\" if not EMBED_ENABLED else \"No new embeddings needed\"\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.104623,
     "end_time": "2025-09-13T23:16:15.221004",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.116381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 13 (REPLACE): Reorder and save ---\n",
    "\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    # Keep BOTH the coalesced and raw category columns\n",
    "    \"category\",               # <- coalesced (dim → yaml → plaid)\n",
    "    \"category_display\",       # alias of coalesced for clarity in BI\n",
    "    \"category_plaid\",         # original Plaid category\n",
    "    \"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\n",
    "    # analytics flags\n",
    "    \"is_subscription\",\"is_anomaly\",\"z_by_merchant\",\n",
    "    # optional flow flag if present\n",
    "    \"is_non_spend_flow\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Enriched CSV saved → {ENRICHED_OUT_PATH}\")\n",
    "print(f\"📄 Copy saved → {ENRICHED_COPY_PATH}\")\n",
    "print(\"Column sanity (first 12):\", list(df_out.columns)[:12])\n",
    "print(\"Nulls check — category:\", int(df_out['category'].isna().sum()),\n",
    "      \"| category_display:\", int(df_out['category_display'].isna().sum()),\n",
    "      \"| category_plaid:\", int(df_out['category_plaid'].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell DQ: Data Quality gate (print + write MD; optionally fail CI) ---\n",
    "\n",
    "import os, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DQ_PATH_MD   = INSIGHTS_DIR / \"dq_report.md\"\n",
    "\n",
    "# ----- Config (env-driven) -----\n",
    "FAIL_ON_ERROR   = (os.getenv(\"DQ_FAIL_ON_ERROR\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n",
    "UNCAT_WARN_PCT  = float(os.getenv(\"DQ_UNCATEGORIZED_WARN_PCT\", \"0.50\"))\n",
    "MAX_FUTURE_DAYS = int(os.getenv(\"DQ_MAX_FUTURE_DAYS\", \"1\"))\n",
    "MIN_ROWS        = int(os.getenv(\"DQ_MIN_ROWS\", \"10\"))\n",
    "\n",
    "# Use the most complete frame available (df_out after Cell 13; otherwise df)\n",
    "dq = df_out.copy() if 'df_out' in globals() else df.copy()\n",
    "\n",
    "now = pd.Timestamp.now().normalize()\n",
    "future_cutoff = now + pd.Timedelta(days=MAX_FUTURE_DAYS)\n",
    "\n",
    "def pick_cat_col(frame: pd.DataFrame) -> str | None:\n",
    "    for c in [\"category_display\",\"category\",\"category_final\",\"category_plaid\",\"category_plaid_legacy\"]:\n",
    "        if c in frame.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "cat_col = pick_cat_col(dq)\n",
    "\n",
    "def is_uncat(x) -> bool:\n",
    "    if pd.isna(x): return True\n",
    "    s = str(x).strip().lower()\n",
    "    return (s == \"\") or (s in {\"uncategorized\",\"none\",\"nan\"})\n",
    "\n",
    "# ----- Core metrics -----\n",
    "n_rows = int(len(dq))\n",
    "\n",
    "# Date checks\n",
    "if \"date\" in dq.columns and n_rows:\n",
    "    n_date_null = int(dq[\"date\"].isna().sum())\n",
    "    n_future = int((dq[\"date\"] > future_cutoff).sum())\n",
    "    min_date = str(pd.to_datetime(dq[\"date\"]).min().date()) if dq[\"date\"].notna().any() else \"n/a\"\n",
    "    max_date = str(pd.to_datetime(dq[\"date\"]).max().date()) if dq[\"date\"].notna().any() else \"n/a\"\n",
    "else:\n",
    "    n_date_null, n_future, min_date, max_date = n_rows, 0, \"n/a\", \"n/a\"\n",
    "\n",
    "# Amount checks (coerce to numeric first)\n",
    "neg_share = None\n",
    "n_amt_null = n_amt_inf = n_amt_nonfinite = 0\n",
    "if \"amount\" in dq.columns and n_rows:\n",
    "    amt_num = pd.to_numeric(dq[\"amount\"], errors=\"coerce\")\n",
    "    n_amt_null = int(amt_num.isna().sum())\n",
    "    n_amt_inf  = int(np.isinf(amt_num.fillna(0)).sum())   # ±inf count\n",
    "    n_amt_nonfinite = n_amt_null + n_amt_inf              # total non-finite (NaN or ±inf)\n",
    "    neg_share = float((amt_num < 0).mean()) if len(amt_num) else None\n",
    "\n",
    "# Duplicates\n",
    "n_dup_txn_uid = int(dq[\"txn_uid\"].duplicated(keep=False).sum()) if \"txn_uid\" in dq.columns else 0\n",
    "n_dup_txn_id  = int(dq[\"transaction_id\"].fillna(\"\").duplicated(keep=False).sum()) if \"transaction_id\" in dq.columns else 0\n",
    "\n",
    "# Categorization snapshot\n",
    "uncat_share = uncat_count = None\n",
    "if cat_col:\n",
    "    uncat_mask = dq[cat_col].apply(is_uncat)\n",
    "    uncat_count = int(uncat_mask.sum())\n",
    "    uncat_share = float(uncat_count) / float(n_rows or 1)\n",
    "\n",
    "# Top categories (by $ amount)\n",
    "top_cats_txt = \"\"\n",
    "if cat_col and n_rows:\n",
    "    top_cats = (dq[~dq[cat_col].apply(is_uncat)]\n",
    "                .groupby(cat_col, dropna=False)[\"amount\"]\n",
    "                .sum()\n",
    "                .sort_values(ascending=False)\n",
    "                .head(5))\n",
    "    if len(top_cats):\n",
    "        top_cats_txt = \", \".join([f\"{k}: ${float(v):,.0f}\" for k,v in top_cats.items()])\n",
    "\n",
    "# ----- Findings list -----\n",
    "findings = []  # (level, code, message)\n",
    "def add(level, code, msg): findings.append((level, code, msg))\n",
    "\n",
    "# Size\n",
    "if n_rows < MIN_ROWS:\n",
    "    add(\"ERROR\", \"FEW_ROWS\", f\"Only {n_rows} rows (min required {MIN_ROWS}).\")\n",
    "else:\n",
    "    add(\"INFO\", \"ROWS_OK\", f\"Row count: {n_rows}.\")\n",
    "\n",
    "# Dates\n",
    "if \"date\" not in dq.columns:\n",
    "    add(\"ERROR\", \"NO_DATE\", \"Missing 'date' column.\")\n",
    "else:\n",
    "    if n_date_null > 0:\n",
    "        add(\"ERROR\", \"DATE_NULLS\", f\"{n_date_null} rows have null dates.\")\n",
    "    if n_future > 0:\n",
    "        add(\"ERROR\", \"FUTURE_DATES\", f\"{n_future} rows are later than {future_cutoff.date()} (MAX_FUTURE_DAYS={MAX_FUTURE_DAYS}).\")\n",
    "    add(\"INFO\", \"DATE_RANGE\", f\"Date range: {min_date} → {max_date}.\")\n",
    "\n",
    "# Amounts\n",
    "if \"amount\" not in dq.columns:\n",
    "    add(\"ERROR\", \"NO_AMOUNT\", \"Missing 'amount' column.\")\n",
    "else:\n",
    "    if n_amt_null > 0:\n",
    "        add(\"ERROR\", \"AMOUNT_NULLS\", f\"{n_amt_null} rows have null amounts.\")\n",
    "    if n_amt_inf > 0:\n",
    "        add(\"ERROR\", \"AMOUNT_INF\", f\"{n_amt_inf} rows have infinite amounts (±inf).\")\n",
    "    add(\"INFO\", \"AMOUNT_NONFINITE\", f\"Total non-finite (NaN or ±inf): {n_amt_nonfinite}.\")\n",
    "    add(\"INFO\", \"SIGN_SPLIT\", f\"Share negative amounts: {0 if neg_share is None else round(neg_share*100,1)}%.\")\n",
    "\n",
    "# Duplicates\n",
    "if \"txn_uid\" in dq.columns and n_dup_txn_uid > 0:\n",
    "    add(\"WARN\", \"DUP_TXN_UID\", f\"{n_dup_txn_uid} rows share duplicate txn_uid values.\")\n",
    "if \"transaction_id\" in dq.columns and dq[\"transaction_id\"].notna().any() and n_dup_txn_id > 0:\n",
    "    add(\"WARN\", \"DUP_TRANSACTION_ID\", f\"{n_dup_txn_id} rows share duplicate Plaid transaction_id values.\")\n",
    "\n",
    "# Categorization\n",
    "if cat_col:\n",
    "    add(\"INFO\", \"CATEGORY_COL\", f\"Using category column: {cat_col}.\")\n",
    "    if uncat_share is not None:\n",
    "        if uncat_share >= UNCAT_WARN_PCT:\n",
    "            add(\"WARN\", \"UNCAT_HIGH\", f\"Uncategorized {uncat_share:.1%} (threshold {UNCAT_WARN_PCT:.0%}).\")\n",
    "        else:\n",
    "            add(\"INFO\", \"UNCAT_OK\", f\"Uncategorized {uncat_share:.1%}.\")\n",
    "    if top_cats_txt:\n",
    "        add(\"INFO\", \"TOP_CATS\", f\"Top categories (by $): {top_cats_txt}\")\n",
    "else:\n",
    "    add(\"WARN\", \"NO_CATEGORY_COL\", \"No category column found (category/category_display/etc.).\")\n",
    "\n",
    "# YAML/source coverage (optional)\n",
    "if \"source_final\" in dq.columns:\n",
    "    vc = dq[\"source_final\"].fillna(\"raw\").value_counts().to_dict()\n",
    "    add(\"INFO\", \"SOURCE_MIX\", \"source_final mix: \" + \", \".join(f\"{k}={v}\" for k,v in vc.items()))\n",
    "\n",
    "# ----- Render Markdown -----\n",
    "buf = io.StringIO()\n",
    "buf.write(\"# Data Quality Report\\n\\n\")\n",
    "buf.write(f\"- Rows: **{n_rows}**\\n\")\n",
    "buf.write(f\"- Date range: **{min_date} → {max_date}**\\n\")\n",
    "if uncat_share is not None:\n",
    "    buf.write(f\"- Uncategorized: **{uncat_share:.1%}** (threshold {UNCAT_WARN_PCT:.0%})\\n\")\n",
    "buf.write(f\"- Fail on error: **{FAIL_ON_ERROR}**\\n\")\n",
    "buf.write(\"\\n---\\n\\n\")\n",
    "\n",
    "def as_line(lvl, code, msg):\n",
    "    badge = {\"ERROR\":\"🛑\",\"WARN\":\"⚠️\",\"INFO\":\"ℹ️\"}.get(lvl, \"•\")\n",
    "    return f\"- {badge} **{lvl}** `{code}` — {msg}\"\n",
    "\n",
    "for lvl in (\"ERROR\",\"WARN\",\"INFO\"):\n",
    "    rows = [as_line(L, C, M) for (L,C,M) in findings if L == lvl]\n",
    "    if rows:\n",
    "        buf.write(f\"## {lvl}S\\n\\n\")\n",
    "        buf.write(\"\\n\".join(rows) + \"\\n\\n\")\n",
    "\n",
    "Path(DQ_PATH_MD).write_text(buf.getvalue(), encoding=\"utf-8\")\n",
    "\n",
    "# Console summary\n",
    "errs = [f for f in findings if f[0] == \"ERROR\"]\n",
    "warns = [f for f in findings if f[0] == \"WARN\"]\n",
    "print(\"\\n=== DATA QUALITY REPORT ===\")\n",
    "print(f\"Rows={n_rows} | Errors={len(errs)} Warnings={len(warns)}\")\n",
    "for L,C,M in findings:\n",
    "    print(f\"[{L}] {C}: {M}\")\n",
    "print(f\"\\nWrote DQ markdown → {DQ_PATH_MD}\")\n",
    "\n",
    "# Optionally fail the run in CI\n",
    "if FAIL_ON_ERROR and len(errs) > 0:\n",
    "    raise SystemExit(\"DQ_FAIL_ON_ERROR=1 and one or more ERRORs were found. See dq_report.md.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.787872,
     "end_time": "2025-09-13T23:16:16.031924",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.244052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 14 (UPDATED): Weekly Executive Digest (WoW + MoM), AI + HTML/MD/CSV ---\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIGEST_JSON   = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "DIGEST_MD     = INSIGHTS_DIR / \"digest_latest.md\"\n",
    "DIGEST_FLAT   = INSIGHTS_DIR / \"digest_latest_flat.csv\"\n",
    "EMAIL_HTML    = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "EMAIL_SUBJECT = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "\n",
    "# -------- 1) Last COMPLETED week (Mon–Sun), compare WoW --------\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wd = int(now.weekday())  # Mon=0 ... Sun=6\n",
    "days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()    # prior Monday\n",
    "prev_end = (pd.Timestamp(wk_end) - pd.Timedelta(days=7)).date()\n",
    "prev_start = (pd.Timestamp(prev_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "def _short_range(ws, we):\n",
    "    try:\n",
    "        ws_dt = pd.to_datetime(ws).date(); we_dt = pd.to_datetime(we).date()\n",
    "        return f\"{ws_dt.month}/{ws_dt.day} - {we_dt.month}/{we_dt.day}\"\n",
    "    except Exception:\n",
    "        return f\"{ws} - {we}\"\n",
    "\n",
    "# -------- 1b) Exclusions: Wealthfront moves are not spend/income; Apple Cash stays --------\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "# Normalize candidate category columns (don’t create if missing)\n",
    "for col in (\"category_display\",\"category\",\"category_final\",\"category_plaid\"):\n",
    "    if col in base.columns:\n",
    "        s = base[col].astype(str)\n",
    "        base[col] = s.where(~s.str.strip().isin([\"\", \"nan\", \"None\"]), np.nan)\n",
    "\n",
    "def _best_category_col(frame: pd.DataFrame) -> str | None:\n",
    "    candidates = [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]\n",
    "    for c in candidates:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Window slices\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "cur_w  = base[(base[\"date_only\"] >= wk_start) & (base[\"date_only\"] <= wk_end)]\n",
    "prev_w = base[(base[\"date_only\"] >= prev_start) & (base[\"date_only\"] <= prev_end)]\n",
    "\n",
    "# -------- 2) Robust sign detection --------\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "def spend_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a < 0].abs().sum()) if expenses_are_negative else float(a[a > 0].sum())\n",
    "\n",
    "def income_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a > 0].sum()) if expenses_are_negative else float(a[a < 0].abs().sum())\n",
    "\n",
    "# WoW\n",
    "cur_spend   = round(spend_sum(cur_w), 2)\n",
    "prev_spend  = round(spend_sum(prev_w), 2)\n",
    "cur_income  = round(income_sum(cur_w), 2)\n",
    "prev_income = round(income_sum(prev_w), 2)\n",
    "\n",
    "spend_delta     = round(cur_spend - prev_spend, 2)\n",
    "spend_delta_pct = round((spend_delta / prev_spend), 4) if prev_spend else (1.0 if cur_spend else 0.0)\n",
    "\n",
    "# -------- 3) MoM (MTD vs aligned days in prior month) --------\n",
    "cur_month_start = pd.Timestamp(wk_end).to_period('M').start_time.date()\n",
    "cur_mtd_end     = wk_end\n",
    "prev_month = (pd.Timestamp(wk_end).to_period('M') - 1)\n",
    "prev_month_start = prev_month.start_time.date()\n",
    "prev_month_end   = prev_month.end_time.date()\n",
    "days_into_m = (pd.Timestamp(cur_mtd_end) - pd.Timestamp(cur_month_start)).days\n",
    "aligned_prev_m_end = (pd.Timestamp(prev_month_start) + pd.Timedelta(days=days_into_m)).date()\n",
    "if aligned_prev_m_end > prev_month_end:\n",
    "    aligned_prev_m_end = prev_month_end\n",
    "\n",
    "cur_m = base[(base[\"date_only\"] >= cur_month_start) & (base[\"date_only\"] <= cur_mtd_end)]\n",
    "prev_m_aligned = base[(base[\"date_only\"] >= prev_month_start) & (base[\"date_only\"] <= aligned_prev_m_end)]\n",
    "cur_spend_m   = round(spend_sum(cur_m), 2)\n",
    "prev_spend_m  = round(spend_sum(prev_m_aligned), 2)\n",
    "cur_income_m  = round(income_sum(cur_m), 2)\n",
    "prev_income_m = round(income_sum(prev_m_aligned), 2)\n",
    "spend_delta_m     = round(cur_spend_m - prev_spend_m, 2)\n",
    "spend_delta_pct_m = round((spend_delta_m / prev_spend_m), 4) if prev_spend_m else (1.0 if cur_spend_m else 0.0)\n",
    "\n",
    "# -------- 4) Top drivers (category) --------\n",
    "if expenses_are_negative:\n",
    "    cur_exp = cur_w[cur_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp = cur_w[cur_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "CAT_COL = _best_category_col(cur_exp)\n",
    "if CAT_COL is None:\n",
    "    CAT_COL = \"category_display\"\n",
    "    cur_exp[CAT_COL] = np.nan\n",
    "\n",
    "top_cats_cur = (\n",
    "    cur_exp.groupby(CAT_COL, dropna=False)[\"spend\"]\n",
    "           .sum().sort_values(ascending=False).head(5)\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "def _label(v):\n",
    "    return \"Uncategorized\" if (pd.isna(v) or str(v).strip() in {\"\", \"nan\", \"None\"}) else str(v)\n",
    "\n",
    "if CAT_COL in top_cats_cur.columns:\n",
    "    top_cats_cur[CAT_COL] = top_cats_cur[CAT_COL].apply(_label)\n",
    "\n",
    "# DEBUG — confirm which column was used and a peek at results\n",
    "print(f\"[Weekly] Category column used: {CAT_COL} | non-null in window: {int(cur_exp[CAT_COL].notna().sum())}\")\n",
    "print(top_cats_cur.head(5).to_string(index=False))\n",
    "\n",
    "subs_w  = cur_w.loc[cur_w.get(\"is_subscription\", False) == True]\n",
    "anoms_w = cur_w.loc[cur_w.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "# -------- 5) Payload for AI --------\n",
    "summary_payload = {\n",
    "    \"as_of_date\": pd.Timestamp(wk_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(wk_start), \"end\": str(wk_end), \"label\": \"Last completed week (Mon–Sun)\"},\n",
    "        \"previous\": {\"start\": str(prev_start), \"end\": str(prev_end)}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        # WoW\n",
    "        \"spend_current\": cur_spend,\n",
    "        \"spend_previous\": prev_spend,\n",
    "        \"spend_delta\": spend_delta,\n",
    "        \"spend_delta_pct\": spend_delta_pct,\n",
    "        \"income_current\": cur_income,\n",
    "        \"income_previous\": prev_income,\n",
    "        # MoM (MTD vs aligned prior month MTD)\n",
    "        \"spend_mtd_current\": cur_spend_m,\n",
    "        \"spend_mtd_previous\": prev_spend_m,\n",
    "        \"spend_mtd_delta\": spend_delta_m,\n",
    "        \"spend_mtd_delta_pct\": spend_delta_pct_m,\n",
    "        \"income_mtd_current\": cur_income_m,\n",
    "        \"income_mtd_previous\": prev_income_m,\n",
    "    },\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[CAT_COL]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_cur.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_w[\"display_name\"].nunique()) if len(subs_w) else 0,\n",
    "    \"anomalies_count\": int(anoms_w.shape[0]) if len(anoms_w) else 0,\n",
    "}\n",
    "\n",
    "# -------- 6) Azure summarizer (REQUIRED) with theme + narrative --------\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided aggregates (week-over-week and month-over-month), \"\n",
    "    \"produce an executive digest in STRICT JSON. No invented numbers.\"\n",
    ")\n",
    "USER_INSTRUCTIONS = (\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"theme\": string,\\n'\n",
    "    '  \"narrative\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ],\\n'\n",
    "    '  \"email_subject\": string\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct where spend improved.\\n\"\n",
    "    \"- impact_usd is a rough weekly savings estimate.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call(payload_json: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.15,\n",
    "        max_tokens=750,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "raw = _azure_digest_call(json.dumps(summary_payload))\n",
    "azure_digest = _salvage_json_object(raw)\n",
    "if not isinstance(azure_digest, dict):\n",
    "    raise RuntimeError(\"Azure summarizer returned no valid JSON. Check Azure env, deployment name, or quota.\")\n",
    "\n",
    "# -------- 7) Overlay (protect authoritative totals/window) + sanitize theme --------\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in {\"totals\", \"window\"}:\n",
    "            continue\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "short_rng = _short_range(wk_start, wk_end)\n",
    "\n",
    "base_digest = {\n",
    "    \"insights_version\": 3,\n",
    "    \"window\": summary_payload[\"window\"],\n",
    "    \"totals\": summary_payload[\"totals\"],\n",
    "    \"headline\": f\"Weekly digest {short_rng}\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Total Spend (WoW)\",  \"value\": cur_spend,  \"delta_pct\": spend_delta_pct},\n",
    "        {\"name\":\"Total Income (WoW)\",\"value\": cur_income, \"delta_pct\": None},\n",
    "        {\"name\":\"Total Spend (MoM)\",  \"value\": cur_spend_m,\"delta_pct\": spend_delta_pct_m},\n",
    "        {\"name\":\"Total Income (MoM)\",\"value\": cur_income_m,\"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": _label(t[\"category\"]), \"spend\": float(t[\"spend\"])} for t in summary_payload[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload['subscriptions_count']} active subs this week\"}] if summary_payload[\"subscriptions_count\"] else []) +\n",
    "        ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload['anomalies_count']} anomalies this week\"}] if summary_payload[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": [],\n",
    "    \"theme\": \"\",\n",
    "    \"narrative\": \"\",\n",
    "    \"email_subject\": \"\"\n",
    "}\n",
    "\n",
    "digest = _overlay(base_digest, azure_digest)\n",
    "\n",
    "# Clamp theme to 3 words max; fallback if empty\n",
    "if not isinstance(digest.get(\"theme\",\"\"), str) or not digest[\"theme\"].strip():\n",
    "    digest[\"theme\"] = \"Lean Week\" if spend_delta < 0 else \"Heavier Week\"\n",
    "else:\n",
    "    words = [w for w in re.split(r\"\\s+\", digest[\"theme\"].strip()) if w]\n",
    "    digest[\"theme\"] = \" \".join(words[:3])\n",
    "\n",
    "# -------- 8) Compact summary line for tiles --------\n",
    "def build_compact_summary(d: dict) -> str:\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "    totals = d.get(\"totals\", {}) or {}\n",
    "    spend_val = float(totals.get(\"spend_current\") or 0.0)\n",
    "    dp = totals.get(\"spend_delta_pct\")\n",
    "    if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp / 100.0\n",
    "    dp_txt = f\"{dp*100:+.1f}%\" if isinstance(dp, (int,float)) else \"n/a\"\n",
    "    drivers = d.get(\"top_drivers\") or []\n",
    "    if drivers:\n",
    "        label = (drivers[0].get(\"label\") or \"Uncategorized\").strip()\n",
    "        amt = float(drivers[0].get(\"spend\") or 0.0)\n",
    "        driver_txt = f\"Top driver: {label} (${amt:,.0f})\"\n",
    "    else:\n",
    "        driver_txt = \"Top driver: n/a\"\n",
    "    return f\"{short}: Weekly spend ${spend_val:,.0f} (WoW {dp_txt}). {driver_txt}.\"\n",
    "\n",
    "digest[\"summary\"] = build_compact_summary(digest)\n",
    "\n",
    "# -------- 9) Persist JSON --------\n",
    "with open(DIGEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# -------- 10) Markdown (employer-ready) --------\n",
    "def render_md(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "\n",
    "    lines = [f\"# Weekly Executive Summary ({short})\"]\n",
    "    if d.get(\"theme\"):\n",
    "        lines.append(f\"*{d['theme']}*\")\n",
    "    if d.get(\"narrative\"):\n",
    "        lines.append(f\"\\n{d['narrative'].strip()}\\n\")\n",
    "    else:\n",
    "        lines.append(f\"\\n{d['summary']}\\n\")\n",
    "\n",
    "    # Key Metrics\n",
    "    lines.append(\"## Key Metrics\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        lines.append(f\"- **{name}:** ${val:,.2f}{dp_txt}\")\n",
    "\n",
    "    # Drivers\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        lines.append(\"\\n## Drivers\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            lines.append(f\"- **{label}:** ${float(t.get('spend',0)):,.0f}\")\n",
    "\n",
    "    # Risks\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        lines.append(\"\\n## Risks\")\n",
    "        for r in rk[:5]:\n",
    "            lines.append(f\"- **{r.get('type','note')}:** {r.get('note','')}\")\n",
    "\n",
    "    # Recommendations\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        lines.append(\"\\n## Recommendations\")\n",
    "        for a in ai[:5]:\n",
    "            lines.append(f\"- **{a.get('title','')}** — est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(DIGEST_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md(digest))\n",
    "\n",
    "# -------- 11) Email subject + HTML (clean sections) --------\n",
    "subject = digest.get(\"email_subject\") or f\"{digest.get('theme','Weekly Digest')} — {short_rng}\"\n",
    "with open(EMAIL_SUBJECT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(subject.strip())\n",
    "\n",
    "def render_email_html(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "    theme = (d.get(\"theme\") or \"\").strip()\n",
    "    narrative = (d.get(\"narrative\") or d.get(\"summary\") or \"\").strip()\n",
    "\n",
    "    parts = []\n",
    "    parts.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "    parts.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "    parts.append(f\"<h1 style='margin:0 0 4px 0;font-size:18px;'>Weekly Executive Summary ({short})</h1>\")\n",
    "    if theme:\n",
    "        parts.append(f\"<div style='margin:0 0 12px 0;color:#6b7280;font-style:italic'>{theme}</div>\")\n",
    "    if narrative:\n",
    "        parts.append(f\"<p style='margin:0 0 16px 0'>{narrative}</p>\")\n",
    "\n",
    "    parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Key Metrics</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        parts.append(f\"<li><b>{name}:</b> ${val:,.2f}{dp_txt}</li>\")\n",
    "    parts.append(\"</ul>\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Drivers</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            parts.append(f\"<li><b>{label}:</b> ${float(t.get('spend',0)):,.0f}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Risks</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for r in rk[:5]:\n",
    "            parts.append(f\"<li><b>{r.get('type','note')}:</b> {r.get('note','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Recommendations</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for a in ai[:5]:\n",
    "            parts.append(f\"<li><b>{a.get('title','')}</b> — est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    parts.append(\"</div>\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "with open(EMAIL_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_email_html(digest))\n",
    "\n",
    "# -------- 12) Flat CSV for Power BI (add MoM metrics too) --------\n",
    "flat_rows = []\n",
    "flat_rows.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(wk_end),\n",
    "    \"cur_start\": str(wk_start),\n",
    "    \"cur_end\": str(wk_end),\n",
    "    \"prev_start\": str(prev_start),\n",
    "    \"prev_end\": str(prev_end),\n",
    "    \"headline\": digest.get(\"headline\",\"\"),\n",
    "    \"summary\": digest.get(\"summary\",\"\"),\n",
    "    \"name\": \"Total Spend (WoW)\",\n",
    "    \"value\": float(cur_spend),\n",
    "    \"delta_pct\": float(spend_delta_pct),\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "for m in digest.get(\"key_metrics\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for t in digest.get(\"top_drivers\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": (t.get(\"label\") or \"Uncategorized\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for r in digest.get(\"risks\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "pd.DataFrame(flat_rows).to_csv(DIGEST_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"🧠 Weekly executive digest written:\\n\"\n",
    "    f\"- JSON: {DIGEST_JSON}\\n- MD:   {DIGEST_MD}\\n- HTML: {EMAIL_HTML}\\n- CSV:  {DIGEST_FLAT}\\n- Subject: {EMAIL_SUBJECT}\\n\"\n",
    "    f\"Week Window: {wk_start} -> {wk_end} | Prev: {prev_start} -> {prev_end}\\n\"\n",
    "    f\"WoW Spend: cur={cur_spend} prev={prev_spend} delta={spend_delta} delta_pct={spend_delta_pct:+.4f}\\n\"\n",
    "    f\"MoM MTD Spend: cur={cur_spend_m} prev={prev_spend_m} delta={spend_delta_m} delta_pct={spend_delta_pct_m:+.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "papermill": {
     "duration": 0.172408,
     "end_time": "2025-09-13T23:16:16.231678",
     "exception": false,
     "start_time": "2025-09-13T23:16:16.059270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- enrich_transactions.ipynb — Cell 15 (UPDATED): Monthly Executive Digest (MTD vs prior MTD), AI + HTML/MD/CSV ---\n",
    "import os, re, json, calendar\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MOM_JSON   = INSIGHTS_DIR / \"digest_mom.json\"\n",
    "MOM_MD     = INSIGHTS_DIR / \"digest_mom.md\"\n",
    "MOM_FLAT   = INSIGHTS_DIR / \"digest_mom_flat.csv\"\n",
    "MOM_HTML   = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "MOM_SUBJ   = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# --- 1) Date windows (align with Cell 14; use LA time) ---\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wk_wd = int(now.weekday())\n",
    "days_to_last_sun = 7 if wk_wd == 6 else (wk_wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "cur_month_start = pd.Timestamp(wk_end).to_period('M').start_time.date()\n",
    "cur_mtd_end     = wk_end\n",
    "\n",
    "prev_month = (pd.Timestamp(wk_end).to_period('M') - 1)\n",
    "prev_month_start = prev_month.start_time.date()\n",
    "prev_month_end   = prev_month.end_time.date()\n",
    "\n",
    "# Align prior-month MTD to the same number of days as current MTD\n",
    "days_into_m = (pd.Timestamp(cur_mtd_end) - pd.Timestamp(cur_month_start)).days\n",
    "aligned_prev_m_end = (pd.Timestamp(prev_month_start) + pd.Timedelta(days=days_into_m)).date()\n",
    "if aligned_prev_m_end > prev_month_end:\n",
    "    aligned_prev_m_end = prev_month_end\n",
    "\n",
    "cur_month_name  = calendar.month_name[pd.to_datetime(cur_mtd_end).month]\n",
    "prev_month_name = calendar.month_name[pd.to_datetime(prev_month_start).month]\n",
    "\n",
    "# --- 2) Exclusions: Wealthfront out, Apple Cash in (mirror Cell 14) ---\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "# Normalize candidate category columns\n",
    "for col in (\"category_display\",\"category\",\"category_final\",\"category_plaid\"):\n",
    "    if col in base.columns:\n",
    "        s = base[col].astype(str)\n",
    "        base[col] = s.where(~s.str.strip().isin([\"\", \"nan\", \"None\"]), np.nan)\n",
    "\n",
    "def _best_category_col(frame: pd.DataFrame) -> str | None:\n",
    "    candidates = [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]\n",
    "    for c in candidates:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "cur_m  = base[(base[\"date_only\"] >= cur_month_start) & (base[\"date_only\"] <= cur_mtd_end)]\n",
    "prev_m = base[(base[\"date_only\"] >= prev_month_start) & (base[\"date_only\"] <= aligned_prev_m_end)]\n",
    "\n",
    "# --- 3) Polarity + totals ---\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "def spend_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a < 0].abs().sum()) if expenses_are_negative else float(a[a > 0].sum())\n",
    "\n",
    "def income_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a > 0].sum()) if expenses_are_negative else float(a[a < 0].abs().sum())\n",
    "\n",
    "cur_spend_m   = round(spend_sum(cur_m), 2)\n",
    "prev_spend_m  = round(spend_sum(prev_m), 2)\n",
    "cur_income_m  = round(income_sum(cur_m), 2)\n",
    "prev_income_m = round(income_sum(prev_m), 2)\n",
    "\n",
    "spend_delta_m     = round(cur_spend_m - prev_spend_m, 2)\n",
    "spend_delta_pct_m = round((spend_delta_m / prev_spend_m), 4) if prev_spend_m else (1.0 if cur_spend_m else 0.0)\n",
    "\n",
    "# --- 4) Top drivers this month (category) ---\n",
    "CAT_COL = _best_category_col(cur_m)\n",
    "if CAT_COL is None:\n",
    "    CAT_COL = \"category_display\"\n",
    "    cur_m[CAT_COL] = np.nan\n",
    "\n",
    "if expenses_are_negative:\n",
    "    cur_exp_m = cur_m[cur_m[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp_m = cur_m[cur_m[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "top_cats_m = (\n",
    "    cur_exp_m.groupby(CAT_COL, dropna=False)[\"spend\"]\n",
    "             .sum().sort_values(ascending=False).head(5)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "def _label(v):\n",
    "    return \"Uncategorized\" if (pd.isna(v) or str(v).strip() in {\"\", \"nan\", \"None\"}) else str(v)\n",
    "\n",
    "if CAT_COL in top_cats_m.columns:\n",
    "    top_cats_m[CAT_COL] = top_cats_m[CAT_COL].apply(_label)\n",
    "\n",
    "# DEBUG — confirm which column was used and a peek at results\n",
    "print(f\"[Monthly] Category column used: {CAT_COL} | non-null in MTD: {int(cur_m[CAT_COL].notna().sum())}\")\n",
    "print(top_cats_m.head(5).to_string(index=False))\n",
    "\n",
    "subs_m  = cur_m.loc[cur_m.get(\"is_subscription\", False) == True]\n",
    "anoms_m = cur_m.loc[cur_m.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "# --- 5) Payload for AI ---\n",
    "summary_payload_m = {\n",
    "    \"as_of_date\": pd.Timestamp(cur_mtd_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(cur_month_start), \"end\": str(cur_mtd_end), \"label\": f\"{cur_month_name} MTD\"},\n",
    "        \"previous\": {\"start\": str(prev_month_start), \"end\": str(aligned_prev_m_end), \"label\": f\"{prev_month_name} MTD (aligned)\"}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        \"spend_mtd_current\": cur_spend_m,\n",
    "        \"spend_mtd_previous\": prev_spend_m,\n",
    "        \"spend_mtd_delta\": spend_delta_m,\n",
    "        \"spend_mtd_delta_pct\": spend_delta_pct_m,\n",
    "        \"income_mtd_current\": cur_income_m,\n",
    "        \"income_mtd_previous\": prev_income_m,\n",
    "    },\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[CAT_COL]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_m.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_m[\"display_name\"].nunique()) if len(subs_m) else 0,\n",
    "    \"anomalies_count\": int(anoms_m.shape[0]) if len(anoms_m) else 0,\n",
    "}\n",
    "\n",
    "# --- 6) Azure summarizer (REQUIRED): same personality as WoW ---\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY_M = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided month-to-date aggregates (vs the same number of days in the prior month), \"\n",
    "    \"produce an executive **monthly** digest in STRICT JSON. No invented numbers.\"\n",
    ")\n",
    "USER_INSTRUCTIONS_M = (\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"theme\": string,\\n'\n",
    "    '  \"narrative\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ],\\n'\n",
    "    '  \"email_subject\": string\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct where spend improved.\\n\"\n",
    "    \"- impact_usd is a rough **weekly** savings estimate; you may still propose monthly actions.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call_m(payload_json: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY_M},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS_M + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.15,\n",
    "        max_tokens=750,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "raw_m = _azure_digest_call_m(json.dumps(summary_payload_m))\n",
    "azure_digest_m = _salvage_json_object(raw_m)\n",
    "if not isinstance(azure_digest_m, dict):\n",
    "    raise RuntimeError(\"Azure MoM summarizer returned no valid JSON. Check Azure env/deployment/quota.\")\n",
    "\n",
    "# --- 7) Overlay + defaults ---\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in {\"totals\", \"window\"}:\n",
    "            continue\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "base_m = {\n",
    "    \"insights_version\": 3,\n",
    "    \"window\": summary_payload_m[\"window\"],\n",
    "    \"totals\": summary_payload_m[\"totals\"],\n",
    "    \"headline\": f\"Monthly digest — {cur_month_name} MTD\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Total Spend (MoM MTD)\",  \"value\": cur_spend_m,  \"delta_pct\": spend_delta_pct_m},\n",
    "        {\"name\":\"Total Income (MoM MTD)\",\"value\": cur_income_m, \"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": _label(t[\"category\"]), \"spend\": float(t[\"spend\"])} for t in summary_payload_m[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload_m['subscriptions_count']} active subs this month-to-date\"}] if summary_payload_m[\"subscriptions_count\"] else []) +\n",
    "        ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload_m['anomalies_count']} anomalies this month-to-date\"}] if summary_payload_m[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": [],\n",
    "    \"theme\": \"\",\n",
    "    \"narrative\": \"\",\n",
    "    \"email_subject\": \"\"\n",
    "}\n",
    "digest_m = _overlay(base_m, azure_digest_m)\n",
    "\n",
    "# Limit theme to 3 words max; fallback if empty\n",
    "if not isinstance(digest_m.get(\"theme\",\"\"), str) or not digest_m[\"theme\"].strip():\n",
    "    digest_m[\"theme\"] = \"Steady MTD\" if spend_delta_m <= 0 else \"Upward MTD\"\n",
    "else:\n",
    "    words = [w for w in re.split(r\"\\s+\", digest_m[\"theme\"].strip()) if w]\n",
    "    digest_m[\"theme\"] = \" \".join(words[:3])\n",
    "\n",
    "# --- 8) Save JSON ---\n",
    "with open(MOM_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest_m, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 9) Markdown (month names; narrative first) ---\n",
    "def render_md_m(d):\n",
    "    heading = f\"# Monthly Executive Summary ({cur_month_name} MTD)\"\n",
    "    lines = [heading]\n",
    "    if d.get(\"theme\"):\n",
    "        lines.append(f\"*{d['theme']}*\")\n",
    "    if d.get(\"narrative\"):\n",
    "        lines.append(f\"\\n{d['narrative'].strip()}\\n\")\n",
    "\n",
    "    lines.append(\"## Key Metrics (MoM)\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        lines.append(f\"- **{name}:** ${val:,.2f}{dp_txt}\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        lines.append(\"\\n## Drivers (MTD)\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            lines.append(f\"- **{label}:** ${float(t.get('spend',0)):,.0f}\")\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        lines.append(\"\\n## Risks\")\n",
    "        for r in rk[:5]:\n",
    "            lines.append(f\"- **{r.get('type','note')}:** {r.get('note','')}\")\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        lines.append(\"\\n## Recommendations\")\n",
    "        for a in ai[:5]:\n",
    "            lines.append(f\"- **{a.get('title','')}** — est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(MOM_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md_m(digest_m))\n",
    "\n",
    "# --- 10) Email subject + HTML (clean sections; month name only) ---\n",
    "subject_m = digest_m.get(\"email_subject\") or f\"{digest_m.get('theme','Monthly Digest')} — {cur_month_name} MTD\"\n",
    "with open(MOM_SUBJ, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(subject_m.strip())\n",
    "\n",
    "def render_email_html_m(d):\n",
    "    theme = (d.get(\"theme\") or \"\").strip()\n",
    "    narrative = (d.get(\"narrative\") or \"\").strip()\n",
    "    parts = []\n",
    "    parts.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "    parts.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "    parts.append(f\"<h1 style='margin:0 0 4px 0;font-size:18px;'>Monthly Executive Summary ({cur_month_name} MTD)</h1>\")\n",
    "    if theme:\n",
    "        parts.append(f\"<div style='margin:0 0 12px 0;color:#6b7280;font-style:italic'>{theme}</div>\")\n",
    "    if narrative:\n",
    "        parts.append(f\"<p style='margin:0 0 16px 0'>{narrative}</p>\")\n",
    "\n",
    "    parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Key Metrics (MoM)</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        parts.append(f\"<li><b>{name}:</b> ${val:,.2f}{dp_txt}</li>\")\n",
    "    parts.append(\"</ul>\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Drivers (MTD)</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            parts.append(f\"<li><b>{label}:</b> ${float(t.get('spend',0)):,.0f}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Risks</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for r in rk[:5]:\n",
    "            parts.append(f\"<li><b>{r.get('type','note')}:</b> {r.get('note','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Recommendations</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for a in ai[:5]:\n",
    "            parts.append(f\"<li><b>{a.get('title','')}</b> — est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    parts.append(\"</div>\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "with open(MOM_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_email_html_m(digest_m))\n",
    "\n",
    "# --- 11) Flat CSV for Power BI (MoM) ---\n",
    "flat_rows_m = []\n",
    "flat_rows_m.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(cur_mtd_end),\n",
    "    \"cur_start\": str(cur_month_start),\n",
    "    \"cur_end\": str(cur_mtd_end),\n",
    "    \"prev_start\": str(prev_month_start),\n",
    "    \"prev_end\": str(aligned_prev_m_end),\n",
    "    \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "    \"summary\": digest_m.get(\"narrative\",\"\"),\n",
    "    \"name\": \"Total Spend (MoM MTD)\",\n",
    "    \"value\": float(cur_spend_m),\n",
    "    \"delta_pct\": float(spend_delta_pct_m),\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "for m in digest_m.get(\"key_metrics\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for t in digest_m.get(\"top_drivers\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": (t.get(\"label\") or \"Uncategorized\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for r in digest_m.get(\"risks\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "pd.DataFrame(flat_rows_m).to_csv(MOM_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"🧾 Monthly executive digest written (MTD):\\n\"\n",
    "    f\"- JSON: {MOM_JSON}\\n- MD:   {MOM_MD}\\n- HTML: {MOM_HTML}\\n- CSV:  {MOM_FLAT}\\n- Subject: {MOM_SUBJ}\\n\"\n",
    "    f\"MTD Window: {cur_month_name} {cur_month_start} -> {cur_mtd_end} | Prior aligned: {prev_month_name} {prev_month_start} -> {aligned_prev_m_end}\\n\"\n",
    "    f\"MoM MTD Spend: cur={cur_spend_m} prev={prev_spend_m} delta={spend_delta_m} delta_pct={spend_delta_pct_m:+.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "papermill": {
     "duration": 0.226546,
     "end_time": "2025-09-13T23:16:16.491086",
     "exception": false,
     "start_time": "2025-09-13T23:16:16.264540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Cell 16: Combine Weekly + Monthly outputs for Power BI + Email bundle ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weekly artifacts from Cell 14\n",
    "W_JSON = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "W_FLAT = INSIGHTS_DIR / \"digest_latest_flat.csv\"\n",
    "W_HTML = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "W_SUBJ = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "\n",
    "# Monthly artifacts from Cell 15\n",
    "M_JSON = INSIGHTS_DIR / \"digest_mom.json\"\n",
    "M_FLAT = INSIGHTS_DIR / \"digest_mom_flat.csv\"\n",
    "M_HTML = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "M_SUBJ = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# Combined outputs\n",
    "C_FLAT = INSIGHTS_DIR / \"digest_combined_flat.csv\"\n",
    "C_HTML = INSIGHTS_DIR / \"digest_combined_email.html\"\n",
    "C_SUBJ = INSIGHTS_DIR / \"digest_combined_subject.txt\"\n",
    "\n",
    "# --- 1) Combine flat CSVs with a 'period' column ---\n",
    "frames = []\n",
    "if W_FLAT.exists():\n",
    "    w = pd.read_csv(W_FLAT)\n",
    "    w[\"period\"] = \"WoW\"\n",
    "    frames.append(w)\n",
    "if M_FLAT.exists():\n",
    "    m = pd.read_csv(M_FLAT)\n",
    "    m[\"period\"] = \"MoM\"\n",
    "    frames.append(m)\n",
    "\n",
    "if frames:\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined.to_csv(C_FLAT, index=False)\n",
    "else:\n",
    "    pd.DataFrame(columns=[\"row_type\",\"period\"]).to_csv(C_FLAT, index=False)\n",
    "\n",
    "# --- 2) Build combined subject (uses theme if available) ---\n",
    "def read_text(p: Path) -> str:\n",
    "    try:\n",
    "        return (p.read_text(encoding=\"utf-8\") or \"\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def read_json(p: Path):\n",
    "    try:\n",
    "        import json\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "wj = read_json(W_JSON)\n",
    "mj = read_json(M_JSON)\n",
    "\n",
    "weekly_theme = (wj.get(\"theme\") or \"\").strip()\n",
    "month_name = \"\"\n",
    "try:\n",
    "    month_name = pd.to_datetime(mj.get(\"window\",{}).get(\"current\",{}).get(\"end\",\"\")).strftime(\"%B\")\n",
    "except Exception:\n",
    "    # fallback by looking at any MTD end in flat\n",
    "    try:\n",
    "        mf = pd.read_csv(M_FLAT)\n",
    "        if \"as_of_end\" in mf.columns and len(mf):\n",
    "            month_name = pd.to_datetime(mf[\"as_of_end\"].iloc[0]).strftime(\"%B\")\n",
    "    except Exception:\n",
    "        month_name = \"\"\n",
    "\n",
    "subject_week = read_text(W_SUBJ) or (weekly_theme and f\"{weekly_theme} — Weekly\") or \"Weekly Summary\"\n",
    "subject_month = read_text(M_SUBJ) or (month_name and f\"{month_name} MTD\") or \"Monthly MTD\"\n",
    "combined_subject = f\"{subject_week} | {subject_month}\"\n",
    "Path(C_SUBJ).write_text(combined_subject, encoding=\"utf-8\")\n",
    "\n",
    "# --- 3) Build combined HTML (reuses generated HTML blocks if present) ---\n",
    "def read_html(p: Path) -> str:\n",
    "    try:\n",
    "        return p.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "w_html = read_html(W_HTML)\n",
    "m_html = read_html(M_HTML)\n",
    "\n",
    "# Minimal wrapper that keeps each section's styling intact\n",
    "combined_html = []\n",
    "combined_html.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "combined_html.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "combined_html.append(\"<h1 style='margin:0 0 12px 0;font-size:20px;'>Executive Summary — Weekly & Month-to-Date</h1>\")\n",
    "\n",
    "if w_html:\n",
    "    # Strip outer wrappers if present to avoid nested <html> tags\n",
    "    combined_html.append(\"<section style='margin-bottom:24px;border-bottom:1px solid #e5e7eb;padding-bottom:16px;'>\")\n",
    "    combined_html.append(w_html)\n",
    "    combined_html.append(\"</section>\")\n",
    "\n",
    "if m_html:\n",
    "    combined_html.append(\"<section style='margin-top:16px;'>\")\n",
    "    combined_html.append(m_html)\n",
    "    combined_html.append(\"</section>\")\n",
    "\n",
    "combined_html.append(\"</div>\")\n",
    "\n",
    "Path(C_HTML).write_text(\"\".join(combined_html), encoding=\"utf-8\")\n",
    "\n",
    "print(\n",
    "    \"📦 Combined artifacts:\\n\"\n",
    "    f\"- CSV:  {C_FLAT}\\n\"\n",
    "    f\"- HTML: {C_HTML}\\n\"\n",
    "    f\"- Subject: {C_SUBJ}\\n\"\n",
    "    f\"(Sources -> Weekly: {W_FLAT.name}, Monthly: {M_FLAT.name})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 16.5 (REPLACE): Exec-ready charts with wider figs, extra padding, percentile gradients ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P_LINE     = INSIGHTS_DIR / \"weekly_spend_line.png\"\n",
    "P_DONUT    = INSIGHTS_DIR / \"weekly_top_categories_donut.png\"\n",
    "P_MOVEMENT = INSIGHTS_DIR / \"weekly_category_movement.png\"\n",
    "\n",
    "# Fonts / styling (portable)\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Segoe UI\", \"DejaVu Sans\", \"Arial\", \"Liberation Sans\"]\n",
    "\n",
    "def fmt_usd(x):\n",
    "    try:\n",
    "        return f\"${float(x):,.0f}\"\n",
    "    except Exception:\n",
    "        return \"$0\"\n",
    "\n",
    "def spend_series(frame: pd.DataFrame, expenses_are_negative: bool) -> pd.Series:\n",
    "    a = frame[\"amount\"].astype(float)\n",
    "    return a.where(a < 0, 0).abs() if expenses_are_negative else a.where(a > 0, 0)\n",
    "\n",
    "def prefer_category_column(frame: pd.DataFrame) -> str:\n",
    "    for c in [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return \"category\"\n",
    "\n",
    "# --- Common filters (mirror weekly digest logic) ---\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "\n",
    "# Week window (Mon–Sun), consistent with Cell 14\n",
    "try:\n",
    "    _ = wk_start, wk_end\n",
    "except NameError:\n",
    "    try:\n",
    "        now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "    except Exception:\n",
    "        now = pd.Timestamp.now().normalize()\n",
    "    wd = int(now.weekday())  # Mon=0..Sun=6\n",
    "    days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "    wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()\n",
    "    wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "prev_wk_start = (pd.Timestamp(wk_start) - pd.Timedelta(days=7)).date()\n",
    "prev_wk_end   = (pd.Timestamp(wk_start) - pd.Timedelta(days=1)).date()\n",
    "\n",
    "# Polarity\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "# ================= 1) Daily trend — last 28 days (classic style; wider fig & padding) =================\n",
    "plot_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=27)).date()\n",
    "tw = base[(base[\"date_only\"] >= plot_start) & (base[\"date_only\"] <= wk_end)].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11.2, 5.8), dpi=144)  # wider\n",
    "if tw.empty:\n",
    "    ax.text(0.5, 0.5, \"No spend data (last 28 days)\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    tw[\"dt\"] = pd.to_datetime(tw[\"date_only\"])\n",
    "    daily = (tw.assign(spend=spend_series(tw, expenses_are_negative))\n",
    "               .groupby(\"dt\", dropna=False)[\"spend\"].sum()\n",
    "               .sort_index())\n",
    "    idx = pd.date_range(start=pd.to_datetime(plot_start), end=pd.to_datetime(wk_end), freq=\"D\")\n",
    "    daily = daily.reindex(idx, fill_value=0.0)\n",
    "    ma7 = daily.rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Classic lines\n",
    "    ax.plot(daily.index, daily.values, marker=\"o\", linewidth=2, label=\"Total Spend\")\n",
    "    ax.plot(ma7.index, ma7.values, linestyle=\"--\", linewidth=2, label=\"7-Day Average\")\n",
    "\n",
    "    # Currency on Y\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, p: fmt_usd(v)))\n",
    "\n",
    "    # X-axis like \"Aug-28\"\n",
    "    def _fmt_mmm_day(xv, pos):\n",
    "        try:\n",
    "            dt = mdates.num2date(xv)\n",
    "            return f\"{dt.strftime('%b')}-{dt.day}\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=6, maxticks=10))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(_fmt_mmm_day))\n",
    "\n",
    "    # Week-start dashed lines in light gray\n",
    "    for vline in [pd.to_datetime(prev_wk_start), pd.to_datetime(wk_start)]:\n",
    "        ax.axvline(vline, linestyle=\"--\", linewidth=1, alpha=0.8, color=\"#d1d5db\")\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=0.35)\n",
    "    ax.set_title(\"Total Spending — Last 28 Days\", pad=8, fontsize=12)\n",
    "    ax.legend(frameon=False, loc=\"upper left\")\n",
    "\n",
    "    # Extra breathing room\n",
    "    ax.set_xlim(daily.index.min(), daily.index.max())\n",
    "    ax.set_ylim(0, max(1.0, float(daily.values.max()) * 1.18))\n",
    "    ax.margins(x=0.03, y=0.14)\n",
    "\n",
    "fig.tight_layout(rect=[0.02, 0.02, 0.98, 0.98])\n",
    "fig.savefig(P_LINE, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Prep week frames\n",
    "cat_col = prefer_category_column(base)\n",
    "\n",
    "cur_w  = base[(base[\"date_only\"] >= wk_start) & (base[\"date_only\"] <= wk_end)].copy()\n",
    "prev_w = base[(base[\"date_only\"] >= prev_wk_start) & (base[\"date_only\"] <= prev_wk_end)].copy()\n",
    "\n",
    "# Spend-only frames\n",
    "if expenses_are_negative:\n",
    "    cur_exp  = cur_w[cur_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "    prev_exp = prev_w[prev_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp  = cur_w[cur_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "    prev_exp = prev_w[prev_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "# Clean categories and exclude admin buckets\n",
    "EXCLUDE_CATS = {\"Transfers\",\"Income\",\"Debt Payments\",\"Fees\"}\n",
    "def _clean_cat(s):\n",
    "    s = (\"\" if pd.isna(s) else str(s)).strip()\n",
    "    return \"Uncategorized\" if s == \"\" or s.lower() in {\"none\",\"nan\"} else s\n",
    "\n",
    "cur_exp[cat_col]  = cur_exp[cat_col].apply(_clean_cat)\n",
    "prev_exp[cat_col] = prev_exp[cat_col].apply(_clean_cat)\n",
    "\n",
    "# ================= 2) Donut — Top Categories This Week (legend bottom; wider fig & bottom margin) =================\n",
    "top_cats = (cur_exp[~cur_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "            .groupby(cat_col, dropna=False)[\"spend\"].sum()\n",
    "            .sort_values(ascending=False).head(6))\n",
    "labels = [str(i) for i in top_cats.index]\n",
    "values = top_cats.values\n",
    "total  = float(top_cats.sum()) or 1.0\n",
    "\n",
    "# Bright palette\n",
    "bright_colors = [\"#6366F1\",\"#F59E0B\",\"#10B981\",\"#EF4444\",\"#3B82F6\",\"#A855F7\",\"#F97316\",\"#06B6D4\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.2, 6.6), dpi=144)  # wider\n",
    "if len(top_cats) == 0:\n",
    "    ax.text(0.5, 0.5, \"No category data for this week\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    ret = ax.pie(values, labels=None, autopct=None, startangle=90, colors=bright_colors[:len(values)])\n",
    "    wedges = ret[0]\n",
    "\n",
    "    # Donut hole\n",
    "    centre_circle = plt.Circle((0,0), 0.55, fc=\"white\")\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "\n",
    "    # Legend at the BOTTOM; no legend title\n",
    "    pretty_labels = [f\"{lab} — {fmt_usd(val)} ({val/total:,.0%})\" for lab, val in zip(labels, values)]\n",
    "    ax.legend(\n",
    "        wedges, pretty_labels,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        ncol=min(3, len(pretty_labels)),\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Top Categories This Week\", pad=8, fontsize=12)\n",
    "    ax.axis('equal')\n",
    "\n",
    "# Leave extra bottom room for the legend\n",
    "fig.tight_layout(rect=[0.02, 0.10, 0.98, 0.98])\n",
    "fig.savefig(P_DONUT, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ================= 3) Category Movement WoW (This Week - Prior) =================\n",
    "cur_s  = (cur_exp[~cur_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "          .groupby(cat_col, dropna=False)[\"spend\"].sum())\n",
    "prev_s = (prev_exp[~prev_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "          .groupby(cat_col, dropna=False)[\"spend\"].sum())\n",
    "\n",
    "cats = sorted(set(cur_s.index) | set(prev_s.index))\n",
    "cur_s  = cur_s.reindex(cats, fill_value=0.0)\n",
    "prev_s = prev_s.reindex(cats, fill_value=0.0)\n",
    "delta = (cur_s - prev_s)\n",
    "\n",
    "# Keep top movers by absolute change\n",
    "delta = delta[delta != 0].sort_values(key=np.abs, ascending=False).head(10)\n",
    "labels_mv = list(delta.index)\n",
    "vals_mv   = delta.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11.8, 6.6), dpi=144)  # wider\n",
    "if len(delta) == 0:\n",
    "    ax.text(0.5, 0.5, \"No category movement WoW\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    abs_mv = np.abs(vals_mv)\n",
    "    max_abs = float(abs_mv.max()) if len(abs_mv) else 1.0\n",
    "\n",
    "    # ----- Percentile-based color intensity (robust to scale) -----\n",
    "    # Rank by absolute change (ascending); convert to 0..1 percentile\n",
    "    if len(abs_mv) > 1:\n",
    "        ranks = abs_mv.argsort().argsort() + 1  # 1..N\n",
    "        pct = ranks / float(len(abs_mv))        # 0..1\n",
    "    else:\n",
    "        pct = np.array([1.0])\n",
    "\n",
    "    def hex_to_rgb(h):\n",
    "        h = h.lstrip(\"#\")\n",
    "        return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        return \"#{:02X}{:02X}{:02X}\".format(*rgb)\n",
    "\n",
    "    def lerp(a, b, t):\n",
    "        return int(a + (b - a) * t)\n",
    "\n",
    "    def lerp_hex(c1, c2, t):\n",
    "        r1,g1,b1 = hex_to_rgb(c1); r2,g2,b2 = hex_to_rgb(c2)\n",
    "        return rgb_to_hex((lerp(r1,r2,t), lerp(g1,g2,t), lerp(b1,b2,t)))\n",
    "\n",
    "    # Tailwind-ish ramps (light -> dark)\n",
    "    RED_LIGHT   = \"#FECACA\"  # red-300\n",
    "    RED_DARK    = \"#B91C1C\"  # red-700\n",
    "    GREEN_LIGHT = \"#BBF7D0\"  # green-200\n",
    "    GREEN_DARK  = \"#065F46\"  # emerald-900\n",
    "\n",
    "    # Keep saturation between 0.35 and 1.0 to stay vivid\n",
    "    t = 0.35 + 0.65 * pct\n",
    "    colors = [\n",
    "        lerp_hex(GREEN_LIGHT, GREEN_DARK, ti) if v < 0 else lerp_hex(RED_LIGHT, RED_DARK, ti)\n",
    "        for v, ti in zip(vals_mv, t)\n",
    "    ]\n",
    "\n",
    "    # Bars\n",
    "    y = np.arange(len(labels_mv))[::-1]\n",
    "    ax.barh(y, vals_mv, height=0.55, color=colors, edgecolor=\"none\")\n",
    "\n",
    "    # Y labels\n",
    "    ax.set_yticks(y, labels_mv)\n",
    "\n",
    "    # Extra horizontal padding so labels never crowd the axis\n",
    "    pad_abs = max(30.0, (float(max_abs) if len(abs_mv) else 1.0) * 0.22)\n",
    "    xmin = -max_abs - pad_abs\n",
    "    xmax =  max_abs + pad_abs\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.margins(x=0.06)\n",
    "\n",
    "    # Annotate each bar with $ change, placed slightly outside the bar\n",
    "    for yi, v in zip(y, vals_mv):\n",
    "        offset = max(10.0, (float(max_abs) if len(abs_mv) else 1.0) * 0.06)\n",
    "        x_text = v + (offset if v >= 0 else -offset)\n",
    "        ha = \"left\" if v >= 0 else \"right\"\n",
    "        ax.text(x_text, yi, f\"{fmt_usd(v)}\", va=\"center\", ha=ha, fontsize=10)\n",
    "\n",
    "    # Zero reference line + grid\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1, alpha=0.7, color=\"#9ca3af\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: fmt_usd(x)))\n",
    "    ax.grid(axis=\"x\", linestyle=\":\", alpha=0.35)\n",
    "\n",
    "    ax.set_title(\"Category Movement WoW (This Week vs Prior)\", pad=10, fontsize=12)\n",
    "\n",
    "fig.tight_layout(rect=[0.03, 0.02, 0.99, 0.98])\n",
    "fig.savefig(P_MOVEMENT, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"📊 Charts written →\", P_LINE.name, P_DONUT.name, P_MOVEMENT.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16.7-DEBUG — verify wkhtmltopdf path and test it\n",
    "import os, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "raw = (os.getenv(\"WKHTMLTOPDF_PATH\", \"\") or \"\").strip().strip('\"')\n",
    "print(\"WKHTMLTOPDF_PATH env:\", raw or \"<not set>\")\n",
    "\n",
    "candidates = []\n",
    "if raw:\n",
    "    p = Path(raw)\n",
    "    if p.is_dir():\n",
    "        candidates += [p / \"bin\" / \"wkhtmltopdf.exe\", p / \"wkhtmltopdf.exe\"]\n",
    "    else:\n",
    "        candidates.append(p)\n",
    "\n",
    "# common Windows paths\n",
    "candidates += [\n",
    "    Path(r\"C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\"),\n",
    "    Path(r\"C:\\Program Files (x86)\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\"),\n",
    "    Path(r\"C:\\ProgramData\\chocolatey\\bin\\wkhtmltopdf.exe\"),\n",
    "]\n",
    "\n",
    "# PATH\n",
    "which = shutil.which(\"wkhtmltopdf\")\n",
    "if which:\n",
    "    candidates.append(Path(which))\n",
    "\n",
    "seen = set()\n",
    "good = None\n",
    "for p in candidates:\n",
    "    if not p: \n",
    "        continue\n",
    "    p = Path(p)\n",
    "    if str(p).lower() in seen:\n",
    "        continue\n",
    "    seen.add(str(p).lower())\n",
    "    print(\"Check:\", p, \"| exists:\", p.exists(), \"| isfile:\", p.is_file())\n",
    "    if p.exists() and p.is_file():\n",
    "        try:\n",
    "            out = subprocess.run([str(p), \"--version\"], capture_output=True, text=True, check=True)\n",
    "            print(\"→ version:\", (out.stdout or out.stderr).strip())\n",
    "            good = str(p.resolve())\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"→ version check failed:\", e)\n",
    "\n",
    "if not good:\n",
    "    raise SystemExit(\"No working wkhtmltopdf.exe found. Update WKHTMLTOPDF_PATH to the full EXE path.\")\n",
    "else:\n",
    "    print(\"Resolved wkhtmltopdf EXE:\", good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 16.8 (REPLACE): PDF via wkhtmltopdf only (no WeasyPrint) ---\n",
    "\n",
    "import os, shutil, subprocess, re\n",
    "from pathlib import Path\n",
    "import pdfkit  # pip install pdfkit\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMAIL_HTML = INSIGHTS_DIR / \"digest_combined_email.html\"\n",
    "# Fallback to weekly if combined is missing\n",
    "if not EMAIL_HTML.exists():\n",
    "    alt1 = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "    alt2 = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "    if alt1.exists():\n",
    "        EMAIL_HTML = alt1\n",
    "    elif alt2.exists():\n",
    "        EMAIL_HTML = alt2\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No digest HTML found in insights/ to convert.\")\n",
    "\n",
    "PDF_OUT = INSIGHTS_DIR / \"executive_digest.pdf\"\n",
    "\n",
    "def resolve_wkhtmltopdf():\n",
    "    raw = (os.getenv(\"WKHTMLTOPDF_PATH\", \"\") or \"\").strip().strip('\"')\n",
    "    candidates = []\n",
    "    if raw:\n",
    "        p = Path(raw)\n",
    "        if p.is_dir():\n",
    "            candidates += [p / \"bin\" / \"wkhtmltopdf.exe\", p / \"wkhtmltopdf.exe\"]\n",
    "        else:\n",
    "            candidates.append(p)\n",
    "    candidates += [\n",
    "        Path(r\"C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\"),\n",
    "        Path(r\"C:\\Program Files (x86)\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\"),\n",
    "        Path(r\"C:\\ProgramData\\chocolatey\\bin\\wkhtmltopdf.exe\"),\n",
    "    ]\n",
    "    which = shutil.which(\"wkhtmltopdf\")\n",
    "    if which:\n",
    "        candidates.append(Path(which))\n",
    "\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            if c and Path(c).exists() and Path(c).is_file():\n",
    "                return str(Path(c).resolve())\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(\n",
    "        \"wkhtmltopdf not found. Set WKHTMLTOPDF_PATH to the full executable, e.g.\\n\"\n",
    "        r'  C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "    )\n",
    "\n",
    "WKHTMLTOPDF_EXE = resolve_wkhtmltopdf()\n",
    "print(\"wkhtmltopdf →\", WKHTMLTOPDF_EXE)\n",
    "\n",
    "config = pdfkit.configuration(wkhtmltopdf=WKHTMLTOPDF_EXE)\n",
    "\n",
    "# Enable local file access so inline <img src=\"cid:...\"> and file:// paths work if present\n",
    "options = {\n",
    "    \"enable-local-file-access\": \"\",\n",
    "    \"page-size\": \"Letter\",\n",
    "    \"margin-top\": \"12mm\",\n",
    "    \"margin-right\": \"12mm\",\n",
    "    \"margin-bottom\": \"14mm\",\n",
    "    \"margin-left\": \"12mm\",\n",
    "    \"encoding\": \"UTF-8\",\n",
    "    # Optional: tweak zoom/quality if needed\n",
    "    \"zoom\": \"1.00\",\n",
    "}\n",
    "\n",
    "# Convert HTML file → PDF\n",
    "pdfkit.from_file(str(EMAIL_HTML), str(PDF_OUT), configuration=config, options=options)\n",
    "print(f\"📄 PDF written → {PDF_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 17: Email dispatch (inline images only, NO attachments) ---\n",
    "# Gmail-ready SMTP, kill switch, inline PNG charts via CID.\n",
    "# Removes all file attachments (CSVs and image fallbacks).\n",
    "\n",
    "import os, smtplib, ssl, re\n",
    "from pathlib import Path\n",
    "from email.message import EmailMessage\n",
    "\n",
    "def _mask(s):\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:3] + \"…\" + s[-3:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PDF_PATH = DATA_PROCESSED / \"insights\" / \"executive_digest.pdf\"\n",
    "EMAIL_ATTACH_PDF = (os.getenv(\"EMAIL_ATTACH_PDF\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n",
    "\n",
    "# Primary artifacts for body/subject\n",
    "C_HTML = INSIGHTS_DIR / \"digest_combined_email.html\"\n",
    "C_SUBJ = INSIGHTS_DIR / \"digest_combined_subject.txt\"\n",
    "W_HTML = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "W_SUBJ = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "M_HTML = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "M_SUBJ = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# Chart PNGs (from Cell 16.5)\n",
    "WEEKLY_LINE_PATH = INSIGHTS_DIR / \"weekly_spend_line.png\"\n",
    "WEEKLY_PIE_PATH  = INSIGHTS_DIR / \"weekly_top_categories_pie.png\"\n",
    "\n",
    "# ---------------- Kill switch ----------------\n",
    "EMAIL_ENABLED = (os.getenv(\"EMAIL_ENABLED\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "EMAIL_KILL_FILE = STATE_DIR / \"EMAIL_KILL\"\n",
    "if EMAIL_KILL_FILE.exists():\n",
    "    EMAIL_ENABLED = False\n",
    "\n",
    "EMAIL_DRY_RUN = (os.getenv(\"EMAIL_DRY_RUN\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n",
    "\n",
    "if not EMAIL_ENABLED:\n",
    "    print(\"✋ Email sending disabled (kill switch). Set EMAIL_ENABLED=1 and remove .state/EMAIL_KILL to re-enable.\")\n",
    "else:\n",
    "    # ---------------- SMTP (Gmail-ready) ----------------\n",
    "    SMTP_HOST = os.getenv(\"SMTP_HOST\", \"smtp.gmail.com\").strip()\n",
    "    SMTP_PORT = int(os.getenv(\"SMTP_PORT\", \"587\"))\n",
    "    SMTP_SSL_PORT = int(os.getenv(\"SMTP_SSL_PORT\", \"465\"))\n",
    "    SMTP_USERNAME = (os.getenv(\"SMTP_USERNAME\", \"\") or \"\").strip()\n",
    "    SMTP_PASSWORD = (os.getenv(\"SMTP_PASSWORD\", \"\") or \"\").replace(\" \", \"\")  # trim spaces Google shows\n",
    "    SMTP_STARTTLS = (os.getenv(\"SMTP_STARTTLS\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "\n",
    "    EMAIL_FROM = (os.getenv(\"EMAIL_FROM\", \"\") or \"\").strip()\n",
    "    EMAIL_TO   = (os.getenv(\"EMAIL_TO\", \"\") or \"\").strip()\n",
    "    EMAIL_CC   = (os.getenv(\"EMAIL_CC\", \"\") or \"\").strip()\n",
    "    EMAIL_BCC  = (os.getenv(\"EMAIL_BCC\", \"\") or \"\").strip()\n",
    "\n",
    "    SUBJECT_OVERRIDE = os.getenv(\"EMAIL_SUBJECT_OVERRIDE\", \"\").strip()\n",
    "    BODY_HTML_OVERRIDE_PATH = os.getenv(\"EMAIL_BODY_HTML_PATH\", \"\").strip()\n",
    "\n",
    "    # Minimal validation\n",
    "    missing = [k for k,v in {\n",
    "        \"SMTP_HOST\": SMTP_HOST,\n",
    "        \"SMTP_USERNAME\": SMTP_USERNAME,\n",
    "        \"SMTP_PASSWORD\": SMTP_PASSWORD,\n",
    "        \"EMAIL_FROM\": EMAIL_FROM,\n",
    "        \"EMAIL_TO\": EMAIL_TO,\n",
    "    }.items() if not v]\n",
    "    if missing:\n",
    "        raise RuntimeError(\"Email config missing: \" + \", \".join(missing))\n",
    "\n",
    "    # ---------------- Subject & HTML body ----------------\n",
    "    def _read_text(p: Path) -> str:\n",
    "        try: return (p.read_text(encoding=\"utf-8\") or \"\").strip()\n",
    "        except Exception: return \"\"\n",
    "\n",
    "    def _read_html(p: Path) -> str:\n",
    "        try: return p.read_text(encoding=\"utf-8\")\n",
    "        except Exception: return \"\"\n",
    "\n",
    "    subject = SUBJECT_OVERRIDE or _read_text(C_SUBJ) or _read_text(W_SUBJ) or _read_text(M_SUBJ) or \"AI Credit Card Dashboard — Digest\"\n",
    "\n",
    "    if BODY_HTML_OVERRIDE_PATH:\n",
    "        body_html = Path(BODY_HTML_OVERRIDE_PATH).read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        body_html = _read_html(C_HTML) or _read_html(W_HTML) or _read_html(M_HTML)\n",
    "\n",
    "    if not body_html:\n",
    "        body_html = (\n",
    "            \"<!doctype html><meta charset='utf-8'>\"\n",
    "            \"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\"\n",
    "            \"<h1 style='margin:0 0 8px 0;font-size:18px;'>AI Credit Card Dashboard — Digest</h1>\"\n",
    "            \"<p>No HTML digest was found this run. Check earlier cells for generation status.</p>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    # Plain-text fallback\n",
    "    def _html_to_text(h: str) -> str:\n",
    "        t = re.sub(r\"<(br|/p|/li)>\", \"\\n\", h, flags=re.IGNORECASE)\n",
    "        t = re.sub(r\"<[^>]+>\", \"\", t)\n",
    "        return re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    body_text = _html_to_text(body_html)\n",
    "\n",
    "    # ---------------- Prepare image refs & append <img> once ----------------\n",
    "    img_refs = []\n",
    "    if (INSIGHTS_DIR / \"weekly_spend_line.png\").exists():\n",
    "        img_refs.append((\"weekly_line\", INSIGHTS_DIR / \"weekly_spend_line.png\"))\n",
    "    if (INSIGHTS_DIR / \"weekly_top_categories_donut.png\").exists():\n",
    "        img_refs.append((\"weekly_top_categories_donut\", INSIGHTS_DIR / \"weekly_top_categories_donut.png\"))\n",
    "    if (INSIGHTS_DIR / \"weekly_category_movement.png\").exists():\n",
    "        img_refs.append((\"weekly_category_movement\", INSIGHTS_DIR / \"weekly_category_movement.png\"))\n",
    "\n",
    "    # ---------------- Build email ----------------\n",
    "    def _split_emails(s): return [e.strip() for e in s.split(\",\") if e.strip()]\n",
    "    rcpts = []\n",
    "    seen = set()\n",
    "    for e in _split_emails(EMAIL_TO) + _split_emails(EMAIL_CC) + _split_emails(EMAIL_BCC):\n",
    "        if e.lower() not in seen:\n",
    "            seen.add(e.lower()); rcpts.append(e)\n",
    "    if not rcpts:\n",
    "        raise RuntimeError(\"No recipients found (EMAIL_TO/CC/BCC).\")\n",
    "\n",
    "    msg = EmailMessage()\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = EMAIL_FROM\n",
    "    msg[\"To\"] = \", \".join(_split_emails(EMAIL_TO))\n",
    "    if EMAIL_CC: msg[\"Cc\"] = \", \".join(_split_emails(EMAIL_CC))\n",
    "\n",
    "    # Force multipart/alternative container and add HTML\n",
    "    msg.set_content(body_text)\n",
    "    msg.make_alternative()\n",
    "    msg.add_alternative(body_html, subtype=\"html\")\n",
    "    html_part = msg.get_body(preferencelist=(\"html\",))\n",
    "\n",
    "    # Inline embed only (no fallback attachments)\n",
    "    embedded = []\n",
    "    if html_part is not None and img_refs:\n",
    "        for cid, pth in img_refs:\n",
    "            try:\n",
    "                with open(pth, \"rb\") as f:\n",
    "                    html_part.add_related(\n",
    "                        f.read(),\n",
    "                        maintype=\"image\",\n",
    "                        subtype=\"png\",\n",
    "                        cid=f\"<{cid}>\",\n",
    "                        filename=pth.name\n",
    "                    )\n",
    "                embedded.append(pth.name)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Inline embed failed for {pth.name}: {e}. Skipping image (no attachments by policy).\")\n",
    "    elif img_refs:\n",
    "        print(\"⚠️ Could not locate HTML part; skipping inline images (no attachments by policy).\")\n",
    "   \n",
    "    # ---------------- Optional PDF attachment ----------------\n",
    "    if EMAIL_ATTACH_PDF and PDF_PATH.exists():\n",
    "        try:\n",
    "            with open(PDF_PATH, \"rb\") as f:\n",
    "                msg.add_attachment(\n",
    "                    f.read(),\n",
    "                    maintype=\"application\",\n",
    "                    subtype=\"pdf\",\n",
    "                    filename=PDF_PATH.name\n",
    "                )\n",
    "            print(f\"📎 Attached PDF: {PDF_PATH.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not attach PDF: {e}\")\n",
    "\n",
    "    # ---------------- Send (STARTTLS then SSL fallback) ----------------\n",
    "    context = ssl.create_default_context()\n",
    "\n",
    "    def _try_starttls():\n",
    "        with smtplib.SMTP(SMTP_HOST, SMTP_PORT, timeout=60) as server:\n",
    "            server.ehlo()\n",
    "            if SMTP_STARTTLS: server.starttls(context=context); server.ehlo()\n",
    "            server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "            if EMAIL_DRY_RUN:\n",
    "                print(\"✅ STARTTLS login OK (dry-run).\"); return\n",
    "            server.send_message(msg, to_addrs=rcpts)\n",
    "\n",
    "    def _try_ssl():\n",
    "        with smtplib.SMTP_SSL(SMTP_HOST, SMTP_SSL_PORT, context=context, timeout=60) as server:\n",
    "            server.ehlo()\n",
    "            server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "            if EMAIL_DRY_RUN:\n",
    "                print(\"✅ SSL login OK (dry-run).\"); return\n",
    "            server.send_message(msg, to_addrs=rcpts)\n",
    "\n",
    "    try:\n",
    "        _try_starttls()\n",
    "        print(f\"📧 Email sent via STARTTLS to {', '.join(rcpts)} — subject: {subject}\")\n",
    "    except smtplib.SMTPAuthenticationError as e:\n",
    "        print(\"❌ STARTTLS auth failed:\", e.smtp_error.decode() if hasattr(e, \"smtp_error\") else str(e))\n",
    "        print(\"…attempting SSL on port\", SMTP_SSL_PORT)\n",
    "        _try_ssl()\n",
    "        print(f\"📧 Email sent via SSL to {', '.join(rcpts)} — subject: {subject}\")\n",
    "\n",
    "    print(\"Inline images:\", embedded)\n",
    "    print(\"Attachments: none\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.870682,
   "end_time": "2025-09-13T23:16:17.727110",
   "environment_variables": {},
   "exception": null,
   "input_path": "scripts/enrich_transactions.ipynb",
   "output_path": "scripts/enrich_transactions.ipynb",
   "parameters": {},
   "start_time": "2025-09-13T23:16:02.856428",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
