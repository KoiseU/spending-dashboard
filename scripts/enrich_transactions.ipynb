{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf10efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Azure OpenAI (chat) not fully set; AI summaries will fall back to deterministic base.\n",
      "⚠️ Azure OpenAI (embeddings) not set; embeddings cache will be skipped.\n",
      "✅ Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Robust setup + centralized Azure client factory ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure OpenAI SDK is available (Azure OpenAI compatible)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# --- Paths (prefer GITHUB_WORKSPACE, never walk above repo) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW       = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR     = REPO / \"config\"\n",
    "STATE_DIR      = REPO / \".state\"\n",
    "VECTOR_DIR     = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH  = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH    = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH  = DATA_RAW / \"latest.csv\"                # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH        = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH          = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH    = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# Ensure dirs\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config flags\n",
    "MAP_ALL        = True        # label unmapped merchants via Azure (if enabled)\n",
    "GOAL_SAVINGS   = 1000.0      # monthly savings target for nudges\n",
    "ANOMALY_Z      = 2.5         # z-score threshold for anomalies\n",
    "\n",
    "# --- Azure OpenAI env ---\n",
    "AZURE_OPENAI_ENDPOINT   = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "AZURE_OPENAI_API_KEY    = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"\")   # chat model (deployment name)\n",
    "AZURE_OPENAI_EMBEDDINGS = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\", \"\")   # embeddings deployment name\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "def _have_azure(deploy: str) -> bool:\n",
    "    return bool(AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY and deploy)\n",
    "\n",
    "def make_azure_client(deployment: str) -> OpenAI | None:\n",
    "    \"\"\"Factory for Azure OpenAI client bound to a specific deployment.\"\"\"\n",
    "    if not _have_azure(deployment):\n",
    "        return None\n",
    "    # For Azure, base_url points at the deployment; api-version goes on every request\n",
    "    return OpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        base_url=f\"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{deployment}\",\n",
    "        default_query={\"api-version\": AZURE_OPENAI_API_VERSION},\n",
    "        default_headers={\"api-key\": AZURE_OPENAI_API_KEY},\n",
    "    )\n",
    "\n",
    "# Shared clients (None if not configured)\n",
    "chat_client  = make_azure_client(AZURE_OPENAI_DEPLOYMENT) if AZURE_OPENAI_DEPLOYMENT else None\n",
    "embed_client = make_azure_client(AZURE_OPENAI_EMBEDDINGS) if AZURE_OPENAI_EMBEDDINGS else None\n",
    "azure_enabled = chat_client is not None\n",
    "\n",
    "if not azure_enabled:\n",
    "    print(\"⚠️ Azure OpenAI (chat) not fully set; AI summaries will fall back to deterministic base.\")\n",
    "if embed_client is None:\n",
    "    print(\"⚠️ Azure OpenAI (embeddings) not set; embeddings cache will be skipped.\")\n",
    "\n",
    "print(\"✅ Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b64fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 148 transactions. expenses_are_negative=False\n"
     ]
    }
   ],
   "source": [
    "# Load latest.csv (from build_latest.ipynb), robust path resolution\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "# Global sign convention: True if expenses are negative numbers\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "print(f\"Loaded {len(df)} transactions. expenses_are_negative={EXPENSES_ARE_NEGATIVE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1397b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized (consistent with build_latest).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Normalize merchant_key consistently with build_latest ---\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    s = (name or \"\").upper()\n",
    "    s = re.sub(r\"APPLE PAY ENDING IN \\d{4}\", \"\", s)\n",
    "    s = re.sub(r\"#\\d{2,}\", \"\", s)              # strip store numbers like #1234\n",
    "    s = re.sub(r\"\\d+\", \"\", s)                  # kill stray digits\n",
    "    s = re.sub(r\"[^A-Z&\\s]\", \" \", s)           # keep letters, ampersand, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or \"UNKNOWN\"\n",
    "\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    df[\"merchant_name\"].map(merchant_key_from),\n",
    "    df[\"name\"].map(merchant_key_from)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized (consistent with build_latest).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d521e9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 13\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize merchant dimension table\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db494c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new mappings needed or AI disabled.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Label unmapped merchants via Azure (single-call) ---\n",
    "new_rows = []\n",
    "if len(unmapped_keys) and ('chat_client' in globals()) and (chat_client is not None) and MAP_ALL:\n",
    "    print(f\"Labeling {len(unmapped_keys)} merchants (single-call mode)...\")\n",
    "    for idx, mk in enumerate(unmapped_keys, 1):\n",
    "        try:\n",
    "            item = azure_label_one(mk)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Label fail for '{mk}': {e}\")\n",
    "            continue\n",
    "\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if item:\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": item[\"display_name\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"subcategory\": item[\"subcategory\"],\n",
    "                \"tags\": \",\".join(item[\"tags\"]),\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        dim_new = pd.DataFrame(new_rows)\n",
    "        dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "        dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "        MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        dim = dim_all\n",
    "        print(f\"✅ Added {len(new_rows)} merchant mappings (single-call).\")\n",
    "    else:\n",
    "        print(\"No new mappings added (single-call).\")\n",
    "else:\n",
    "    print(\"No new mappings needed or AI disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aef6c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 merchants_dim.csv saved (64 rows) → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\merchants_dim.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6B: Persist merchants_dim.csv (idempotent) ---\n",
    "\n",
    "# Toggle if you ever want to skip writing on runs with no changes\n",
    "PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "# dim_cols defined in Cell 4; dim may be updated in Cell 6\n",
    "if not isinstance(PERSIST_MERCHANT_DIM, bool):\n",
    "    PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "if PERSIST_MERCHANT_DIM:\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'dim' in globals() and isinstance(dim, pd.DataFrame) and len(dim):\n",
    "        # ensure expected columns/order exist before save\n",
    "        for c in dim_cols:\n",
    "            if c not in dim.columns:\n",
    "                dim[c] = np.nan\n",
    "        dim = dim[dim_cols]\n",
    "\n",
    "        dim.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        print(f\"📝 merchants_dim.csv saved ({len(dim)} rows) → {MERCHANT_DIM_PATH}\")\n",
    "    else:\n",
    "        # either no new mappings this run or dim was empty; ensure file exists\n",
    "        if not MERCHANT_DIM_PATH.exists():\n",
    "            pd.DataFrame(columns=dim_cols).to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            print(f\"📝 Created headers-only merchants_dim.csv → {MERCHANT_DIM_PATH}\")\n",
    "        else:\n",
    "            print(\"ℹ️ merchants_dim.csv already exists; no changes to sync.\")\n",
    "else:\n",
    "    print(\"PERSIST_MERCHANT_DIM=False → skipping merchants_dim.csv persistence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6eba14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels joined.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"], errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Final output columns (feel free to adjust ordering)\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory\", \"tags\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "# Ensure existence even if null\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "# Canonical display name fallback\n",
    "df[\"display_name\"] = df[\"display_name\"].fillna(df[\"merchant_key\"])\n",
    "\n",
    "print(\"Labels joined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be3dff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_29992\\4184111192.py:47: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Subscription detection (sign-aware, idempotent) ---\n",
    "\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # use absolute spend magnitudes for stability\n",
    "    amounts = g[\"amount\"].abs().to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # gaps in days\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9)) if mu > 0 else 1.0\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Clean any leftover artifacts from previous runs (e.g., is_subscription_x from merges)\n",
    "for col in [c for c in df.columns if c.startswith(\"is_subscription\") and c != \"is_subscription\"]:\n",
    "    df.drop(columns=col, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Respect your sign convention\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "if EXPENSES_ARE_NEGATIVE:\n",
    "    outflows = df.loc[(df[\"amount\"] < 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "    outflows[\"amount\"] = outflows[\"amount\"].abs()\n",
    "else:\n",
    "    outflows = df.loc[(df[\"amount\"] > 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "\n",
    "subs_map = {}\n",
    "for disp, g in outflows.groupby(\"display_name\", dropna=False):\n",
    "    try:\n",
    "        subs_map[disp] = bool(detect_subscription(g[[\"date\", \"amount\"]]))\n",
    "    except Exception:\n",
    "        subs_map[disp] = False\n",
    "\n",
    "df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e514f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 1\n"
     ]
    }
   ],
   "source": [
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6675fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,541.28 (+2,537.77 vs prior)\n",
      "Top 3 merchants: WITHDRAWAL ALLY TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ACH ECC WEB ACH TRACE ($1,494.22), WITHDRAWAL AMEX EPAYMENT TYPE ACH PMT ID DATA ER AM CO AMEX EPAYMENT NAME KOSISONNA UGOCHUKWU ACH ECC WEB ACH TRACE ($777.78), PETAL ($738.96)\n",
      "Biggest category driver: nan ($2,868.17)\n",
      "\n",
      "Saved digest → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest → {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f17a1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut nan by 35%\n",
      "\n",
      "Saved goal nudges → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Suggest % cuts in top categories to reach GOAL_SAVINGS over next 30 days\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges → {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d67e86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new embeddings added (none missing or embeddings disabled).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 12: Build text and cache embeddings (reuse shared embed_client) ---\n",
    "def build_search_text(row):\n",
    "    parts = [\n",
    "        str(row.get(\"display_name\") or \"\"),\n",
    "        str(row.get(\"name\") or \"\"),\n",
    "        str(row.get(\"merchant_name\") or \"\"),\n",
    "        str(row.get(\"category\") or \"\"),\n",
    "        str(row.get(\"subcategory\") or \"\"),\n",
    "        str(row.get(\"tags\") or \"\"),\n",
    "    ]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Limit to recent rows for cost control\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df[~embed_df[\"txn_uid\"].isin(existing)][[\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "def get_embeddings(texts: list[str]) -> list | None:\n",
    "    if embed_client is None:\n",
    "        return None\n",
    "    # The model name is the deployment name on Azure\n",
    "    res = embed_client.embeddings.create(model=AZURE_OPENAI_EMBEDDINGS, input=list(texts))\n",
    "    # Return raw vectors (list[float]) as provided\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed) and embed_client is not None:\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        if vecs is None:\n",
    "            break\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} → total {len(merged)}\")\n",
    "else:\n",
    "    print(\"No new embeddings added (none missing or embeddings disabled).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7420962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched CSV saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "📄 Copy saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Reorder and save\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    \"category\",\"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\"is_subscription\",\"is_anomaly\",\"z_by_merchant\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Enriched CSV saved → {ENRICHED_OUT_PATH}\")\n",
    "print(f\"📄 Copy saved → {ENRICHED_COPY_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9d447e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Weekly executive digest written (WoW):\n",
      "- JSON: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.json\n",
      "- MD:   C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.md\n",
      "- CSV:  C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_flat.csv\n",
      "Window: 2025-09-01 -> 2025-09-07 | Prev: 2025-08-25 -> 2025-08-31\n",
      "Polarity (cur/global): pos/pos | cur +/− counts: 5/4 | cur spend total: 526.40 | prev spend total: 1629.20\n",
      "Filtered out Wealthfront rows this week: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 14: Weekly Executive Digest (WoW) — Wealthfront excluded, Apple Cash kept (Friends) + Azure overlay + flat CSV ---\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIGEST_JSON = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "DIGEST_MD   = INSIGHTS_DIR / \"digest_latest.md\"\n",
    "DIGEST_FLAT = INSIGHTS_DIR / \"digest_latest_flat.csv\"   # PBI-friendly\n",
    "\n",
    "# -------- 0) Digest-only filters & helpers --------\n",
    "def _upper_text_cols(frame):\n",
    "    # Safe uppercase concat of display_name / merchant_name / name\n",
    "    for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "        if c not in frame.columns:\n",
    "            frame[c] = \"\"\n",
    "    return (frame[\"display_name\"].astype(str) + \" \" +\n",
    "            frame[\"merchant_name\"].astype(str) + \" \" +\n",
    "            frame[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "def apply_digest_filters(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exclude Wealthfront moves (HYSA transfers/withdrawals), but do NOT exclude Apple Cash.\n",
    "    \"\"\"\n",
    "    txt = _upper_text_cols(frame)\n",
    "    is_wealthfront = txt.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "    is_apple_cash  = txt.str.contains(r\"\\bAPPLE CASH\\b\", na=False)\n",
    "    # Drop Wealthfront unless it's also Apple Cash (rare, but explicit)\n",
    "    keep_mask = ~(is_wealthfront & ~is_apple_cash)\n",
    "    out = frame.loc[keep_mask].copy()\n",
    "\n",
    "    # For digest presentation only: tidy Apple Cash labeling if blank/noisy\n",
    "    # - Display name -> \"APPLE CASH\"\n",
    "    # - Category -> \"Transfers: Friends\" if blank/NaN (you said it's usually sending to a friend)\n",
    "    ac_mask = _upper_text_cols(out).str.contains(r\"\\bAPPLE CASH\\b\", na=False)\n",
    "    if \"display_name\" not in out.columns:\n",
    "        out[\"display_name\"] = out.get(\"merchant_key\", \"\")\n",
    "    out.loc[ac_mask, \"display_name\"] = \"APPLE CASH\"\n",
    "    if \"category\" not in out.columns:\n",
    "        out[\"category\"] = \"\"\n",
    "    cat_blank = out[\"category\"].isna() | (out[\"category\"].astype(str).str.strip() == \"\")\n",
    "    out.loc[ac_mask & cat_blank, \"category\"] = \"Transfers to Friends/Family\"\n",
    "    return out\n",
    "\n",
    "# -------- 1) Last COMPLETED week (Mon–Sun), compare WoW --------\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wd = int(now.weekday())  # Mon=0 ... Sun=6\n",
    "days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()    # prior Monday\n",
    "prev_end = (pd.Timestamp(wk_end) - pd.Timedelta(days=7)).date()\n",
    "prev_start = (pd.Timestamp(prev_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "# Digest view: apply Wealthfront exclusion + Apple Cash tidy BEFORE slicing weeks\n",
    "df_w_all = apply_digest_filters(df.copy())\n",
    "df_w_all[\"date_only\"] = df_w_all[\"date\"].dt.date\n",
    "\n",
    "cur  = df_w_all[(df_w_all[\"date_only\"] >= wk_start) & (df_w_all[\"date_only\"] <= wk_end)]\n",
    "prev = df_w_all[(df_w_all[\"date_only\"] >= prev_start) & (df_w_all[\"date_only\"] <= prev_end)]\n",
    "\n",
    "# -------- 2) Robust polarity inference (current-week first, then fallback to full filtered set) --------\n",
    "def infer_orientation(frame) -> str | None:\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    pos = int((a > 0).sum())\n",
    "    neg = int((a < 0).sum())\n",
    "    if pos == 0 and neg == 0:\n",
    "        return None\n",
    "    return \"neg\" if neg > pos else \"pos\"\n",
    "\n",
    "orient_cur = infer_orientation(cur)\n",
    "orient_all = infer_orientation(df_w_all)\n",
    "orient = orient_cur or orient_all or \"pos\"  # default to positive-outflow if ambiguous\n",
    "\n",
    "def spend_series(frame, orient_hint: str) -> pd.Series:\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    if orient_hint == \"neg\":\n",
    "        s = a[a < 0].abs()\n",
    "        if s.empty and (a > 0).any():\n",
    "            s = a[a > 0]\n",
    "    else:\n",
    "        s = a[a > 0]\n",
    "        if s.empty and (a < 0).any():\n",
    "            s = a[a < 0].abs()\n",
    "    return s\n",
    "\n",
    "def income_series(frame, orient_hint: str) -> pd.Series:\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    if orient_hint == \"neg\":\n",
    "        inc = a[a > 0]\n",
    "        if inc.empty and (a < 0).any():\n",
    "            inc = a[a < 0].abs()\n",
    "    else:\n",
    "        inc = a[a < 0].abs()\n",
    "        if inc.empty and (a > 0).any():\n",
    "            inc = a[a > 0]\n",
    "    return inc\n",
    "\n",
    "cur_spend_ser   = spend_series(cur, orient)\n",
    "prev_spend_ser  = spend_series(prev, orient)\n",
    "cur_income_ser  = income_series(cur, orient)\n",
    "prev_income_ser = income_series(prev, orient)\n",
    "\n",
    "cur_spend   = round(float(cur_spend_ser.sum()), 2)\n",
    "prev_spend  = round(float(prev_spend_ser.sum()), 2)\n",
    "cur_income  = round(float(cur_income_ser.sum()), 2)\n",
    "prev_income = round(float(prev_income_ser.sum()), 2)\n",
    "\n",
    "spend_delta     = round(cur_spend - prev_spend, 2)\n",
    "spend_delta_pct = round((spend_delta / prev_spend), 4) if prev_spend else (1.0 if cur_spend else 0.0)\n",
    "\n",
    "# Top drivers this week based on the actual spend vector we used\n",
    "if not cur_spend_ser.empty:\n",
    "    cur_exp = cur.loc[cur_spend_ser.index].copy()\n",
    "    cur_exp = cur_exp.assign(spend=cur_spend_ser.values)\n",
    "else:\n",
    "    cur_exp = cur.assign(spend=0.0)\n",
    "\n",
    "top_merchants_cur = (\n",
    "    cur_exp.groupby(\"display_name\", dropna=False)[\"spend\"]\n",
    "          .sum().sort_values(ascending=False).head(5)\n",
    "          .reset_index()\n",
    ")\n",
    "top_cats_cur = (\n",
    "    cur_exp.groupby(\"category\", dropna=False)[\"spend\"]\n",
    "          .sum().sort_values(ascending=False).head(5)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "subs_w  = cur.loc[cur.get(\"is_subscription\", False) == True]\n",
    "anoms_w = cur.loc[cur.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "summary_payload = {\n",
    "    \"as_of_date\": pd.Timestamp(wk_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(wk_start), \"end\": str(wk_end), \"label\": \"Last completed week (Mon-Sun)\"},\n",
    "        \"previous\": {\"start\": str(prev_start), \"end\": str(prev_end)}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        \"spend_current\": cur_spend,\n",
    "        \"spend_previous\": prev_spend,\n",
    "        \"spend_delta\": spend_delta,\n",
    "        \"spend_delta_pct\": spend_delta_pct,\n",
    "        \"income_current\": cur_income,\n",
    "        \"income_previous\": prev_income,\n",
    "    },\n",
    "    \"top_merchants\": [\n",
    "        {\"display_name\": str(r[\"display_name\"]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_merchants_cur.iterrows()\n",
    "    ],\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[\"category\"]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_cur.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_w[\"display_name\"].nunique()) if len(subs_w) else 0,\n",
    "    \"anomalies_count\": int(anoms_w.shape[0]) if len(anoms_w) else 0,\n",
    "}\n",
    "\n",
    "# -------- 3) Azure summarizer (overlay JSON, never empty) --------\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided aggregates for the last completed week and the previous week, \"\n",
    "    \"produce an executive digest in STRICT JSON. Do not invent numbers. Keep it concise.\"\n",
    ")\n",
    "USER_INSTRUCTIONS = (\n",
    "    \"Compare the current week vs previous week (WoW). \"\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ]\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct for improvements if spend fell.\\n\"\n",
    "    \"- impact_usd is a rough weekly savings estimate.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call(payload_json: str) -> str:\n",
    "    assert chat_client is not None\n",
    "    resp = chat_client.chat.completions.create(  # Azure OpenAI\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=600,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Deterministic base digest (ASCII hyphens)\n",
    "base_digest = {\n",
    "    \"insights_version\": 2,\n",
    "    \"window\": summary_payload[\"window\"],\n",
    "    \"totals\": summary_payload[\"totals\"],\n",
    "    \"headline\": f\"Weekly digest {wk_start}-{wk_end}\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Spend (week)\",  \"value\": cur_spend,  \"delta_pct\": spend_delta_pct},\n",
    "        {\"name\":\"Income (week)\", \"value\": cur_income, \"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": t[\"category\"], \"spend\": float(t[\"spend\"])} for t in summary_payload[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload['subscriptions_count']} active subs this week\"}] if summary_payload[\"subscriptions_count\"] else [])\n",
    "        + ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload['anomalies_count']} anomalies this week\"}] if summary_payload[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": []\n",
    "}\n",
    "\n",
    "azure_digest = None\n",
    "if 'chat_client' in globals() and chat_client is not None:\n",
    "    try:\n",
    "        raw = _azure_digest_call(json.dumps(summary_payload))\n",
    "        azure_digest = _salvage_json_object(raw)\n",
    "    except Exception:\n",
    "        azure_digest = None\n",
    "\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "digest = _overlay(base_digest, azure_digest)\n",
    "\n",
    "# -------- 4) Compact summary string for text tile / email subject line --------\n",
    "def build_compact_summary(d: dict) -> str:\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    km = {m.get(\"name\",\"\"): m for m in d.get(\"key_metrics\", [])}\n",
    "    spend_m = km.get(\"Spend (week)\")\n",
    "    spend_val = float(spend_m.get(\"value\",0)) if spend_m else 0.0\n",
    "    dp = spend_m.get(\"delta_pct\") if spend_m else None\n",
    "    dp_txt = f\"{dp*100:+.1f}%\" if isinstance(dp,(int,float)) else \"n/a\"\n",
    "    top = (d.get(\"top_drivers\") or [])\n",
    "    if top:\n",
    "        top_label = top[0].get(\"label\") or \"\"\n",
    "        top_amt = float(top[0].get(\"spend\",0) or 0.0)\n",
    "        driver_txt = f\"Top driver: {top_label} (${top_amt:,.0f})\"\n",
    "    else:\n",
    "        driver_txt = \"Top driver: n/a\"\n",
    "    return f\"{ws}-{we}: Weekly spend ${spend_val:,.0f} (WoW {dp_txt}). {driver_txt}.\"\n",
    "\n",
    "digest[\"summary\"] = build_compact_summary(digest)\n",
    "\n",
    "# -------- 5) Persist JSON + Markdown + flat CSV --------\n",
    "with open(DIGEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def render_md(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    lines = [f\"## Weekly Digest: {ws}-{we}\", f\"{d.get('headline','Executive digest')}\"]\n",
    "    if d.get(\"summary\"):\n",
    "        lines.append(f\"\\n{d['summary']}\\n\")\n",
    "    km = d.get(\"key_metrics\", [])[:5]\n",
    "    if km:\n",
    "        lines.append(\"\\n**Key metrics (WoW)**\")\n",
    "        for m in km:\n",
    "            dp = m.get(\"delta_pct\", None)\n",
    "            dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "            lines.append(f\"- {m['name']}: ${m['value']:,.2f}{dp_txt}\")\n",
    "    td = d.get(\"top_drivers\", [])[:5]\n",
    "    if td:\n",
    "        lines.append(\"\\n**Top drivers (this week)**\")\n",
    "        for t in td:\n",
    "            label = t.get(\"label\") or t.get(\"display_name\") or \"\"\n",
    "            lines.append(f\"- {label}: ${float(t.get('spend',0)):,.2f}\")\n",
    "    rk = d.get(\"risks\", [])[:5]\n",
    "    if rk:\n",
    "        lines.append(\"\\n**Risks**\")\n",
    "        for r in rk:\n",
    "            lines.append(f\"- {r.get('type','note')}: {r.get('note','')}\")\n",
    "    ai = d.get(\"action_items\", [])[:5]\n",
    "    if ai:\n",
    "        lines.append(\"\\n**Action items**\")\n",
    "        for a in ai:\n",
    "            lines.append(f\"- {a['title']} — est. weekly impact ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(DIGEST_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md(digest))\n",
    "\n",
    "# Flat table for Power BI ingestion (one row per element with type)\n",
    "flat_rows = []\n",
    "\n",
    "# Header row (window + totals; includes summary string)\n",
    "flat_rows.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(wk_end),\n",
    "    \"cur_start\": str(wk_start),\n",
    "    \"cur_end\": str(wk_end),\n",
    "    \"prev_start\": str(prev_start),\n",
    "    \"prev_end\": str(prev_end),\n",
    "    \"headline\": digest.get(\"headline\", \"\"),\n",
    "    \"summary\": digest.get(\"summary\", \"\"),\n",
    "    \"name\": \"Spend (week)\",\n",
    "    \"value\": cur_spend,\n",
    "    \"delta_pct\": spend_delta_pct,\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "\n",
    "# Dedicated summary row for a text card\n",
    "flat_rows.append({\n",
    "    \"row_type\": \"summary\",\n",
    "    \"as_of_end\": str(wk_end),\n",
    "    \"cur_start\": str(wk_start),\n",
    "    \"cur_end\": str(wk_end),\n",
    "    \"prev_start\": str(prev_start),\n",
    "    \"prev_end\": str(prev_end),\n",
    "    \"headline\": digest.get(\"headline\", \"\"),\n",
    "    \"summary\": digest.get(\"summary\", \"\"),\n",
    "    \"name\": \"\",\n",
    "    \"value\": None,\n",
    "    \"delta_pct\": None,\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "\n",
    "# Key metrics\n",
    "for m in digest.get(\"key_metrics\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Top drivers\n",
    "for t in digest.get(\"top_drivers\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": t.get(\"label\",\"\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Risks\n",
    "for r in digest.get(\"risks\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "\n",
    "# Action items\n",
    "for a in digest.get(\"action_items\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"action\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\", \"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": a.get(\"title\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": a.get(\"rationale\",\"\"),\n",
    "        \"impact_usd\": float(a.get(\"impact_usd\",0) or 0.0),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(flat_rows).to_csv(DIGEST_FLAT, index=False)\n",
    "\n",
    "# -------- 6) Debug footer + quick counts after filtering --------\n",
    "print(\n",
    "    \"🧠 Weekly executive digest written (WoW):\\n\"\n",
    "    f\"- JSON: {DIGEST_JSON}\\n- MD:   {DIGEST_MD}\\n- CSV:  {DIGEST_FLAT}\\n\"\n",
    "    f\"Window: {wk_start} -> {wk_end} | Prev: {prev_start} -> {prev_end}\\n\"\n",
    "    f\"Polarity (cur/global): {orient_cur}/{orient_all} | cur +/− counts: \"\n",
    "    f\"{int((cur['amount']>0).sum())}/{int((cur['amount']<0).sum())} | \"\n",
    "    f\"cur spend total: {cur_spend:.2f} | prev spend total: {prev_spend:.2f}\\n\"\n",
    "    f\"Filtered out Wealthfront rows this week: \"\n",
    "    f\"{int((_upper_text_cols(df).str.contains('WEALTHFRONT', na=False)).sum()) - int((_upper_text_cols(df_w_all).str.contains('WEALTHFRONT', na=False)).sum())}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f0afa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✉️ Weekly AI summary written:\n",
      "- Subject (csv): C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_email_subject.csv\n",
      "- Subject (txt): C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_email_subject.txt\n",
      "- Markdown:      C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_email.md\n",
      "- HTML:          C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_email.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 15: Weekly AI narrative (Theme + MD + HTML + Subject CSV, fixed structure) ---\n",
    "import json, html, csv, re\n",
    "from pathlib import Path\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMAIL_MD_PATH    = INSIGHTS_DIR / \"digest_latest_email.md\"\n",
    "EMAIL_HTML_PATH  = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "EMAIL_SUBJ_PATH  = INSIGHTS_DIR / \"digest_latest_email_subject.txt\"\n",
    "EMAIL_SUBJ_CSV   = INSIGHTS_DIR / \"digest_latest_email_subject.csv\"  # single-column CSV\n",
    "\n",
    "def _get_chat_client():\n",
    "    if 'chat_client' in globals() and chat_client is not None: return chat_client\n",
    "    if 'client' in globals() and client is not None: return client\n",
    "    return None\n",
    "\n",
    "def _safe_num(x, default=0.0):\n",
    "    try: return float(x)\n",
    "    except Exception: return default\n",
    "\n",
    "def _short_range_dash(ws_str, we_str):\n",
    "    try:\n",
    "        ws = pd.to_datetime(ws_str).date()\n",
    "        we = pd.to_datetime(we_str).date()\n",
    "        return f\"{ws.month}-{ws.day} to {we.month}-{we.day}\"\n",
    "    except Exception:\n",
    "        return f\"{ws_str} to {we_str}\"\n",
    "\n",
    "def _strip_merge_markers(s: str) -> str:\n",
    "    return re.sub(r\"<<<<<<<.*?=======|>>>>>>>.*?$\", \"\", s, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "def _sanitize_subject(s: str) -> str:\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = _strip_merge_markers(s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Deterministic fallback subject\n",
    "def _fallback_subject(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", \"\")\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", \"\")\n",
    "    short = _short_range_dash(ws, we)\n",
    "    km = {m.get(\"name\",\"\"): m for m in d.get(\"key_metrics\", [])}\n",
    "    spend_m = km.get(\"Spend (week)\")\n",
    "    spend_val = _safe_num(spend_m.get(\"value\")) if spend_m else 0.0\n",
    "    dp = spend_m.get(\"delta_pct\") if spend_m else None\n",
    "    dp_txt = f\"{float(dp)*100:+.1f}%\" if isinstance(dp,(int,float)) else \"n/a\"\n",
    "    return f\"This Week {short} — Spend ${spend_val:,.0f} (WoW {dp_txt})\"\n",
    "\n",
    "# Simple MD->HTML renderer (headings + bullets)\n",
    "def _simple_html_from_md(md_text, title_color=\"#0f172a\"):\n",
    "    html_lines = []\n",
    "    open_ul = False\n",
    "    for raw in md_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if line.startswith(\"# \"):\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f'<h1 style=\"margin:0 0 12px 0;color:{title_color};font-weight:800;\">{html.escape(line[2:])}</h1>')\n",
    "        elif line.startswith(\"## \"):\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f'<h2 style=\"margin:20px 0 10px 0;color:{title_color};font-weight:700;\">{html.escape(line[3:])}</h2>')\n",
    "        elif line.startswith(\"- \"):\n",
    "            if not open_ul:\n",
    "                html_lines.append(\"<ul style='margin:4px 0 12px 22px;padding:0;'>\"); open_ul=True\n",
    "            html_lines.append(f\"<li>{html.escape(line[2:])}</li>\")\n",
    "        elif line == \"\":\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append('<div style=\"height:6px\"></div>')\n",
    "        else:\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f\"<p style='margin:6px 0'>{html.escape(line)}</p>\")\n",
    "    if open_ul: html_lines.append(\"</ul>\")\n",
    "    body = \"\\n\".join(html_lines)\n",
    "    return f\"<!doctype html><meta charset='utf-8'><div style='font-family:Segoe UI,system-ui,-apple-system,Arial;line-height:1.45;font-size:14px;color:#111827;'>{body}</div>\"\n",
    "\n",
    "# Persona + prompts\n",
    "persona = os.getenv(\"AI_SUMMARY_PERSONA\", \"Crisp, witty, CFO-style, confident\")\n",
    "ws = digest.get(\"window\", {}).get(\"current\", {}).get(\"start\", \"\")\n",
    "we = digest.get(\"window\", {}).get(\"current\", {}).get(\"end\", \"\")\n",
    "short_range = _short_range_dash(ws, we)\n",
    "\n",
    "payload_json = json.dumps({\n",
    "    \"window\": digest.get(\"window\", {}),\n",
    "    \"totals\": digest.get(\"totals\", {}),\n",
    "    \"key_metrics\": digest.get(\"key_metrics\", []),\n",
    "    \"top_drivers\": digest.get(\"top_drivers\", []),\n",
    "    \"risks\": digest.get(\"risks\", []),\n",
    "    \"action_items\": digest.get(\"action_items\", []),\n",
    "    \"summary_line\": digest.get(\"summary\", \"\")\n",
    "}, ensure_ascii=False)\n",
    "\n",
    "THEME_SYSTEM = (\n",
    "    \"Return ONLY a 2–3 word theme that reflects the *data* (e.g., when spend is sharply down, a frugal vibe; \"\n",
    "    \"when up, a celebratory/alert vibe). Keep it tasteful; no emojis; no trailing punctuation.\"\n",
    ")\n",
    "\n",
    "SUBJECT_SYSTEM = (\n",
    "    \"Return ONLY an email subject, 8–12 words, including: the short date range (M-D to M-D), \"\n",
    "    \"weekly spend and WoW delta if present, and the theme. No quotes, no newlines.\"\n",
    ")\n",
    "\n",
    "SUMMARY_SYSTEM = (\n",
    "    f\"You are an analytics copywriter. Tone: {persona}. \"\n",
    "    \"Output **Markdown only** with this **exact structure** and exactly one top-level H1:\\n\"\n",
    "    f\"# <Theme> — Weekly Executive Summary ({short_range})\\n\"\n",
    "    \"\\n\"\n",
    "    \"Then H2 sections in this order (no extras, no repeats):\\n\"\n",
    "    \"## Snapshot\\n\"\n",
    "    \"## Drivers\\n\"\n",
    "    \"## Category Mix\\n\"\n",
    "    \"## Subscriptions & Anomalies\\n\"\n",
    "    \"## Cash Flow\\n\"\n",
    "    \"## Notables\\n\"\n",
    "    \"## Recommendations\\n\"\n",
    "    \"## Next Week Watchlist\\n\"\n",
    "    \"\\n\"\n",
    "    \"- Base the mood and wording on the numbers you are given.\\n\"\n",
    "    \"- Use very short lines or bullets under each section; do not repeat the H1 or add another H1.\\n\"\n",
    "    \"- In 'Drivers', include a short 'Top Spending Categories:' line then 3–4 bullets using the provided aggregates.\\n\"\n",
    ")\n",
    "\n",
    "chat = _get_chat_client()\n",
    "subject_txt = _fallback_subject(digest)\n",
    "md_txt = None\n",
    "theme_txt = \"Weekly Snapshot\"\n",
    "\n",
    "try:\n",
    "    if chat is not None:\n",
    "        # Theme (data-aware)\n",
    "        theme_resp = chat.chat_completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=[{\"role\":\"system\",\"content\": THEME_SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\": \"Aggregates:\\n\"+payload_json}],\n",
    "            temperature=0.6, max_tokens=16,\n",
    "        )\n",
    "        theme_txt = (theme_resp.choices[0].message.content or \"\").strip() or theme_txt\n",
    "\n",
    "        # Subject\n",
    "        subj_resp = chat.chat_completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=[{\"role\":\"system\",\"content\": SUBJECT_SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\": f\"Date range: {short_range}\\nAggregates:\\n{payload_json}\\nTheme: {theme_txt}\"}],\n",
    "            temperature=0.3, max_tokens=64,\n",
    "        )\n",
    "        st = (subj_resp.choices[0].message.content or \"\").strip()\n",
    "        subject_txt = st or subject_txt\n",
    "\n",
    "        # Body (single H1 + exact sections)\n",
    "        body_resp = chat.chat_completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=[{\"role\":\"system\",\"content\": SUMMARY_SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\": f\"Theme: {theme_txt}\\nAggregates:\\n{payload_json}\"}],\n",
    "            temperature=0.45, max_tokens=1200,\n",
    "        )\n",
    "        mt = (body_resp.choices[0].message.content or \"\").strip()\n",
    "        md_txt = mt if mt else None\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Fallback MD if AI not available\n",
    "if md_txt is None:\n",
    "    # Build a compact, sectioned fallback from deterministic data\n",
    "    km = {m.get(\"name\",\"\"): m for m in digest.get(\"key_metrics\", [])}\n",
    "    spend_m = km.get(\"Spend (week)\")\n",
    "    spend_val = _safe_num(spend_m.get(\"value\")) if spend_m else 0.0\n",
    "    dp = spend_m.get(\"delta_pct\") if spend_m else None\n",
    "    dp_txt = f\"{float(dp)*100:+.1f}%\" if isinstance(dp,(int,float)) else \"n/a\"\n",
    "    intro = f\"Welcome to your weekly check-in. Spend ${spend_val:,.2f} (WoW {dp_txt}).\"\n",
    "    top = digest.get(\"top_drivers\", [])[:4]\n",
    "    lines = [f\"# {theme_txt} — Weekly Executive Summary ({short_range})\",\n",
    "             \"## Snapshot\",\n",
    "             f\"- Total Spend: ${spend_val:,.2f} (WoW {dp_txt})\",\n",
    "             \"## Drivers\",\n",
    "             \"Top Spending Categories:\"]\n",
    "    for t in top:\n",
    "        lines.append(f\"- {t.get('label','')}: ${_safe_num(t.get('spend')):,.0f}\")\n",
    "    lines += [\n",
    "        \"## Category Mix\",\n",
    "        \"- Mix stable; see dashboard for breakdown.\",\n",
    "        \"## Subscriptions & Anomalies\",\n",
    "        f\"- Anomalies: {digest.get('anomalies_count', 0)}\",\n",
    "        \"## Cash Flow\",\n",
    "        \"- Net cash flow covered this week.\",\n",
    "        \"## Notables\",\n",
    "        \"- Largest line item listed above.\",\n",
    "        \"## Recommendations\",\n",
    "        \"- Keep discretionary spend in check.\",\n",
    "        \"## Next Week Watchlist\",\n",
    "        \"- Review anomalies; monitor categories that may rebound.\",\n",
    "    ]\n",
    "    md_txt = \"\\n\".join([intro, \"\"] + lines) + \"\\n\"\n",
    "\n",
    "# Final sanitize (no merge markers / duplicates)\n",
    "subject_txt = _sanitize_subject(subject_txt)\n",
    "md_txt = _strip_merge_markers(md_txt)\n",
    "# Ensure only one H1 that starts with \"<Theme> — Weekly Executive Summary\"\n",
    "lines = [l for l in md_txt.splitlines()]\n",
    "first_h1_seen = False\n",
    "clean = []\n",
    "for l in lines:\n",
    "    if l.startswith(\"# \"):\n",
    "        if first_h1_seen:\n",
    "            # drop any later H1 lines\n",
    "            continue\n",
    "        first_h1_seen = True\n",
    "        clean.append(l)\n",
    "    else:\n",
    "        clean.append(l)\n",
    "md_txt = \"\\n\".join(clean)\n",
    "\n",
    "# Write outputs\n",
    "EMAIL_MD_PATH.write_text(md_txt, encoding=\"utf-8\")\n",
    "EMAIL_HTML_PATH.write_text(_simple_html_from_md(md_txt), encoding=\"utf-8\")\n",
    "EMAIL_SUBJ_PATH.write_text(subject_txt, encoding=\"utf-8\")\n",
    "with open(EMAIL_SUBJ_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"subject\"]); writer.writeheader(); writer.writerow({\"subject\": subject_txt})\n",
    "\n",
    "print(\n",
    "    \"✉️ Weekly AI summary written:\\n\"\n",
    "    f\"- Subject (csv): {EMAIL_SUBJ_CSV}\\n\"\n",
    "    f\"- Subject (txt): {EMAIL_SUBJ_PATH}\\n\"\n",
    "    f\"- Markdown:      {EMAIL_MD_PATH}\\n\"\n",
    "    f\"- HTML:          {EMAIL_HTML_PATH}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe61641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📆 MoM AI summary written:\n",
      "- Subject (csv): C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_email_subject.csv\n",
      "- Subject (txt): C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_email_subject.txt\n",
      "- Markdown:      C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_email.md\n",
      "- HTML:          C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_email.html\n",
      "- Flat CSV:      C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_flat.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 16: Monthly AI narrative (Month name subject, macro tone, fixed structure) ---\n",
    "import calendar, csv, html, json, re\n",
    "\n",
    "M_INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "M_INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "M_JSON  = M_INSIGHTS_DIR / \"digest_mom.json\"\n",
    "M_MD    = M_INSIGHTS_DIR / \"digest_mom_email.md\"\n",
    "M_HTML  = M_INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "M_SUBJ  = M_INSIGHTS_DIR / \"digest_mom_email_subject.txt\"\n",
    "M_SUBJ_CSV = M_INSIGHTS_DIR / \"digest_mom_email_subject.csv\"\n",
    "M_FLAT  = M_INSIGHTS_DIR / \"digest_mom_flat.csv\"\n",
    "\n",
    "def _strip_merge_markers(s: str) -> str:\n",
    "    return re.sub(r\"<<<<<<<.*?=======|>>>>>>>.*?$\", \"\", s, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "def _sanitize_one_line(s: str) -> str:\n",
    "    s = s.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "    s = _strip_merge_markers(s)\n",
    "    s = re.sub(r\"\\s{2,}\",\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _month_bounds(ts: pd.Timestamp):\n",
    "    y, m = ts.year, ts.month\n",
    "    start = pd.Timestamp(year=y, month=m, day=1).date()\n",
    "    last_day = calendar.monthrange(y, m)[1]\n",
    "    end = pd.Timestamp(year=y, month=m, day=last_day).date()\n",
    "    return start, end\n",
    "\n",
    "# last completed month (America/Los_Angeles)\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\")\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now()\n",
    "first_of_this_month = pd.Timestamp(year=now.year, month=now.month, day=1, tz=getattr(now, 'tz', None))\n",
    "prev_month_end = (first_of_this_month - pd.Timedelta(days=1)).tz_localize(None) if hasattr(first_of_this_month, 'tz') else (first_of_this_month - pd.Timedelta(days=1))\n",
    "cm_start, cm_end = _month_bounds(pd.Timestamp(year=prev_month_end.year, month=prev_month_end.month, day=1))\n",
    "pm_end_dt = pd.Timestamp(cm_start) - pd.Timedelta(days=1)\n",
    "pm_start, pm_end = _month_bounds(pm_end_dt)\n",
    "\n",
    "# Wealthfront filter (keep Apple Cash)\n",
    "base = df.copy()\n",
    "def _concat_upper(frame):\n",
    "    for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "        if c not in frame.columns: frame[c] = \"\"\n",
    "    return (frame[\"display_name\"].astype(str)+\" \"+frame[\"merchant_name\"].astype(str)+\" \"+frame[\"name\"].astype(str)).str.upper()\n",
    "txt_all = _concat_upper(base)\n",
    "base = base.loc[~(txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False) & ~txt_all.str.contains(r\"\\bAPPLE CASH\\b\", na=False))].copy()\n",
    "\n",
    "# Slices\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "cur  = base[(base[\"date_only\"] >= cm_start) & (base[\"date_only\"] <= cm_end)]\n",
    "prev = base[(base[\"date_only\"] >= pm_start) & (base[\"date_only\"] <= pm_end)]\n",
    "\n",
    "def _infer_orient(frame) -> str | None:\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    pos, neg = int((a > 0).sum()), int((a < 0).sum())\n",
    "    if pos == 0 and neg == 0: return None\n",
    "    return \"neg\" if neg > pos else \"pos\"\n",
    "orient = _infer_orient(cur) or _infer_orient(base) or \"pos\"\n",
    "\n",
    "def _series_spend(frame, hint: str):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    s = a[a < 0].abs() if hint == \"neg\" else a[a > 0]\n",
    "    if s.empty: s = a[a > 0] if hint == \"neg\" else a[a < 0].abs()\n",
    "    return s\n",
    "def _series_income(frame, hint: str):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    inc = a[a > 0] if hint == \"neg\" else a[a < 0].abs()\n",
    "    if inc.empty: inc = a[a < 0].abs() if hint == \"neg\" else a[a > 0]\n",
    "    return inc\n",
    "\n",
    "cur_spend   = float(_series_spend(cur, orient).sum())\n",
    "prev_spend  = float(_series_spend(prev, orient).sum())\n",
    "cur_income  = float(_series_income(cur, orient).sum())\n",
    "prev_income = float(_series_income(prev, orient).sum())\n",
    "delta = cur_spend - prev_spend\n",
    "delta_pct = (delta / prev_spend) if prev_spend else (1.0 if cur_spend else 0.0)\n",
    "\n",
    "# Drivers by category (ensure no NaN)\n",
    "cur_cat = cur.copy()\n",
    "cur_cat[\"category\"] = cur_cat[\"category\"].fillna(\"Uncategorized\").replace(\"\", \"Uncategorized\")\n",
    "svec = _series_spend(cur_cat, orient)\n",
    "cur_exp = cur_cat.loc[svec.index].assign(spend=svec.values) if not cur_cat.empty else cur_cat.assign(spend=0.0)\n",
    "top_cats = (cur_exp.groupby(\"category\", dropna=False)[\"spend\"]\n",
    "            .sum().sort_values(ascending=False).head(5).reset_index())\n",
    "\n",
    "subs_m = cur.loc[cur.get(\"is_subscription\", False) == True]\n",
    "anoms_m = cur.loc[cur.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "mom_payload = {\n",
    "    \"window\": {\"current\": {\"start\": str(cm_start), \"end\": str(cm_end), \"label\": \"Last completed month\"},\n",
    "               \"previous\": {\"start\": str(pm_start), \"end\": str(pm_end)}},\n",
    "    \"totals\": {\"spend_current\": round(cur_spend, 2), \"spend_previous\": round(prev_spend, 2),\n",
    "               \"spend_delta\": round(delta, 2), \"spend_delta_pct\": round(delta_pct, 4),\n",
    "               \"income_current\": round(cur_income, 2), \"income_previous\": round(prev_income, 2)},\n",
    "    \"top_categories\": [{\"category\": str(r[\"category\"]), \"spend\": float(r[\"spend\"])} for _, r in top_cats.iterrows()],\n",
    "    \"subscriptions_count\": int(subs_m[\"display_name\"].nunique()) if len(subs_m) else 0,\n",
    "    \"anomalies_count\": int(anoms_m.shape[0]) if len(anoms_m) else 0,\n",
    "}\n",
    "with open(M_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mom_payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Month-name subject (computed, macro lens)\n",
    "month_name = pd.to_datetime(mom_payload[\"window\"][\"current\"][\"start\"]).strftime(\"%B\")\n",
    "val = mom_payload[\"totals\"][\"spend_current\"]\n",
    "pct = mom_payload[\"totals\"][\"spend_delta_pct\"] * 100.0\n",
    "trend_word = \"Up\" if pct >= 0 else \"Down\"\n",
    "msubj = f\"{month_name} Spend: ${val:,.0f} — {trend_word} {abs(pct):.0f}% MoM\"\n",
    "msubj = _sanitize_one_line(msubj)\n",
    "M_SUBJ.write_text(msubj, encoding=\"utf-8\")\n",
    "with open(M_SUBJ_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"subject\"]); writer.writeheader(); writer.writerow({\"subject\": msubj})\n",
    "\n",
    "# Ask AI for the *body* in the same style, but toned down\n",
    "def _simple_html_from_md(md_text, title_color=\"#0f172a\"):\n",
    "    html_lines, open_ul = [], False\n",
    "    for raw in md_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if line.startswith(\"# \"):\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f'<h1 style=\"margin:0 0 12px 0;color:{title_color};font-weight:800;\">{html.escape(line[2:])}</h1>')\n",
    "        elif line.startswith(\"## \"):\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f'<h2 style=\"margin:20px 0 10px 0;color:{title_color};font-weight:700;\">{html.escape(line[3:])}</h2>')\n",
    "        elif line.startswith(\"- \"):\n",
    "            if not open_ul:\n",
    "                html_lines.append(\"<ul style='margin:4px 0 12px 22px;padding:0;'>\"); open_ul=True\n",
    "            html_lines.append(f\"<li>{html.escape(line[2:])}</li>\")\n",
    "        elif line == \"\":\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append('<div style=\"height:6px\"></div>')\n",
    "        else:\n",
    "            if open_ul: html_lines.append(\"</ul>\"); open_ul=False\n",
    "            html_lines.append(f\"<p style='margin:6px 0'>{html.escape(line)}</p>\")\n",
    "    if open_ul: html_lines.append(\"</ul>\")\n",
    "    body = \"\\n\".join(html_lines)\n",
    "    return f\"<!doctype html><meta charset='utf-8'><div style='font-family:Segoe UI,system-ui,-apple-system,Arial;line-height:1.45;font-size:14px;color:#111827;'>{body}</div>\"\n",
    "\n",
    "chat = None\n",
    "if 'chat_client' in globals() and chat_client is not None: chat = chat_client\n",
    "elif 'client' in globals() and client is not None: chat = client\n",
    "\n",
    "short_month = month_name  # for header\n",
    "payload_json_m = json.dumps(mom_payload, ensure_ascii=False)\n",
    "\n",
    "THEME_MO_SYSTEM = (\n",
    "    \"Return ONLY a 2–3 word theme for a monthly personal finance report, \"\n",
    "    \"based on the provided aggregates. No punctuation beyond hyphens.\"\n",
    ")\n",
    "BODY_MO_SYSTEM = (\n",
    "    \"You are an analytics copywriter. Tone: concise, macro, CFO-style. \"\n",
    "    \"Output Markdown with exactly one H1 and these H2 sections (no extras, no repeats):\\n\"\n",
    "    f\"# <Theme> — Monthly Executive Summary ({short_month})\\n\"\n",
    "    \"## Snapshot\\n\"\n",
    "    \"## Key Metrics\\n\"\n",
    "    \"## Drivers (Top Categories)\\n\"\n",
    "    \"## Category Mix\\n\"\n",
    "    \"## Subscriptions & Anomalies\\n\"\n",
    "    \"## Cash Flow\\n\"\n",
    "    \"## Notables\\n\"\n",
    "    \"## Next Month Watchlist\\n\"\n",
    "    \"- Keep lines short (≤12 words) and macro in perspective.\\n\"\n",
    ")\n",
    "\n",
    "md_text = None\n",
    "try:\n",
    "    if chat is not None:\n",
    "        # Theme\n",
    "        theme_resp = chat.chat_completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=[{\"role\":\"system\",\"content\": THEME_MO_SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\": \"Aggregates:\\n\"+payload_json_m}],\n",
    "            temperature=0.6, max_tokens=16,\n",
    "        )\n",
    "        theme_m = (theme_resp.choices[0].message.content or \"\").strip() or f\"{short_month} Overview\"\n",
    "\n",
    "        # Body\n",
    "        body_resp = chat.chat_completions.create(\n",
    "            model=AZURE_OPENAI_DEPLOYMENT,\n",
    "            messages=[{\"role\":\"system\",\"content\": BODY_MO_SYSTEM},\n",
    "                      {\"role\":\"user\",\"content\": f\"Theme: {theme_m}\\nAggregates:\\n{payload_json_m}\"}],\n",
    "            temperature=0.4, max_tokens=1000,\n",
    "        )\n",
    "        md_text = (body_resp.choices[0].message.content or \"\").strip()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if md_text is None:\n",
    "    # Fallback: simple macro summary\n",
    "    md_lines = [\n",
    "        f\"# {short_month} — Monthly Executive Summary\",\n",
    "        \"## Snapshot\",\n",
    "        f\"- Spend: ${mom_payload['totals']['spend_current']:,.2f} (MoM {mom_payload['totals']['spend_delta_pct']*100:+.1f}%)\",\n",
    "        f\"- Income: ${mom_payload['totals']['income_current']:,.2f}\",\n",
    "        \"## Key Metrics\",\n",
    "        f\"- Prev Spend: ${mom_payload['totals']['spend_previous']:,.2f}\",\n",
    "        f\"- Delta: ${mom_payload['totals']['spend_delta']:,.2f}\",\n",
    "        \"## Drivers (Top Categories)\",\n",
    "    ]\n",
    "    for t in mom_payload[\"top_categories\"]:\n",
    "        md_lines.append(f\"- {t['category']}: ${t['spend']:,.0f}\")\n",
    "    md_lines += [\n",
    "        \"## Category Mix\",\n",
    "        \"- Mix stable; see dashboard.\",\n",
    "        \"## Subscriptions & Anomalies\",\n",
    "        f\"- Subscriptions: {mom_payload['subscriptions_count']} | Anomalies: {mom_payload['anomalies_count']}\",\n",
    "        \"## Cash Flow\",\n",
    "        \"- Macro cash position stable.\",\n",
    "        \"## Notables\",\n",
    "        \"- Largest categories listed above.\",\n",
    "        \"## Next Month Watchlist\",\n",
    "        \"- Monitor categories with rising trend.\",\n",
    "    ]\n",
    "    md_text = \"\\n\".join(md_lines) + \"\\n\"\n",
    "\n",
    "# Sanitize & write\n",
    "md_text = _strip_merge_markers(md_text)\n",
    "lines = md_text.splitlines()\n",
    "first_h1=False\n",
    "clean=[]\n",
    "for l in lines:\n",
    "    if l.startswith(\"# \"):\n",
    "        if first_h1: continue\n",
    "        first_h1=True\n",
    "    clean.append(l)\n",
    "md_text = \"\\n\".join(clean)\n",
    "\n",
    "M_MD.write_text(md_text, encoding=\"utf-8\")\n",
    "M_HTML.write_text(_simple_html_from_md(md_text), encoding=\"utf-8\")\n",
    "\n",
    "# Flat CSV (unchanged from earlier)\n",
    "flat = [{\n",
    "    \"row_type\":\"header\",\n",
    "    \"cur_start\": mom_payload[\"window\"][\"current\"][\"start\"],\n",
    "    \"cur_end\":   mom_payload[\"window\"][\"current\"][\"end\"],\n",
    "    \"prev_start\":mom_payload[\"window\"][\"previous\"][\"start\"],\n",
    "    \"prev_end\":  mom_payload[\"window\"][\"previous\"][\"end\"],\n",
    "    \"name\":\"Spend (month)\",\n",
    "    \"value\": mom_payload[\"totals\"][\"spend_current\"],\n",
    "    \"delta_pct\": mom_payload[\"totals\"][\"spend_delta_pct\"],\n",
    "    \"label\":\"\", \"spend\":None, \"note\":\"\", \"impact_usd\":None\n",
    "}]\n",
    "for t in mom_payload[\"top_categories\"]:\n",
    "    flat.append({\n",
    "        \"row_type\":\"driver\",\"cur_start\":mom_payload[\"window\"][\"current\"][\"start\"],\"cur_end\":mom_payload[\"window\"][\"current\"][\"end\"],\n",
    "        \"prev_start\":mom_payload[\"window\"][\"previous\"][\"start\"],\"prev_end\":mom_payload[\"window\"][\"previous\"][\"end\"],\n",
    "        \"name\":\"\", \"value\":None, \"delta_pct\":None, \"label\":t[\"category\"], \"spend\":t[\"spend\"], \"note\":\"\", \"impact_usd\":None\n",
    "    })\n",
    "pd.DataFrame(flat).to_csv(M_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"📆 MoM AI summary written:\\n\"\n",
    "    f\"- Subject (csv): {M_SUBJ_CSV}\\n\"\n",
    "    f\"- Subject (txt): {M_SUBJ}\\n\"\n",
    "    f\"- Markdown:      {M_MD}\\n\"\n",
    "    f\"- HTML:          {M_HTML}\\n\"\n",
    "    f\"- Flat CSV:      {M_FLAT}\\n\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
