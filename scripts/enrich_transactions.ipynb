{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf10efdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:10.775841Z",
     "iopub.status.busy": "2025-09-13T23:16:10.774848Z",
     "iopub.status.idle": "2025-09-13T23:16:14.038548Z",
     "shell.execute_reply": "2025-09-13T23:16:14.038025Z"
    },
    "papermill": {
     "duration": 3.283808,
     "end_time": "2025-09-13T23:16:14.042970",
     "exception": false,
     "start_time": "2025-09-13T23:16:10.759162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure config â†’ endpoint: httpâ€¦.com | key: 1kTHâ€¦msql | chat deployment: gpt-4o-mini | version: 2024-02-15-preview\n",
      "âœ… AzureOpenAI client initialized (deployment-aware).\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 1: Imports + robust Azure OpenAI init (original) ---\n",
    "import os, re, json, math, hashlib, ast\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure OpenAI SDK\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# Optional dotenv to load local secrets\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "    find_dotenv = None\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:4] + \"â€¦\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# --- Paths (robust) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "start = Path(gw).resolve() if gw else cwd\n",
    "repo_root = next((p for p in [start, *start.parents] if (p / \".git\").exists()), start)\n",
    "REPO = repo_root\n",
    "\n",
    "DATA_RAW       = REPO / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = REPO / \"data\" / \"processed\"\n",
    "CONFIG_DIR     = REPO / \"config\"\n",
    "STATE_DIR      = REPO / \".state\"\n",
    "VECTOR_DIR     = REPO / \"vectorstore\"\n",
    "\n",
    "MERCHANT_DIM_PATH  = CONFIG_DIR / \"merchants_dim.csv\"\n",
    "LATEST_CSV_PATH    = DATA_RAW / \"latest.csv\"\n",
    "ENRICHED_OUT_PATH  = DATA_RAW / \"latest.csv\"               # overwrite stable file for Power BI\n",
    "ENRICHED_COPY_PATH = DATA_PROCESSED / \"latest_enriched.csv\"\n",
    "DIGEST_PATH        = DATA_PROCESSED / \"digest_latest.txt\"\n",
    "GOAL_PATH          = DATA_PROCESSED / \"goal_nudges_latest.txt\"\n",
    "EMBEDDINGS_PATH    = VECTOR_DIR / \"embeddings.parquet\"\n",
    "\n",
    "# Ensure dirs\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config flags\n",
    "MAP_ALL    = True\n",
    "GOAL_SAVINGS = 1000.0\n",
    "ANOMALY_Z  = 2.5\n",
    "\n",
    "# --- Load .envs (mirror the build notebook behavior) ---\n",
    "def load_envs():\n",
    "    if load_dotenv is None:\n",
    "        return\n",
    "    # Explicit override\n",
    "    abs_override = os.getenv(\"ENV_PATH\", str(REPO / \"scripts\" / \".env\"))\n",
    "    if abs_override and Path(abs_override).exists():\n",
    "        try:\n",
    "            load_dotenv(abs_override, override=False, encoding=\"utf-8\")\n",
    "        except TypeError:\n",
    "            load_dotenv(abs_override, override=False)\n",
    "    # Common locations\n",
    "    for p in [\n",
    "        REPO / \"scripts\" / \".env\",\n",
    "        REPO / \".env\",\n",
    "        REPO / \"config\" / \".env\",\n",
    "        cwd / \".env\",\n",
    "    ]:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                load_dotenv(str(p), override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(str(p), override=False)\n",
    "    # Last-ditch: auto-find\n",
    "    if find_dotenv:\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        if found:\n",
    "            try:\n",
    "                load_dotenv(found, override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(found, override=False)\n",
    "\n",
    "load_envs()\n",
    "\n",
    "# --- Azure OpenAI env with fallbacks & normalization (AZURE CLIENT) ---\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT    = (os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"\").strip().rstrip(\"/\")\n",
    "AZURE_OPENAI_API_KEY     = (os.getenv(\"AZURE_OPENAI_API_KEY\")  or \"\").strip()\n",
    "AZURE_OPENAI_DEPLOYMENT  = (os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or \"\").strip()  # chat/completions deployment name\n",
    "AZURE_OPENAI_API_VERSION = (os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-02-15-preview\").strip()\n",
    "\n",
    "def _mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:4] + \"â€¦\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "print(\n",
    "    \"Azure config â†’\",\n",
    "    \"endpoint:\", _mask(AZURE_OPENAI_ENDPOINT),\n",
    "    \"| key:\", _mask(AZURE_OPENAI_API_KEY),\n",
    "    \"| chat deployment:\", AZURE_OPENAI_DEPLOYMENT or \"<missing>\",\n",
    "    \"| version:\", AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "_missing = [k for k,v in {\n",
    "    \"AZURE_OPENAI_ENDPOINT\": AZURE_OPENAI_ENDPOINT,\n",
    "    \"AZURE_OPENAI_API_KEY\": AZURE_OPENAI_API_KEY,\n",
    "    \"AZURE_OPENAI_DEPLOYMENT\": AZURE_OPENAI_DEPLOYMENT,\n",
    "}.items() if not v]\n",
    "if _missing:\n",
    "    raise RuntimeError(\"Azure OpenAI configuration missing: \" + \", \".join(_missing))\n",
    "\n",
    "# âœ… Use AzureOpenAI so the library builds /openai/deployments/{deployment}/... paths\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "print(\"âœ… AzureOpenAI client initialized (deployment-aware).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b64fe17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.069704Z",
     "iopub.status.busy": "2025-09-13T23:16:14.069704Z",
     "iopub.status.idle": "2025-09-13T23:16:14.108711Z",
     "shell.execute_reply": "2025-09-13T23:16:14.107190Z"
    },
    "papermill": {
     "duration": 0.056261,
     "end_time": "2025-09-13T23:16:14.110812",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.054551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 156 transactions. expenses_are_negative=False\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 2: Load latest.csv (original) ---\n",
    "candidates = [\n",
    "    LATEST_CSV_PATH,\n",
    "    Path(os.getenv(\"OUTPUT_DIR\", str(REPO / \"data\" / \"raw\"))) / \"latest.csv\",\n",
    "    REPO / \"data\" / \"raw\" / \"latest.csv\",\n",
    "]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "if src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"latest.csv not found.\\nChecked:\\n- \" + \"\\n- \".join(str(p) for p in candidates) +\n",
    "        f\"\\nCWD={Path.cwd()}  REPO={REPO}\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Ensure expected columns exist\n",
    "expected = {\"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"bank_name\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"latest.csv missing columns: {missing}\")\n",
    "\n",
    "# Ensure card_name exists (fallback to bank_name)\n",
    "if \"card_name\" not in df.columns:\n",
    "    df[\"card_name\"] = df[\"bank_name\"]\n",
    "\n",
    "# Coerce types\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Basic cleanups\n",
    "df[\"merchant_name\"] = df[\"merchant_name\"].fillna(\"\")\n",
    "df[\"name\"] = df[\"name\"].fillna(\"\")\n",
    "\n",
    "# A robust unique id for each transaction (for embeddings & caching)\n",
    "def make_txn_uid(row):\n",
    "    key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "df[\"txn_uid\"] = df.apply(make_txn_uid, axis=1)\n",
    "\n",
    "# Global sign convention: True if expenses are negative numbers\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "print(f\"Loaded {len(df)} transactions. expenses_are_negative={EXPENSES_ARE_NEGATIVE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c361c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ENRICH DIAG] category non-null count: 156\n",
      "category\n",
      "Transfers                46\n",
      "Debt Payments            32\n",
      "Shopping                 30\n",
      "Dining                   17\n",
      "Services                 14\n",
      "Transportation            7\n",
      "Entertainment             4\n",
      "Home Improvement          2\n",
      "Uncategorized             1\n",
      "Fees                      1\n",
      "Health                    1\n",
      "Government/Non-Profit     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[ENRICH DIAG] category non-null count:\", df[\"category\"].notna().sum())\n",
    "print(df[\"category\"].fillna(\"<<NULL>>\").value_counts().head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1397b142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.140303Z",
     "iopub.status.busy": "2025-09-13T23:16:14.140303Z",
     "iopub.status.idle": "2025-09-13T23:16:14.156760Z",
     "shell.execute_reply": "2025-09-13T23:16:14.154749Z"
    },
    "papermill": {
     "duration": 0.037493,
     "end_time": "2025-09-13T23:16:14.160311",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.122818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant keys normalized (consistent with build_latest).\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 3: Normalize merchant_key (original) ---\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalization for merchant identity:\n",
    "    - Canonicalize brand patterns (AMZN/AMAZON, PAYPAL, SQUARE, APPLE.COM/BILL, GOOGLE*)\n",
    "    - Strip bank noise (POS/DEBIT/CHECK CRD/ACH/ZELLE/TRANSFER/etc.)\n",
    "    - Remove store numbers/digits/punctuation; keep letters, &, spaces, and '/' '.' for brand URLs\n",
    "    - Collapse whitespace; fallback to 'UNKNOWN'\n",
    "    \"\"\"\n",
    "    u = (name or \"\").upper()\n",
    "\n",
    "    # Canonical brand replacements (before stripping)\n",
    "    canon = [\n",
    "        (r\"AMZN\\s+MKTPL?C?E?|AMAZON\\.?\\s*COM\", \"AMAZON\"),\n",
    "        (r\"APPLE\\.?\\s*COM/?BILL\", \"APPLE.COM/BILL\"),\n",
    "        (r\"\\bGOOGLE\\*\", \"GOOGLE \"),\n",
    "        (r\"\\bSQC?\\*\", \"SQUARE \"),\n",
    "        (r\"\\bPAYPAL\\*?\", \"PAYPAL \"),\n",
    "    ]\n",
    "    for pat, repl in canon:\n",
    "        u = re.sub(pat, repl, u)\n",
    "\n",
    "    # Strip common bank/payments noise tokens\n",
    "    noise = [\n",
    "        r\"APPLE PAY ENDING IN \\d{4}\",\n",
    "        r\"POS(?:\\s+PURCHASE)?\",\n",
    "        r\"DEBIT(?:\\s+CARD)?(?:\\s+PURCHASE)?\",\n",
    "        r\"CHECK ?CRD\",\n",
    "        r\"VISA(?:\\s+POS)?\", r\"MASTERCARD\", r\"DISCOVER\", r\"AMEX\",\n",
    "        r\"ACH(?:\\s+(CREDIT|DEBIT))?\", r\"WEB AUTHORIZED PMT\", r\"ONLINE PMT\",\n",
    "        r\"ZELLE(?:\\s+PAYMENT)?\", r\"VENMO(?:\\s+PAYMENT)?\",\n",
    "        r\"XFER\", r\"TRANSFER\",\n",
    "        r\"PURCHASE\", r\"PENDING\", r\"REVERSAL\", r\"ADJ(?:USTMENT)?\",\n",
    "        r\"ID[: ]?\\d+\",\n",
    "    ]\n",
    "    for pat in noise:\n",
    "        u = re.sub(rf\"\\b{pat}\\b\", \" \", u)\n",
    "\n",
    "    # Remove store numbers & digits\n",
    "    u = re.sub(r\"#\\d{2,}\", \" \", u)\n",
    "    u = re.sub(r\"\\d+\", \" \", u)\n",
    "\n",
    "    # Keep letters, '&', spaces, plus '/' '.' for URLish brands; collapse spaces\n",
    "    u = re.sub(r\"[^A-Z&\\s\\./]\", \" \", u)\n",
    "    u = re.sub(r\"\\s+\", \" \", u).strip()\n",
    "\n",
    "    # Post-canon tidy\n",
    "    u = u.replace(\"APPLE COM BILL\", \"APPLE.COM/BILL\").strip()\n",
    "    return u or \"UNKNOWN\"\n",
    "\n",
    "# Use 'merchant_name' when available, else 'name'\n",
    "df[\"merchant_key\"] = np.where(\n",
    "    df[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    df[\"merchant_name\"].map(merchant_key_from),\n",
    "    df[\"name\"].map(merchant_key_from)\n",
    ")\n",
    "\n",
    "print(\"Merchant keys normalized (consistent with build_latest).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d521e9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.198541Z",
     "iopub.status.busy": "2025-09-13T23:16:14.197550Z",
     "iopub.status.idle": "2025-09-13T23:16:14.213672Z",
     "shell.execute_reply": "2025-09-13T23:16:14.212099Z"
    },
    "papermill": {
     "duration": 0.039783,
     "end_time": "2025-09-13T23:16:14.216681",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.176898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmapped merchants needing AI labels: 0\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 4: Load or initialize merchant dimension table (original) ---\n",
    "dim_cols = [\n",
    "    \"merchant_key\", \"display_name\", \"category\", \"subcategory\", \"tags\",\n",
    "    \"source\", \"confidence\", \"last_updated\"\n",
    "]\n",
    "if MERCHANT_DIM_PATH.exists():\n",
    "    dim = pd.read_csv(MERCHANT_DIM_PATH)\n",
    "    # ensure columns\n",
    "    for c in dim_cols:\n",
    "        if c not in dim.columns:\n",
    "            dim[c] = np.nan\n",
    "    dim = dim[dim_cols]\n",
    "else:\n",
    "    dim = pd.DataFrame(columns=dim_cols)\n",
    "\n",
    "# Left-join to see which keys are already mapped\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Identify unmapped merchants\n",
    "unmapped_keys = sorted(k for k in df.loc[df[\"display_name\"].isna(), \"merchant_key\"].unique() if k != \"UNKNOWN\")\n",
    "print(f\"Unmapped merchants needing AI labels: {len(unmapped_keys)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db494c68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.253833Z",
     "iopub.status.busy": "2025-09-13T23:16:14.253833Z",
     "iopub.status.idle": "2025-09-13T23:16:14.266027Z",
     "shell.execute_reply": "2025-09-13T23:16:14.264983Z"
    },
    "papermill": {
     "duration": 0.035064,
     "end_time": "2025-09-13T23:16:14.269573",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.234509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new mappings needed or AI disabled.\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 5: Label unmapped merchants via Azure (original) ---\n",
    "new_rows = []\n",
    "if len(unmapped_keys) and ('chat_client' in globals()) and (chat_client is not None) and MAP_ALL:\n",
    "    print(f\"Labeling {len(unmapped_keys)} merchants (single-call mode)...\")\n",
    "    for idx, mk in enumerate(unmapped_keys, 1):\n",
    "        try:\n",
    "            item = azure_label_one(mk)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Label fail for '{mk}': {e}\")\n",
    "            continue\n",
    "\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if item:\n",
    "            new_rows.append({\n",
    "                \"merchant_key\": mk,\n",
    "                \"display_name\": item[\"display_name\"],\n",
    "                \"category\": item[\"category\"],\n",
    "                \"subcategory\": item[\"subcategory\"],\n",
    "                \"tags\": \",\".join(item[\"tags\"]),\n",
    "                \"source\": \"azure\",\n",
    "                \"confidence\": 0.90,\n",
    "                \"last_updated\": now\n",
    "            })\n",
    "\n",
    "    if new_rows:\n",
    "        dim_new = pd.DataFrame(new_rows)\n",
    "        dim_all = pd.concat([dim, dim_new], ignore_index=True)\n",
    "        dim_all = dim_all.sort_values(\"last_updated\").drop_duplicates([\"merchant_key\"], keep=\"last\")\n",
    "        MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        dim_all.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        dim = dim_all\n",
    "        print(f\"âœ… Added {len(new_rows)} merchant mappings (single-call).\")\n",
    "    else:\n",
    "        print(\"No new mappings added (single-call).\")\n",
    "else:\n",
    "    print(\"No new mappings needed or AI disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef6c86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.312766Z",
     "iopub.status.busy": "2025-09-13T23:16:14.312766Z",
     "iopub.status.idle": "2025-09-13T23:16:14.340662Z",
     "shell.execute_reply": "2025-09-13T23:16:14.339647Z"
    },
    "papermill": {
     "duration": 0.055568,
     "end_time": "2025-09-13T23:16:14.345195",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.289627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ merchants_dim.csv saved (10 rows) â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\merchants_dim.csv\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 6: Persist merchants_dim.csv (original) ---\n",
    "# Toggle if you ever want to skip writing on runs with no changes\n",
    "PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "# dim_cols defined in Cell 4; dim may be updated in Cell 6\n",
    "if not isinstance(PERSIST_MERCHANT_DIM, bool):\n",
    "    PERSIST_MERCHANT_DIM = True\n",
    "\n",
    "if PERSIST_MERCHANT_DIM:\n",
    "    MERCHANT_DIM_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'dim' in globals() and isinstance(dim, pd.DataFrame) and len(dim):\n",
    "        # ensure expected columns/order exist before save\n",
    "        for c in dim_cols:\n",
    "            if c not in dim.columns:\n",
    "                dim[c] = np.nan\n",
    "        dim = dim[dim_cols]\n",
    "\n",
    "        dim.to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "        print(f\"ðŸ“ merchants_dim.csv saved ({len(dim)} rows) â†’ {MERCHANT_DIM_PATH}\")\n",
    "    else:\n",
    "        # either no new mappings this run or dim was empty; ensure file exists\n",
    "        if not MERCHANT_DIM_PATH.exists():\n",
    "            pd.DataFrame(columns=dim_cols).to_csv(MERCHANT_DIM_PATH, index=False)\n",
    "            print(f\"ðŸ“ Created headers-only merchants_dim.csv â†’ {MERCHANT_DIM_PATH}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ merchants_dim.csv already exists; no changes to sync.\")\n",
    "else:\n",
    "    print(\"PERSIST_MERCHANT_DIM=False â†’ skipping merchants_dim.csv persistence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eba14bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.420567Z",
     "iopub.status.busy": "2025-09-13T23:16:14.419564Z",
     "iopub.status.idle": "2025-09-13T23:16:14.438466Z",
     "shell.execute_reply": "2025-09-13T23:16:14.437256Z"
    },
    "papermill": {
     "duration": 0.073115,
     "end_time": "2025-09-13T23:16:14.442487",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.369372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Labels joined. category_display built with precedence: dim â†’ YAML â†’ Plaid.\n",
      "Category sample: {'Transfers': 46, 'Debt Payments': 32, 'Shopping': 30, 'Dining': 17, 'Services': 14, 'Transportation': 7, 'Entertainment': 4, 'Home Improvement': 2, 'Uncategorized': 1, 'Fees': 1}\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 7 (FIXED): Join labels + build display & category ---\n",
    "\n",
    "# 0) Cache Plaid's original category BEFORE any drops/merges\n",
    "if \"category\" in df.columns:\n",
    "    plaid_category_cached = df[\"category\"].copy()\n",
    "else:\n",
    "    plaid_category_cached = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "\n",
    "# 1) Remove prior label cols from earlier merges (do NOT drop our cached var)\n",
    "df = df.drop(\n",
    "    columns=[\"display_name\",\"category\",\"subcategory\",\"tags\",\"source\",\"confidence\",\"last_updated\"],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# 2) Merge merchants_dim (right has display_name/category/subcategory/tags)\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# 3) Restore Plaid category safely\n",
    "df[\"category_plaid\"] = plaid_category_cached\n",
    "\n",
    "# 4) Ensure YAML-final columns exist (from build_latest)\n",
    "for m in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\"]:\n",
    "    if m not in df.columns:\n",
    "        df[m] = pd.NA\n",
    "\n",
    "# 5) Rename dim category fields to explicit names (if present)\n",
    "if \"category\" in df.columns and \"category_dim\" not in df.columns:\n",
    "    df = df.rename(columns={\"category\": \"category_dim\"})\n",
    "if \"subcategory\" in df.columns and \"subcategory_dim\" not in df.columns:\n",
    "    df = df.rename(columns={\"subcategory\": \"subcategory_dim\"})\n",
    "\n",
    "# Helper: pick first non-empty/non-NaN string\n",
    "def pick_first_nonblank(row, cols):\n",
    "    for c in cols:\n",
    "        if c in row.index:\n",
    "            v = row[c]\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s and s.lower() not in {\"nan\", \"none\"}:\n",
    "                return s\n",
    "    return \"\"\n",
    "\n",
    "# 6) DISPLAY NAME â†’ dim.display_name â†’ YAML display_name_final â†’ merchant_key\n",
    "df[\"display_name\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"display_name\", \"display_name_final\", \"merchant_key\"]),\n",
    "    axis=1\n",
    ")\n",
    "df.loc[df[\"display_name\"].str.strip().eq(\"\"), \"display_name\"] = df[\"merchant_key\"]\n",
    "\n",
    "# 7) CATEGORY (primary) â†’ dim.category_dim â†’ YAML category_final â†’ Plaid category_plaid\n",
    "df[\"category_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"category_dim\", \"category_final\", \"category_plaid\"]),\n",
    "    axis=1\n",
    ").astype(str).str.strip()\n",
    "df.loc[df[\"category_display\"].eq(\"\") | df[\"category_display\"].str.lower().eq(\"none\"), \"category_display\"] = \"Uncategorized\"\n",
    "\n",
    "# Keep legacy 'category' column in sync for downstream code/Power BI\n",
    "df[\"category\"] = df[\"category_display\"]\n",
    "\n",
    "# 8) SUBCATEGORY/TAGS (same precedence)\n",
    "df[\"subcategory_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"subcategory_dim\", \"subcategory_final\"]),\n",
    "    axis=1\n",
    ")\n",
    "df[\"tags_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"tags\", \"tags_final\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep legacy columns populated for downstream\n",
    "df[\"subcategory\"] = df.get(\"subcategory_display\")\n",
    "df[\"tags\"] = df.get(\"tags_display\")\n",
    "\n",
    "# 9) Ensure required columns exist for save step\n",
    "final_cols = [\n",
    "    \"txn_uid\", \"date\", \"bank_name\", \"card_name\",\n",
    "    \"merchant_key\", \"display_name\",\n",
    "    \"category\", \"subcategory_display\", \"tags_display\",\n",
    "    \"name\", \"merchant_name\", \"amount\"\n",
    "]\n",
    "for c in final_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "\n",
    "print(\"âœ… Labels joined. category_display built with precedence: dim â†’ YAML â†’ Plaid.\")\n",
    "print(\"Category sample:\", df[\"category_display\"].value_counts(dropna=False).head(10).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed9d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AI Guess] category_display refreshed.\n"
     ]
    }
   ],
   "source": [
    "# Re-merge guesses to current df and rebuild display\n",
    "df = df.drop(columns=[\"display_name\",\"category_dim\",\"subcategory_dim\",\"tags\",\"source\",\"confidence\",\"last_updated\"],\n",
    "             errors=\"ignore\")\n",
    "df = df.merge(dim, on=\"merchant_key\", how=\"left\", suffixes=(\"\", \"_dim\"))\n",
    "\n",
    "# Helper\n",
    "def pick_first_nonblank(row, cols):\n",
    "    for c in cols:\n",
    "        if c in row.index:\n",
    "            v = row[c]\n",
    "            if pd.isna(v): \n",
    "                continue\n",
    "            s = str(v).strip()\n",
    "            if s and s.lower() not in {\"nan\",\"none\"}:\n",
    "                return s\n",
    "    return \"\"\n",
    "\n",
    "# --- FIX: build display_name then fill empties from merchant_key (no .replace here)\n",
    "df[\"display_name\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"display_name\", \"display_name_final\", \"merchant_key\"]),\n",
    "    axis=1\n",
    ")\n",
    "empty_mask = df[\"display_name\"].astype(str).str.strip().eq(\"\")\n",
    "df.loc[empty_mask, \"display_name\"] = df.loc[empty_mask, \"merchant_key\"]\n",
    "\n",
    "# Explicitly name dim columns if present\n",
    "if \"category\" in df.columns and \"category_dim\" not in df.columns:\n",
    "    df.rename(columns={\"category\":\"category_dim\"}, inplace=True)\n",
    "if \"subcategory\" in df.columns and \"subcategory_dim\" not in df.columns:\n",
    "    df.rename(columns={\"subcategory\":\"subcategory_dim\"}, inplace=True)\n",
    "\n",
    "# category_display precedence: dim â†’ YAML â†’ Plaid, then fallback\n",
    "df[\"category_display\"] = df.apply(\n",
    "    lambda r: pick_first_nonblank(r, [\"category_dim\",\"category_final\",\"category_plaid\"]),\n",
    "    axis=1\n",
    ").astype(str).str.strip()\n",
    "df.loc[df[\"category_display\"].eq(\"\") | df[\"category_display\"].str.lower().eq(\"none\"), \"category_display\"] = \"Uncategorized\"\n",
    "\n",
    "# Keep legacy 'category' in sync\n",
    "df[\"category\"] = df[\"category_display\"]\n",
    "\n",
    "print(\"[AI Guess] category_display refreshed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3dff8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.489794Z",
     "iopub.status.busy": "2025-09-13T23:16:14.489794Z",
     "iopub.status.idle": "2025-09-13T23:16:14.704107Z",
     "shell.execute_reply": "2025-09-13T23:16:14.703090Z"
    },
    "papermill": {
     "duration": 0.241299,
     "end_time": "2025-09-13T23:16:14.707109",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.465810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriptions flagged: 0 candidates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosis\\AppData\\Local\\Temp\\ipykernel_46048\\2379579808.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 8: Subscription detection (original) ---\n",
    "def detect_subscription(group: pd.DataFrame) -> bool:\n",
    "    g = group.dropna(subset=[\"date\", \"amount\"]).sort_values(\"date\")\n",
    "    if len(g) < 3:\n",
    "        return False\n",
    "\n",
    "    # use absolute spend magnitudes for stability\n",
    "    amounts = g[\"amount\"].abs().to_numpy(dtype=float)\n",
    "    amounts = amounts[np.isfinite(amounts)]\n",
    "    if amounts.size < 3:\n",
    "        return False\n",
    "\n",
    "    # gaps in days\n",
    "    ts_ns = g[\"date\"].astype(\"int64\").to_numpy()\n",
    "    gaps_days = np.diff(ts_ns) / 86_400_000_000_000\n",
    "    if gaps_days.size < 2:\n",
    "        return False\n",
    "\n",
    "    monthlyish_med = float(np.median(gaps_days))\n",
    "    frac_monthly = float(np.mean((gaps_days >= 27) & (gaps_days <= 33))) if gaps_days.size else 0.0\n",
    "\n",
    "    mu = float(np.mean(amounts))\n",
    "    cv = float(np.std(amounts) / (mu + 1e-9)) if mu > 0 else 1.0\n",
    "\n",
    "    return (27 <= monthlyish_med <= 33) and (frac_monthly >= 0.6) and (cv <= 0.2)\n",
    "\n",
    "# Clean any leftover artifacts from previous runs (e.g., is_subscription_x from merges)\n",
    "for col in [c for c in df.columns if c.startswith(\"is_subscription\") and c != \"is_subscription\"]:\n",
    "    df.drop(columns=col, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Respect your sign convention\n",
    "EXPENSES_ARE_NEGATIVE = (df[\"amount\"] < 0).sum() > (df[\"amount\"] > 0).sum()\n",
    "if EXPENSES_ARE_NEGATIVE:\n",
    "    outflows = df.loc[(df[\"amount\"] < 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "    outflows[\"amount\"] = outflows[\"amount\"].abs()\n",
    "else:\n",
    "    outflows = df.loc[(df[\"amount\"] > 0) & df[\"date\"].notna(), [\"display_name\", \"date\", \"amount\"]].copy()\n",
    "\n",
    "subs_map = {}\n",
    "for disp, g in outflows.groupby(\"display_name\", dropna=False):\n",
    "    try:\n",
    "        subs_map[disp] = bool(detect_subscription(g[[\"date\", \"amount\"]]))\n",
    "    except Exception:\n",
    "        subs_map[disp] = False\n",
    "\n",
    "df[\"is_subscription\"] = df[\"display_name\"].map(subs_map).fillna(False).astype(bool)\n",
    "\n",
    "print(f\"Subscriptions flagged: {int(df['is_subscription'].sum())} candidates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e514f206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.753684Z",
     "iopub.status.busy": "2025-09-13T23:16:14.752576Z",
     "iopub.status.idle": "2025-09-13T23:16:14.791019Z",
     "shell.execute_reply": "2025-09-13T23:16:14.788285Z"
    },
    "papermill": {
     "duration": 0.065294,
     "end_time": "2025-09-13T23:16:14.794753",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.729459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged: 2\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 9: Anomaly detection (original) ---\n",
    "def zscores(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    if sd == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mu) / sd\n",
    "\n",
    "df[\"amount_abs\"] = df[\"amount\"].abs()\n",
    "df[\"z_by_merchant\"] = (\n",
    "    df.groupby(\"display_name\", dropna=False)[\"amount_abs\"]\n",
    "      .transform(zscores)\n",
    ")\n",
    "df[\"is_anomaly\"] = (df[\"z_by_merchant\"] >= ANOMALY_Z)\n",
    "\n",
    "print(f\"Anomalies flagged: {int(df['is_anomaly'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6675fb24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.861537Z",
     "iopub.status.busy": "2025-09-13T23:16:14.860095Z",
     "iopub.status.idle": "2025-09-13T23:16:14.895130Z",
     "shell.execute_reply": "2025-09-13T23:16:14.894062Z"
    },
    "papermill": {
     "duration": 0.08206,
     "end_time": "2025-09-13T23:16:14.898637",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.816577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: last 30 days vs prior 30\n",
      "Spend: $5,794.09 (+1,953.32 vs prior)\n",
      "Top 3 merchants: Petal ($1,451.92), Withdrawal AMEX EPAYMENT / TYPE: ACH PMT ID: 0005000008 DATA: ER AM CO: AMEX EPAYMENT NAME: KOSISONNA UGOCHUKWU %% ACH ECC WEB %% ACH Trace 091000011489512 ($777.78), Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953 ($504.22)\n",
      "Biggest category driver: Debt Payments ($3,377.79)\n",
      "\n",
      "Saved digest â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\digest_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 10: 30-day digest (original) ---\n",
    "today = pd.Timestamp(date.today())\n",
    "cut1 = today - pd.Timedelta(days=30)\n",
    "cut2 = today - pd.Timedelta(days=60)\n",
    "\n",
    "cur = df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "prev = df[(df[\"date\"] > cut2) & (df[\"date\"] <= cut1) & (df[\"amount\"] > 0)]\n",
    "\n",
    "cur_total = cur[\"amount\"].sum()\n",
    "prev_total = prev[\"amount\"].sum()\n",
    "delta = cur_total - prev_total\n",
    "\n",
    "top_merchants = (\n",
    "    cur.groupby(\"display_name\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top_category = (\n",
    "    cur.groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "       .sort_values(ascending=False)\n",
    "       .head(1)\n",
    ")\n",
    "top_category_name = top_category.index[0] if len(top_category) else \"N/A\"\n",
    "top_category_amt = float(top_category.iloc[0]) if len(top_category) else 0.0\n",
    "\n",
    "digest = []\n",
    "digest.append(f\"Period: last 30 days vs prior 30\")\n",
    "digest.append(f\"Spend: ${cur_total:,.2f} ({'+' if delta>=0 else ''}{delta:,.2f} vs prior)\")\n",
    "digest.append(\"Top 3 merchants: \" + \", \".join([f\"{m} (${v:,.2f})\" for m, v in top_merchants.items()]))\n",
    "digest.append(f\"Biggest category driver: {top_category_name} (${top_category_amt:,.2f})\")\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "with open(DIGEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(digest))\n",
    "\n",
    "print(\"\\n\".join(digest))\n",
    "print(f\"\\nSaved digest â†’ {DIGEST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f17a1129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:14.961570Z",
     "iopub.status.busy": "2025-09-13T23:16:14.960552Z",
     "iopub.status.idle": "2025-09-13T23:16:14.987378Z",
     "shell.execute_reply": "2025-09-13T23:16:14.986371Z"
    },
    "papermill": {
     "duration": 0.059493,
     "end_time": "2025-09-13T23:16:14.989894",
     "exception": false,
     "start_time": "2025-09-13T23:16:14.930401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal: Save $1,000 next 30 days\n",
      "- Cut Debt Payments by 30%\n",
      "\n",
      "Saved goal nudges â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\goal_nudges_latest.txt\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 11: Goal nudges (original) ---\n",
    "cur_by_cat = (\n",
    "    df[(df[\"date\"] > cut1) & (df[\"amount\"] > 0)]\n",
    "      .groupby(\"category\", dropna=False)[\"amount\"].sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "nudges = []\n",
    "remaining = GOAL_SAVINGS\n",
    "for cat, amt in cur_by_cat.items():\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "    # propose cutting up to 40% of this category\n",
    "    max_cut = 0.40 * amt\n",
    "    if max_cut <= 0:\n",
    "        continue\n",
    "    pct_needed = min(remaining / amt, 0.40)  # cap at 40%\n",
    "    if pct_needed > 0:\n",
    "        nudges.append((cat, pct_needed))\n",
    "        remaining -= pct_needed * amt\n",
    "\n",
    "lines = [f\"Goal: Save ${GOAL_SAVINGS:,.0f} next 30 days\"]\n",
    "if nudges:\n",
    "    for (cat, pct) in nudges:\n",
    "        lines.append(f\"- Cut {cat} by {pct*100:.0f}%\")\n",
    "else:\n",
    "    lines.append(\"- Spending already low or insufficient category concentration to suggest cuts.\")\n",
    "\n",
    "with open(GOAL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "print(f\"\\nSaved goal nudges â†’ {GOAL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67e86df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:15.050700Z",
     "iopub.status.busy": "2025-09-13T23:16:15.050700Z",
     "iopub.status.idle": "2025-09-13T23:16:15.084647Z",
     "shell.execute_reply": "2025-09-13T23:16:15.083632Z"
    },
    "papermill": {
     "duration": 0.073422,
     "end_time": "2025-09-13T23:16:15.088652",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.015230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_MODEL: text-embedding-3-large\n",
      "No new embeddings needed\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 12 (UPDATED): Embeddings cache (NA-safe) ---\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "EMBED_MODEL = (os.getenv(\"AZURE_OPENAI_EMBEDDINGS\") or os.getenv(\"OPENAI_EMBEDDINGS_DEPLOYMENT\") or \"\").strip()\n",
    "EMBED_ENABLED = bool(EMBED_MODEL)\n",
    "print(\"EMBED_MODEL:\", EMBED_MODEL or \"<disabled>\")\n",
    "\n",
    "def safe_str(v) -> str:\n",
    "    \"\"\"Return '' for None/NaN/pd.NA/'nan'/'None', else a clean string.\"\"\"\n",
    "    try:\n",
    "        if pd.isna(v):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(v)\n",
    "    return \"\" if s.strip().lower() in {\"nan\", \"none\"} else s\n",
    "\n",
    "def build_search_text(row: pd.Series) -> str:\n",
    "    # Prefer 'category_display' if you created it; fallback to 'category'\n",
    "    cat_col = \"category_display\" if \"category_display\" in row.index else \"category\"\n",
    "    fields = [\"display_name\", \"name\", \"merchant_name\", cat_col, \"subcategory\", \"tags\"]\n",
    "    parts = [safe_str(row.get(f)) for f in fields]\n",
    "    return \" | \".join(p for p in parts if p)\n",
    "\n",
    "# Limit to recent rows for cost control\n",
    "embed_df = df.sort_values(\"date\", ascending=False).head(500).copy()\n",
    "embed_df[\"search_text\"] = embed_df.apply(build_search_text, axis=1)\n",
    "\n",
    "# Load existing cache (parquet with list column is fine under pyarrow)\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    try:\n",
    "        old = pd.read_parquet(EMBEDDINGS_PATH)\n",
    "        if \"txn_uid\" not in old.columns or \"embedding\" not in old.columns:\n",
    "            old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "    except Exception:\n",
    "        old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "else:\n",
    "    old = pd.DataFrame(columns=[\"txn_uid\",\"embedding\"])\n",
    "\n",
    "existing = set(old[\"txn_uid\"]) if len(old) else set()\n",
    "to_embed = embed_df.loc[~embed_df[\"txn_uid\"].isin(existing), [\"txn_uid\", \"search_text\"]]\n",
    "\n",
    "# Azure embeddings config (enabled only if a deployment name is set)\n",
    "EMBED_MODEL = (os.getenv(\"AZURE_OPENAI_EMBEDDINGS\") or os.getenv(\"OPENAI_EMBEDDINGS_DEPLOYMENT\") or \"\").strip()\n",
    "EMBED_ENABLED = bool(EMBED_MODEL)\n",
    "\n",
    "def get_embeddings(texts: list[str]):\n",
    "    if not EMBED_ENABLED:\n",
    "        return None\n",
    "    # Use the same Azure OpenAI client; model is your embeddings deployment name\n",
    "    res = client.embeddings.create(model=EMBED_MODEL, input=list(texts))\n",
    "    return [d.embedding for d in res.data]\n",
    "\n",
    "new_rows = []\n",
    "if len(to_embed) and EMBED_ENABLED:\n",
    "    B = 64\n",
    "    for i in range(0, len(to_embed), B):\n",
    "        chunk = to_embed.iloc[i:i+B]\n",
    "        vecs = get_embeddings(chunk[\"search_text\"].tolist())\n",
    "        if vecs is None:\n",
    "            break\n",
    "        for uid, vec in zip(chunk[\"txn_uid\"].tolist(), vecs):\n",
    "            if vec is not None:\n",
    "                new_rows.append({\"txn_uid\": uid, \"embedding\": vec})\n",
    "\n",
    "if new_rows:\n",
    "    add = pd.DataFrame(new_rows)\n",
    "    merged = pd.concat([old, add], ignore_index=True).drop_duplicates(\"txn_uid\", keep=\"last\")\n",
    "    merged.to_parquet(EMBEDDINGS_PATH, index=False)\n",
    "    print(f\"Embeddings cached: +{len(add)} â†’ total {len(merged)}\")\n",
    "else:\n",
    "    msg = \"Embeddings disabled (no AZURE_OPENAI_EMBEDDINGS)\" if not EMBED_ENABLED else \"No new embeddings needed\"\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7420962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:15.145537Z",
     "iopub.status.busy": "2025-09-13T23:16:15.144158Z",
     "iopub.status.idle": "2025-09-13T23:16:15.216841Z",
     "shell.execute_reply": "2025-09-13T23:16:15.215293Z"
    },
    "papermill": {
     "duration": 0.104623,
     "end_time": "2025-09-13T23:16:15.221004",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.116381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched CSV saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv\n",
      "ðŸ“„ Copy saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\latest_enriched.csv\n",
      "Column sanity (first 12): ['txn_uid', 'date', 'bank_name', 'card_name', 'display_name', 'merchant_key', 'category', 'category_display', 'category_plaid', 'subcategory', 'tags', 'name']\n",
      "Nulls check â€” category: 0 | category_display: 0 | category_plaid: 0\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 13 (REPLACE): Reorder and save ---\n",
    "\n",
    "save_cols = [\n",
    "    \"txn_uid\",\"date\",\"bank_name\",\"card_name\",\n",
    "    \"display_name\",\"merchant_key\",\n",
    "    # Keep BOTH the coalesced and raw category columns\n",
    "    \"category\",               # <- coalesced (dim â†’ yaml â†’ plaid)\n",
    "    \"category_display\",       # alias of coalesced for clarity in BI\n",
    "    \"category_plaid\",         # original Plaid category\n",
    "    \"subcategory\",\"tags\",\n",
    "    \"name\",\"merchant_name\",\n",
    "    \"amount\",\n",
    "    # analytics flags\n",
    "    \"is_subscription\",\"is_anomaly\",\"z_by_merchant\",\n",
    "    # optional flow flag if present\n",
    "    \"is_non_spend_flow\"\n",
    "]\n",
    "\n",
    "for c in save_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "df_out = df[save_cols].sort_values([\"date\", \"bank_name\"], ascending=[False, True])\n",
    "\n",
    "# Write both the stable file (Power BI) and a processed copy\n",
    "df_out.to_csv(ENRICHED_OUT_PATH, index=False)\n",
    "df_out.to_csv(ENRICHED_COPY_PATH, index=False)\n",
    "\n",
    "print(f\"âœ… Enriched CSV saved â†’ {ENRICHED_OUT_PATH}\")\n",
    "print(f\"ðŸ“„ Copy saved â†’ {ENRICHED_COPY_PATH}\")\n",
    "print(\"Column sanity (first 12):\", list(df_out.columns)[:12])\n",
    "print(\"Nulls check â€” category:\", int(df_out['category'].isna().sum()),\n",
    "      \"| category_display:\", int(df_out['category_display'].isna().sum()),\n",
    "      \"| category_plaid:\", int(df_out['category_plaid'].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dbf57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[POST-RUN] category vs category_display (top 12):\n",
      "category               category_display     \n",
      "Transfers              Transfers                46\n",
      "Debt Payments          Debt Payments            32\n",
      "Shopping               Shopping                 30\n",
      "Dining                 Dining                   17\n",
      "Services               Services                 14\n",
      "Transportation         Transportation            7\n",
      "Entertainment          Entertainment             4\n",
      "Home Improvement       Home Improvement          2\n",
      "Fees                   Fees                      1\n",
      "Government/Non-Profit  Government/Non-Profit     1\n",
      "Health                 Health                    1\n",
      "Uncategorized          Uncategorized             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[POST-RUN] category vs category_display (top 12):\")\n",
    "print(df[[\"category\",\"category_display\"]].fillna(\"<<NULL>>\").value_counts().head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d447e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:15.270721Z",
     "iopub.status.busy": "2025-09-13T23:16:15.269631Z",
     "iopub.status.idle": "2025-09-13T23:16:16.027224Z",
     "shell.execute_reply": "2025-09-13T23:16:16.023313Z"
    },
    "papermill": {
     "duration": 0.787872,
     "end_time": "2025-09-13T23:16:16.031924",
     "exception": false,
     "start_time": "2025-09-13T23:16:15.244052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Weekly] Category column used: category_display | non-null in window: 8\n",
      "category_display  spend\n",
      "       Transfers  65.74\n",
      "        Services  60.00\n",
      "        Shopping  11.06\n",
      "Home Improvement   5.39\n",
      "ðŸ§  Weekly executive digest written:\n",
      "- JSON: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.json\n",
      "- MD:   C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest.md\n",
      "- HTML: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_email.html\n",
      "- CSV:  C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_flat.csv\n",
      "- Subject: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_latest_subject.txt\n",
      "Week Window: 2025-09-08 -> 2025-09-14 | Prev: 2025-09-01 -> 2025-09-07\n",
      "WoW Spend: cur=142.19 prev=526.4 delta=-384.21 delta_pct=-0.7299\n",
      "MoM MTD Spend: cur=668.59 prev=890.15 delta=-221.56 delta_pct=-0.2489\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 14 (UPDATED): Weekly Executive Digest (WoW + MoM), AI + HTML/MD/CSV ---\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIGEST_JSON   = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "DIGEST_MD     = INSIGHTS_DIR / \"digest_latest.md\"\n",
    "DIGEST_FLAT   = INSIGHTS_DIR / \"digest_latest_flat.csv\"\n",
    "EMAIL_HTML    = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "EMAIL_SUBJECT = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "\n",
    "# -------- 1) Last COMPLETED week (Monâ€“Sun), compare WoW --------\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wd = int(now.weekday())  # Mon=0 ... Sun=6\n",
    "days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()    # prior Monday\n",
    "prev_end = (pd.Timestamp(wk_end) - pd.Timedelta(days=7)).date()\n",
    "prev_start = (pd.Timestamp(prev_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "def _short_range(ws, we):\n",
    "    try:\n",
    "        ws_dt = pd.to_datetime(ws).date(); we_dt = pd.to_datetime(we).date()\n",
    "        return f\"{ws_dt.month}/{ws_dt.day} - {we_dt.month}/{we_dt.day}\"\n",
    "    except Exception:\n",
    "        return f\"{ws} - {we}\"\n",
    "\n",
    "# -------- 1b) Exclusions: Wealthfront moves are not spend/income; Apple Cash stays --------\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "# Normalize candidate category columns (donâ€™t create if missing)\n",
    "for col in (\"category_display\",\"category\",\"category_final\",\"category_plaid\"):\n",
    "    if col in base.columns:\n",
    "        s = base[col].astype(str)\n",
    "        base[col] = s.where(~s.str.strip().isin([\"\", \"nan\", \"None\"]), np.nan)\n",
    "\n",
    "def _best_category_col(frame: pd.DataFrame) -> str | None:\n",
    "    candidates = [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]\n",
    "    for c in candidates:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Window slices\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "cur_w  = base[(base[\"date_only\"] >= wk_start) & (base[\"date_only\"] <= wk_end)]\n",
    "prev_w = base[(base[\"date_only\"] >= prev_start) & (base[\"date_only\"] <= prev_end)]\n",
    "\n",
    "# -------- 2) Robust sign detection --------\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "def spend_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a < 0].abs().sum()) if expenses_are_negative else float(a[a > 0].sum())\n",
    "\n",
    "def income_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a > 0].sum()) if expenses_are_negative else float(a[a < 0].abs().sum())\n",
    "\n",
    "# WoW\n",
    "cur_spend   = round(spend_sum(cur_w), 2)\n",
    "prev_spend  = round(spend_sum(prev_w), 2)\n",
    "cur_income  = round(income_sum(cur_w), 2)\n",
    "prev_income = round(income_sum(prev_w), 2)\n",
    "\n",
    "spend_delta     = round(cur_spend - prev_spend, 2)\n",
    "spend_delta_pct = round((spend_delta / prev_spend), 4) if prev_spend else (1.0 if cur_spend else 0.0)\n",
    "\n",
    "# -------- 3) MoM (MTD vs aligned days in prior month) --------\n",
    "cur_month_start = pd.Timestamp(wk_end).to_period('M').start_time.date()\n",
    "cur_mtd_end     = wk_end\n",
    "prev_month = (pd.Timestamp(wk_end).to_period('M') - 1)\n",
    "prev_month_start = prev_month.start_time.date()\n",
    "prev_month_end   = prev_month.end_time.date()\n",
    "days_into_m = (pd.Timestamp(cur_mtd_end) - pd.Timestamp(cur_month_start)).days\n",
    "aligned_prev_m_end = (pd.Timestamp(prev_month_start) + pd.Timedelta(days=days_into_m)).date()\n",
    "if aligned_prev_m_end > prev_month_end:\n",
    "    aligned_prev_m_end = prev_month_end\n",
    "\n",
    "cur_m = base[(base[\"date_only\"] >= cur_month_start) & (base[\"date_only\"] <= cur_mtd_end)]\n",
    "prev_m_aligned = base[(base[\"date_only\"] >= prev_month_start) & (base[\"date_only\"] <= aligned_prev_m_end)]\n",
    "cur_spend_m   = round(spend_sum(cur_m), 2)\n",
    "prev_spend_m  = round(spend_sum(prev_m_aligned), 2)\n",
    "cur_income_m  = round(income_sum(cur_m), 2)\n",
    "prev_income_m = round(income_sum(prev_m_aligned), 2)\n",
    "spend_delta_m     = round(cur_spend_m - prev_spend_m, 2)\n",
    "spend_delta_pct_m = round((spend_delta_m / prev_spend_m), 4) if prev_spend_m else (1.0 if cur_spend_m else 0.0)\n",
    "\n",
    "# -------- 4) Top drivers (category) --------\n",
    "if expenses_are_negative:\n",
    "    cur_exp = cur_w[cur_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp = cur_w[cur_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "CAT_COL = _best_category_col(cur_exp)\n",
    "if CAT_COL is None:\n",
    "    CAT_COL = \"category_display\"\n",
    "    cur_exp[CAT_COL] = np.nan\n",
    "\n",
    "top_cats_cur = (\n",
    "    cur_exp.groupby(CAT_COL, dropna=False)[\"spend\"]\n",
    "           .sum().sort_values(ascending=False).head(5)\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "def _label(v):\n",
    "    return \"Uncategorized\" if (pd.isna(v) or str(v).strip() in {\"\", \"nan\", \"None\"}) else str(v)\n",
    "\n",
    "if CAT_COL in top_cats_cur.columns:\n",
    "    top_cats_cur[CAT_COL] = top_cats_cur[CAT_COL].apply(_label)\n",
    "\n",
    "# DEBUG â€” confirm which column was used and a peek at results\n",
    "print(f\"[Weekly] Category column used: {CAT_COL} | non-null in window: {int(cur_exp[CAT_COL].notna().sum())}\")\n",
    "print(top_cats_cur.head(5).to_string(index=False))\n",
    "\n",
    "subs_w  = cur_w.loc[cur_w.get(\"is_subscription\", False) == True]\n",
    "anoms_w = cur_w.loc[cur_w.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "# -------- 5) Payload for AI --------\n",
    "summary_payload = {\n",
    "    \"as_of_date\": pd.Timestamp(wk_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(wk_start), \"end\": str(wk_end), \"label\": \"Last completed week (Monâ€“Sun)\"},\n",
    "        \"previous\": {\"start\": str(prev_start), \"end\": str(prev_end)}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        # WoW\n",
    "        \"spend_current\": cur_spend,\n",
    "        \"spend_previous\": prev_spend,\n",
    "        \"spend_delta\": spend_delta,\n",
    "        \"spend_delta_pct\": spend_delta_pct,\n",
    "        \"income_current\": cur_income,\n",
    "        \"income_previous\": prev_income,\n",
    "        # MoM (MTD vs aligned prior month MTD)\n",
    "        \"spend_mtd_current\": cur_spend_m,\n",
    "        \"spend_mtd_previous\": prev_spend_m,\n",
    "        \"spend_mtd_delta\": spend_delta_m,\n",
    "        \"spend_mtd_delta_pct\": spend_delta_pct_m,\n",
    "        \"income_mtd_current\": cur_income_m,\n",
    "        \"income_mtd_previous\": prev_income_m,\n",
    "    },\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[CAT_COL]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_cur.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_w[\"display_name\"].nunique()) if len(subs_w) else 0,\n",
    "    \"anomalies_count\": int(anoms_w.shape[0]) if len(anoms_w) else 0,\n",
    "}\n",
    "\n",
    "# -------- 6) Azure summarizer (REQUIRED) with theme + narrative --------\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided aggregates (week-over-week and month-over-month), \"\n",
    "    \"produce an executive digest in STRICT JSON. No invented numbers.\"\n",
    ")\n",
    "USER_INSTRUCTIONS = (\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"theme\": string,\\n'\n",
    "    '  \"narrative\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ],\\n'\n",
    "    '  \"email_subject\": string\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct where spend improved.\\n\"\n",
    "    \"- impact_usd is a rough weekly savings estimate.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call(payload_json: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.15,\n",
    "        max_tokens=750,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "raw = _azure_digest_call(json.dumps(summary_payload))\n",
    "azure_digest = _salvage_json_object(raw)\n",
    "if not isinstance(azure_digest, dict):\n",
    "    raise RuntimeError(\"Azure summarizer returned no valid JSON. Check Azure env, deployment name, or quota.\")\n",
    "\n",
    "# -------- 7) Overlay (protect authoritative totals/window) + sanitize theme --------\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in {\"totals\", \"window\"}:\n",
    "            continue\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "short_rng = _short_range(wk_start, wk_end)\n",
    "\n",
    "base_digest = {\n",
    "    \"insights_version\": 3,\n",
    "    \"window\": summary_payload[\"window\"],\n",
    "    \"totals\": summary_payload[\"totals\"],\n",
    "    \"headline\": f\"Weekly digest {short_rng}\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Total Spend (WoW)\",  \"value\": cur_spend,  \"delta_pct\": spend_delta_pct},\n",
    "        {\"name\":\"Total Income (WoW)\",\"value\": cur_income, \"delta_pct\": None},\n",
    "        {\"name\":\"Total Spend (MoM)\",  \"value\": cur_spend_m,\"delta_pct\": spend_delta_pct_m},\n",
    "        {\"name\":\"Total Income (MoM)\",\"value\": cur_income_m,\"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": _label(t[\"category\"]), \"spend\": float(t[\"spend\"])} for t in summary_payload[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload['subscriptions_count']} active subs this week\"}] if summary_payload[\"subscriptions_count\"] else []) +\n",
    "        ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload['anomalies_count']} anomalies this week\"}] if summary_payload[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": [],\n",
    "    \"theme\": \"\",\n",
    "    \"narrative\": \"\",\n",
    "    \"email_subject\": \"\"\n",
    "}\n",
    "\n",
    "digest = _overlay(base_digest, azure_digest)\n",
    "\n",
    "# Clamp theme to 3 words max; fallback if empty\n",
    "if not isinstance(digest.get(\"theme\",\"\"), str) or not digest[\"theme\"].strip():\n",
    "    digest[\"theme\"] = \"Lean Week\" if spend_delta < 0 else \"Heavier Week\"\n",
    "else:\n",
    "    words = [w for w in re.split(r\"\\s+\", digest[\"theme\"].strip()) if w]\n",
    "    digest[\"theme\"] = \" \".join(words[:3])\n",
    "\n",
    "# -------- 8) Compact summary line for tiles --------\n",
    "def build_compact_summary(d: dict) -> str:\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "    totals = d.get(\"totals\", {}) or {}\n",
    "    spend_val = float(totals.get(\"spend_current\") or 0.0)\n",
    "    dp = totals.get(\"spend_delta_pct\")\n",
    "    if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp / 100.0\n",
    "    dp_txt = f\"{dp*100:+.1f}%\" if isinstance(dp, (int,float)) else \"n/a\"\n",
    "    drivers = d.get(\"top_drivers\") or []\n",
    "    if drivers:\n",
    "        label = (drivers[0].get(\"label\") or \"Uncategorized\").strip()\n",
    "        amt = float(drivers[0].get(\"spend\") or 0.0)\n",
    "        driver_txt = f\"Top driver: {label} (${amt:,.0f})\"\n",
    "    else:\n",
    "        driver_txt = \"Top driver: n/a\"\n",
    "    return f\"{short}: Weekly spend ${spend_val:,.0f} (WoW {dp_txt}). {driver_txt}.\"\n",
    "\n",
    "digest[\"summary\"] = build_compact_summary(digest)\n",
    "\n",
    "# -------- 9) Persist JSON --------\n",
    "with open(DIGEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# -------- 10) Markdown (employer-ready) --------\n",
    "def render_md(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "\n",
    "    lines = [f\"# Weekly Executive Summary ({short})\"]\n",
    "    if d.get(\"theme\"):\n",
    "        lines.append(f\"*{d['theme']}*\")\n",
    "    if d.get(\"narrative\"):\n",
    "        lines.append(f\"\\n{d['narrative'].strip()}\\n\")\n",
    "    else:\n",
    "        lines.append(f\"\\n{d['summary']}\\n\")\n",
    "\n",
    "    # Key Metrics\n",
    "    lines.append(\"## Key Metrics\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        lines.append(f\"- **{name}:** ${val:,.2f}{dp_txt}\")\n",
    "\n",
    "    # Drivers\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        lines.append(\"\\n## Drivers\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            lines.append(f\"- **{label}:** ${float(t.get('spend',0)):,.0f}\")\n",
    "\n",
    "    # Risks\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        lines.append(\"\\n## Risks\")\n",
    "        for r in rk[:5]:\n",
    "            lines.append(f\"- **{r.get('type','note')}:** {r.get('note','')}\")\n",
    "\n",
    "    # Recommendations\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        lines.append(\"\\n## Recommendations\")\n",
    "        for a in ai[:5]:\n",
    "            lines.append(f\"- **{a.get('title','')}** â€” est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(DIGEST_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md(digest))\n",
    "\n",
    "# -------- 11) Email subject + HTML (clean sections) --------\n",
    "subject = digest.get(\"email_subject\") or f\"{digest.get('theme','Weekly Digest')} â€” {short_rng}\"\n",
    "with open(EMAIL_SUBJECT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(subject.strip())\n",
    "\n",
    "def render_email_html(d):\n",
    "    ws = d.get(\"window\", {}).get(\"current\", {}).get(\"start\", str(wk_start))\n",
    "    we = d.get(\"window\", {}).get(\"current\", {}).get(\"end\", str(wk_end))\n",
    "    short = _short_range(ws, we)\n",
    "    theme = (d.get(\"theme\") or \"\").strip()\n",
    "    narrative = (d.get(\"narrative\") or d.get(\"summary\") or \"\").strip()\n",
    "\n",
    "    parts = []\n",
    "    parts.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "    parts.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "    parts.append(f\"<h1 style='margin:0 0 4px 0;font-size:18px;'>Weekly Executive Summary ({short})</h1>\")\n",
    "    if theme:\n",
    "        parts.append(f\"<div style='margin:0 0 12px 0;color:#6b7280;font-style:italic'>{theme}</div>\")\n",
    "    if narrative:\n",
    "        parts.append(f\"<p style='margin:0 0 16px 0'>{narrative}</p>\")\n",
    "\n",
    "    parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Key Metrics</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        parts.append(f\"<li><b>{name}:</b> ${val:,.2f}{dp_txt}</li>\")\n",
    "    parts.append(\"</ul>\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Drivers</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            parts.append(f\"<li><b>{label}:</b> ${float(t.get('spend',0)):,.0f}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Risks</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for r in rk[:5]:\n",
    "            parts.append(f\"<li><b>{r.get('type','note')}:</b> {r.get('note','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Recommendations</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for a in ai[:5]:\n",
    "            parts.append(f\"<li><b>{a.get('title','')}</b> â€” est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    parts.append(\"</div>\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "with open(EMAIL_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_email_html(digest))\n",
    "\n",
    "# -------- 12) Flat CSV for Power BI (add MoM metrics too) --------\n",
    "flat_rows = []\n",
    "flat_rows.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(wk_end),\n",
    "    \"cur_start\": str(wk_start),\n",
    "    \"cur_end\": str(wk_end),\n",
    "    \"prev_start\": str(prev_start),\n",
    "    \"prev_end\": str(prev_end),\n",
    "    \"headline\": digest.get(\"headline\",\"\"),\n",
    "    \"summary\": digest.get(\"summary\",\"\"),\n",
    "    \"name\": \"Total Spend (WoW)\",\n",
    "    \"value\": float(cur_spend),\n",
    "    \"delta_pct\": float(spend_delta_pct),\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "for m in digest.get(\"key_metrics\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for t in digest.get(\"top_drivers\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": (t.get(\"label\") or \"Uncategorized\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for r in digest.get(\"risks\", []):\n",
    "    flat_rows.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(wk_end),\n",
    "        \"cur_start\": str(wk_start),\n",
    "        \"cur_end\": str(wk_end),\n",
    "        \"prev_start\": str(prev_start),\n",
    "        \"prev_end\": str(prev_end),\n",
    "        \"headline\": digest.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "pd.DataFrame(flat_rows).to_csv(DIGEST_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"ðŸ§  Weekly executive digest written:\\n\"\n",
    "    f\"- JSON: {DIGEST_JSON}\\n- MD:   {DIGEST_MD}\\n- HTML: {EMAIL_HTML}\\n- CSV:  {DIGEST_FLAT}\\n- Subject: {EMAIL_SUBJECT}\\n\"\n",
    "    f\"Week Window: {wk_start} -> {wk_end} | Prev: {prev_start} -> {prev_end}\\n\"\n",
    "    f\"WoW Spend: cur={cur_spend} prev={prev_spend} delta={spend_delta} delta_pct={spend_delta_pct:+.4f}\\n\"\n",
    "    f\"MoM MTD Spend: cur={cur_spend_m} prev={prev_spend_m} delta={spend_delta_m} delta_pct={spend_delta_pct_m:+.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f0afa5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:16.095634Z",
     "iopub.status.busy": "2025-09-13T23:16:16.094646Z",
     "iopub.status.idle": "2025-09-13T23:16:16.226532Z",
     "shell.execute_reply": "2025-09-13T23:16:16.223355Z"
    },
    "papermill": {
     "duration": 0.172408,
     "end_time": "2025-09-13T23:16:16.231678",
     "exception": false,
     "start_time": "2025-09-13T23:16:16.059270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Monthly] Category column used: category_display | non-null in MTD: 17\n",
      "category_display  spend\n",
      "        Shopping 464.86\n",
      "       Transfers  79.48\n",
      "        Services  60.00\n",
      "  Transportation  58.86\n",
      "Home Improvement   5.39\n",
      "ðŸ§¾ Monthly executive digest written (MTD):\n",
      "- JSON: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom.json\n",
      "- MD:   C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom.md\n",
      "- HTML: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_email.html\n",
      "- CSV:  C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_flat.csv\n",
      "- Subject: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_mom_subject.txt\n",
      "MTD Window: September 2025-09-01 -> 2025-09-14 | Prior aligned: August 2025-08-01 -> 2025-08-14\n",
      "MoM MTD Spend: cur=668.59 prev=890.15 delta=-221.56 delta_pct=-0.2489\n"
     ]
    }
   ],
   "source": [
    "# --- enrich_transactions.ipynb â€” Cell 15 (UPDATED): Monthly Executive Digest (MTD vs prior MTD), AI + HTML/MD/CSV ---\n",
    "import os, re, json, calendar\n",
    "from pathlib import Path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MOM_JSON   = INSIGHTS_DIR / \"digest_mom.json\"\n",
    "MOM_MD     = INSIGHTS_DIR / \"digest_mom.md\"\n",
    "MOM_FLAT   = INSIGHTS_DIR / \"digest_mom_flat.csv\"\n",
    "MOM_HTML   = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "MOM_SUBJ   = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# --- 1) Date windows (align with Cell 14; use LA time) ---\n",
    "try:\n",
    "    now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "except Exception:\n",
    "    now = pd.Timestamp.now().normalize()\n",
    "\n",
    "wk_wd = int(now.weekday())\n",
    "days_to_last_sun = 7 if wk_wd == 6 else (wk_wd + 1)\n",
    "wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()      # inclusive Sunday\n",
    "cur_month_start = pd.Timestamp(wk_end).to_period('M').start_time.date()\n",
    "cur_mtd_end     = wk_end\n",
    "\n",
    "prev_month = (pd.Timestamp(wk_end).to_period('M') - 1)\n",
    "prev_month_start = prev_month.start_time.date()\n",
    "prev_month_end   = prev_month.end_time.date()\n",
    "\n",
    "# Align prior-month MTD to the same number of days as current MTD\n",
    "days_into_m = (pd.Timestamp(cur_mtd_end) - pd.Timestamp(cur_month_start)).days\n",
    "aligned_prev_m_end = (pd.Timestamp(prev_month_start) + pd.Timedelta(days=days_into_m)).date()\n",
    "if aligned_prev_m_end > prev_month_end:\n",
    "    aligned_prev_m_end = prev_month_end\n",
    "\n",
    "cur_month_name  = calendar.month_name[pd.to_datetime(cur_mtd_end).month]\n",
    "prev_month_name = calendar.month_name[pd.to_datetime(prev_month_start).month]\n",
    "\n",
    "# --- 2) Exclusions: Wealthfront out, Apple Cash in (mirror Cell 14) ---\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "# Normalize candidate category columns\n",
    "for col in (\"category_display\",\"category\",\"category_final\",\"category_plaid\"):\n",
    "    if col in base.columns:\n",
    "        s = base[col].astype(str)\n",
    "        base[col] = s.where(~s.str.strip().isin([\"\", \"nan\", \"None\"]), np.nan)\n",
    "\n",
    "def _best_category_col(frame: pd.DataFrame) -> str | None:\n",
    "    candidates = [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]\n",
    "    for c in candidates:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "cur_m  = base[(base[\"date_only\"] >= cur_month_start) & (base[\"date_only\"] <= cur_mtd_end)]\n",
    "prev_m = base[(base[\"date_only\"] >= prev_month_start) & (base[\"date_only\"] <= aligned_prev_m_end)]\n",
    "\n",
    "# --- 3) Polarity + totals ---\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "def spend_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a < 0].abs().sum()) if expenses_are_negative else float(a[a > 0].sum())\n",
    "\n",
    "def income_sum(frame):\n",
    "    a = frame[\"amount\"].dropna()\n",
    "    return float(a[a > 0].sum()) if expenses_are_negative else float(a[a < 0].abs().sum())\n",
    "\n",
    "cur_spend_m   = round(spend_sum(cur_m), 2)\n",
    "prev_spend_m  = round(spend_sum(prev_m), 2)\n",
    "cur_income_m  = round(income_sum(cur_m), 2)\n",
    "prev_income_m = round(income_sum(prev_m), 2)\n",
    "\n",
    "spend_delta_m     = round(cur_spend_m - prev_spend_m, 2)\n",
    "spend_delta_pct_m = round((spend_delta_m / prev_spend_m), 4) if prev_spend_m else (1.0 if cur_spend_m else 0.0)\n",
    "\n",
    "# --- 4) Top drivers this month (category) ---\n",
    "CAT_COL = _best_category_col(cur_m)\n",
    "if CAT_COL is None:\n",
    "    CAT_COL = \"category_display\"\n",
    "    cur_m[CAT_COL] = np.nan\n",
    "\n",
    "if expenses_are_negative:\n",
    "    cur_exp_m = cur_m[cur_m[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp_m = cur_m[cur_m[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "top_cats_m = (\n",
    "    cur_exp_m.groupby(CAT_COL, dropna=False)[\"spend\"]\n",
    "             .sum().sort_values(ascending=False).head(5)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "def _label(v):\n",
    "    return \"Uncategorized\" if (pd.isna(v) or str(v).strip() in {\"\", \"nan\", \"None\"}) else str(v)\n",
    "\n",
    "if CAT_COL in top_cats_m.columns:\n",
    "    top_cats_m[CAT_COL] = top_cats_m[CAT_COL].apply(_label)\n",
    "\n",
    "# DEBUG â€” confirm which column was used and a peek at results\n",
    "print(f\"[Monthly] Category column used: {CAT_COL} | non-null in MTD: {int(cur_m[CAT_COL].notna().sum())}\")\n",
    "print(top_cats_m.head(5).to_string(index=False))\n",
    "\n",
    "subs_m  = cur_m.loc[cur_m.get(\"is_subscription\", False) == True]\n",
    "anoms_m = cur_m.loc[cur_m.get(\"is_anomaly\", False) == True]\n",
    "\n",
    "# --- 5) Payload for AI ---\n",
    "summary_payload_m = {\n",
    "    \"as_of_date\": pd.Timestamp(cur_mtd_end).isoformat(),\n",
    "    \"window\": {\n",
    "        \"current\": {\"start\": str(cur_month_start), \"end\": str(cur_mtd_end), \"label\": f\"{cur_month_name} MTD\"},\n",
    "        \"previous\": {\"start\": str(prev_month_start), \"end\": str(aligned_prev_m_end), \"label\": f\"{prev_month_name} MTD (aligned)\"}\n",
    "    },\n",
    "    \"totals\": {\n",
    "        \"spend_mtd_current\": cur_spend_m,\n",
    "        \"spend_mtd_previous\": prev_spend_m,\n",
    "        \"spend_mtd_delta\": spend_delta_m,\n",
    "        \"spend_mtd_delta_pct\": spend_delta_pct_m,\n",
    "        \"income_mtd_current\": cur_income_m,\n",
    "        \"income_mtd_previous\": prev_income_m,\n",
    "    },\n",
    "    \"top_categories\": [\n",
    "        {\"category\": str(r[CAT_COL]), \"spend\": float(r[\"spend\"])}\n",
    "        for _, r in top_cats_m.iterrows()\n",
    "    ],\n",
    "    \"subscriptions_count\": int(subs_m[\"display_name\"].nunique()) if len(subs_m) else 0,\n",
    "    \"anomalies_count\": int(anoms_m.shape[0]) if len(anoms_m) else 0,\n",
    "}\n",
    "\n",
    "# --- 6) Azure summarizer (REQUIRED): same personality as WoW ---\n",
    "def _salvage_json_object(txt: str):\n",
    "    t = (txt or \"\").strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```(?:json)?\", \"\", t, flags=re.IGNORECASE).strip()\n",
    "        t = re.sub(r\"```$\", \"\", t).strip()\n",
    "    try:\n",
    "        obj = json.loads(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    s, e = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if s != -1 and e != -1 and e > s:\n",
    "        cand = t[s:e+1]\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import ast\n",
    "        obj = ast.literal_eval(t)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "SYSTEM_SUMMARY_M = (\n",
    "    \"You are an analytics copilot for personal finance. \"\n",
    "    \"Using ONLY the provided month-to-date aggregates (vs the same number of days in the prior month), \"\n",
    "    \"produce an executive **monthly** digest in STRICT JSON. No invented numbers.\"\n",
    ")\n",
    "USER_INSTRUCTIONS_M = (\n",
    "    \"Return ONLY a JSON object with keys:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"headline\": string,\\n'\n",
    "    '  \"theme\": string,\\n'\n",
    "    '  \"narrative\": string,\\n'\n",
    "    '  \"key_metrics\": [ {\"name\": string, \"value\": number, \"delta_pct\": number|null} ],\\n'\n",
    "    '  \"top_drivers\": [ {\"label\": string, \"spend\": number} ],\\n'\n",
    "    '  \"risks\": [ {\"type\": \"subscription\"|\"anomaly\"|\"trend\", \"note\": string} ],\\n'\n",
    "    '  \"action_items\": [ {\"title\": string, \"impact_usd\": number, \"rationale\": string} ],\\n'\n",
    "    '  \"email_subject\": string\\n'\n",
    "    \"}\\n\"\n",
    "    \"- Max 5 items per list.\\n\"\n",
    "    \"- Use negative delta_pct where spend improved.\\n\"\n",
    "    \"- impact_usd is a rough **weekly** savings estimate; you may still propose monthly actions.\\n\"\n",
    ")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6))\n",
    "def _azure_digest_call_m(payload_json: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": SYSTEM_SUMMARY_M},\n",
    "            {\"role\":\"user\",\"content\": USER_INSTRUCTIONS_M + \"\\n\\nPAYLOAD:\\n\" + payload_json}\n",
    "        ],\n",
    "        temperature=0.15,\n",
    "        max_tokens=750,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "raw_m = _azure_digest_call_m(json.dumps(summary_payload_m))\n",
    "azure_digest_m = _salvage_json_object(raw_m)\n",
    "if not isinstance(azure_digest_m, dict):\n",
    "    raise RuntimeError(\"Azure MoM summarizer returned no valid JSON. Check Azure env/deployment/quota.\")\n",
    "\n",
    "# --- 7) Overlay + defaults ---\n",
    "def _overlay(base: dict, over: dict | None) -> dict:\n",
    "    if not isinstance(over, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in over.items():\n",
    "        if k in {\"totals\", \"window\"}:\n",
    "            continue\n",
    "        if k in (\"key_metrics\",\"top_drivers\",\"risks\",\"action_items\"):\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                out[k] = v\n",
    "        elif v not in (None, \"\", {}):\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "base_m = {\n",
    "    \"insights_version\": 3,\n",
    "    \"window\": summary_payload_m[\"window\"],\n",
    "    \"totals\": summary_payload_m[\"totals\"],\n",
    "    \"headline\": f\"Monthly digest â€” {cur_month_name} MTD\",\n",
    "    \"key_metrics\": [\n",
    "        {\"name\":\"Total Spend (MoM MTD)\",  \"value\": cur_spend_m,  \"delta_pct\": spend_delta_pct_m},\n",
    "        {\"name\":\"Total Income (MoM MTD)\",\"value\": cur_income_m, \"delta_pct\": None},\n",
    "    ],\n",
    "    \"top_drivers\": [{\"label\": _label(t[\"category\"]), \"spend\": float(t[\"spend\"])} for t in summary_payload_m[\"top_categories\"]],\n",
    "    \"risks\": (\n",
    "        ([{\"type\":\"subscription\",\"note\": f\"{summary_payload_m['subscriptions_count']} active subs this month-to-date\"}] if summary_payload_m[\"subscriptions_count\"] else []) +\n",
    "        ([{\"type\":\"anomaly\",\"note\": f\"{summary_payload_m['anomalies_count']} anomalies this month-to-date\"}] if summary_payload_m[\"anomalies_count\"] else [])\n",
    "    ),\n",
    "    \"action_items\": [],\n",
    "    \"theme\": \"\",\n",
    "    \"narrative\": \"\",\n",
    "    \"email_subject\": \"\"\n",
    "}\n",
    "digest_m = _overlay(base_m, azure_digest_m)\n",
    "\n",
    "# Limit theme to 3 words max; fallback if empty\n",
    "if not isinstance(digest_m.get(\"theme\",\"\"), str) or not digest_m[\"theme\"].strip():\n",
    "    digest_m[\"theme\"] = \"Steady MTD\" if spend_delta_m <= 0 else \"Upward MTD\"\n",
    "else:\n",
    "    words = [w for w in re.split(r\"\\s+\", digest_m[\"theme\"].strip()) if w]\n",
    "    digest_m[\"theme\"] = \" \".join(words[:3])\n",
    "\n",
    "# --- 8) Save JSON ---\n",
    "with open(MOM_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(digest_m, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 9) Markdown (month names; narrative first) ---\n",
    "def render_md_m(d):\n",
    "    heading = f\"# Monthly Executive Summary ({cur_month_name} MTD)\"\n",
    "    lines = [heading]\n",
    "    if d.get(\"theme\"):\n",
    "        lines.append(f\"*{d['theme']}*\")\n",
    "    if d.get(\"narrative\"):\n",
    "        lines.append(f\"\\n{d['narrative'].strip()}\\n\")\n",
    "\n",
    "    lines.append(\"## Key Metrics (MoM)\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        lines.append(f\"- **{name}:** ${val:,.2f}{dp_txt}\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        lines.append(\"\\n## Drivers (MTD)\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            lines.append(f\"- **{label}:** ${float(t.get('spend',0)):,.0f}\")\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        lines.append(\"\\n## Risks\")\n",
    "        for r in rk[:5]:\n",
    "            lines.append(f\"- **{r.get('type','note')}:** {r.get('note','')}\")\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        lines.append(\"\\n## Recommendations\")\n",
    "        for a in ai[:5]:\n",
    "            lines.append(f\"- **{a.get('title','')}** â€” est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "with open(MOM_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_md_m(digest_m))\n",
    "\n",
    "# --- 10) Email subject + HTML (clean sections; month name only) ---\n",
    "subject_m = digest_m.get(\"email_subject\") or f\"{digest_m.get('theme','Monthly Digest')} â€” {cur_month_name} MTD\"\n",
    "with open(MOM_SUBJ, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(subject_m.strip())\n",
    "\n",
    "def render_email_html_m(d):\n",
    "    theme = (d.get(\"theme\") or \"\").strip()\n",
    "    narrative = (d.get(\"narrative\") or \"\").strip()\n",
    "    parts = []\n",
    "    parts.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "    parts.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "    parts.append(f\"<h1 style='margin:0 0 4px 0;font-size:18px;'>Monthly Executive Summary ({cur_month_name} MTD)</h1>\")\n",
    "    if theme:\n",
    "        parts.append(f\"<div style='margin:0 0 12px 0;color:#6b7280;font-style:italic'>{theme}</div>\")\n",
    "    if narrative:\n",
    "        parts.append(f\"<p style='margin:0 0 16px 0'>{narrative}</p>\")\n",
    "\n",
    "    parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Key Metrics (MoM)</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "    for m in (d.get(\"key_metrics\") or [])[:8]:\n",
    "        name = m.get(\"name\",\"\")\n",
    "        val  = float(m.get(\"value\") or 0.0)\n",
    "        dp   = m.get(\"delta_pct\")\n",
    "        if isinstance(dp, (int,float)) and abs(dp) > 1: dp = dp/100.0\n",
    "        dp_txt = f\" ({dp*100:+.1f}%)\" if isinstance(dp,(int,float)) else \"\"\n",
    "        parts.append(f\"<li><b>{name}:</b> ${val:,.2f}{dp_txt}</li>\")\n",
    "    parts.append(\"</ul>\")\n",
    "\n",
    "    td = d.get(\"top_drivers\") or []\n",
    "    if td:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Drivers (MTD)</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for t in td[:5]:\n",
    "            label = (t.get(\"label\") or \"Uncategorized\").strip()\n",
    "            parts.append(f\"<li><b>{label}:</b> ${float(t.get('spend',0)):,.0f}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    rk = d.get(\"risks\") or []\n",
    "    if rk:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Risks</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for r in rk[:5]:\n",
    "            parts.append(f\"<li><b>{r.get('type','note')}:</b> {r.get('note','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    ai = d.get(\"action_items\") or []\n",
    "    if ai:\n",
    "        parts.append(\"<h2 style='margin:16px 0 8px 0;font-size:16px;'>Recommendations</h2><ul style='margin:0 0 12px 18px;'>\")\n",
    "        for a in ai[:5]:\n",
    "            parts.append(f\"<li><b>{a.get('title','')}</b> â€” est. ${float(a.get('impact_usd',0)):,.0f}. {a.get('rationale','')}</li>\")\n",
    "        parts.append(\"</ul>\")\n",
    "\n",
    "    parts.append(\"</div>\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "with open(MOM_HTML, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(render_email_html_m(digest_m))\n",
    "\n",
    "# --- 11) Flat CSV for Power BI (MoM) ---\n",
    "flat_rows_m = []\n",
    "flat_rows_m.append({\n",
    "    \"row_type\": \"header\",\n",
    "    \"as_of_end\": str(cur_mtd_end),\n",
    "    \"cur_start\": str(cur_month_start),\n",
    "    \"cur_end\": str(cur_mtd_end),\n",
    "    \"prev_start\": str(prev_month_start),\n",
    "    \"prev_end\": str(aligned_prev_m_end),\n",
    "    \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "    \"summary\": digest_m.get(\"narrative\",\"\"),\n",
    "    \"name\": \"Total Spend (MoM MTD)\",\n",
    "    \"value\": float(cur_spend_m),\n",
    "    \"delta_pct\": float(spend_delta_pct_m),\n",
    "    \"label\": \"\",\n",
    "    \"spend\": None,\n",
    "    \"note\": \"\",\n",
    "    \"impact_usd\": None,\n",
    "})\n",
    "for m in digest_m.get(\"key_metrics\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"metric\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": m.get(\"name\",\"\"),\n",
    "        \"value\": float(m.get(\"value\",0) or 0.0),\n",
    "        \"delta_pct\": (float(m.get(\"delta_pct\")) if isinstance(m.get(\"delta_pct\"), (int,float)) else None),\n",
    "        \"label\": \"\",\n",
    "        \"spend\": None,\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for t in digest_m.get(\"top_drivers\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"driver\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": (t.get(\"label\") or \"Uncategorized\"),\n",
    "        \"spend\": float(t.get(\"spend\",0) or 0.0),\n",
    "        \"note\": \"\",\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "for r in digest_m.get(\"risks\", []):\n",
    "    flat_rows_m.append({\n",
    "        \"row_type\": \"risk\",\n",
    "        \"as_of_end\": str(cur_mtd_end),\n",
    "        \"cur_start\": str(cur_month_start),\n",
    "        \"cur_end\": str(cur_mtd_end),\n",
    "        \"prev_start\": str(prev_month_start),\n",
    "        \"prev_end\": str(aligned_prev_m_end),\n",
    "        \"headline\": digest_m.get(\"headline\",\"\"),\n",
    "        \"summary\": \"\",\n",
    "        \"name\": \"\",\n",
    "        \"value\": None,\n",
    "        \"delta_pct\": None,\n",
    "        \"label\": r.get(\"type\",\"\"),\n",
    "        \"spend\": None,\n",
    "        \"note\": r.get(\"note\",\"\"),\n",
    "        \"impact_usd\": None,\n",
    "    })\n",
    "pd.DataFrame(flat_rows_m).to_csv(MOM_FLAT, index=False)\n",
    "\n",
    "print(\n",
    "    \"ðŸ§¾ Monthly executive digest written (MTD):\\n\"\n",
    "    f\"- JSON: {MOM_JSON}\\n- MD:   {MOM_MD}\\n- HTML: {MOM_HTML}\\n- CSV:  {MOM_FLAT}\\n- Subject: {MOM_SUBJ}\\n\"\n",
    "    f\"MTD Window: {cur_month_name} {cur_month_start} -> {cur_mtd_end} | Prior aligned: {prev_month_name} {prev_month_start} -> {aligned_prev_m_end}\\n\"\n",
    "    f\"MoM MTD Spend: cur={cur_spend_m} prev={prev_spend_m} delta={spend_delta_m} delta_pct={spend_delta_pct_m:+.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe61641a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T23:16:16.304708Z",
     "iopub.status.busy": "2025-09-13T23:16:16.303642Z",
     "iopub.status.idle": "2025-09-13T23:16:16.487913Z",
     "shell.execute_reply": "2025-09-13T23:16:16.484893Z"
    },
    "papermill": {
     "duration": 0.226546,
     "end_time": "2025-09-13T23:16:16.491086",
     "exception": false,
     "start_time": "2025-09-13T23:16:16.264540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Combined artifacts:\n",
      "- CSV:  C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_combined_flat.csv\n",
      "- HTML: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_combined_email.html\n",
      "- Subject: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\insights\\digest_combined_subject.txt\n",
      "(Sources -> Weekly: digest_latest_flat.csv, Monthly: digest_mom_flat.csv)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 16: Combine Weekly + Monthly outputs for Power BI + Email bundle ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Weekly artifacts from Cell 14\n",
    "W_JSON = INSIGHTS_DIR / \"digest_latest.json\"\n",
    "W_FLAT = INSIGHTS_DIR / \"digest_latest_flat.csv\"\n",
    "W_HTML = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "W_SUBJ = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "\n",
    "# Monthly artifacts from Cell 15\n",
    "M_JSON = INSIGHTS_DIR / \"digest_mom.json\"\n",
    "M_FLAT = INSIGHTS_DIR / \"digest_mom_flat.csv\"\n",
    "M_HTML = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "M_SUBJ = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# Combined outputs\n",
    "C_FLAT = INSIGHTS_DIR / \"digest_combined_flat.csv\"\n",
    "C_HTML = INSIGHTS_DIR / \"digest_combined_email.html\"\n",
    "C_SUBJ = INSIGHTS_DIR / \"digest_combined_subject.txt\"\n",
    "\n",
    "# --- 1) Combine flat CSVs with a 'period' column ---\n",
    "frames = []\n",
    "if W_FLAT.exists():\n",
    "    w = pd.read_csv(W_FLAT)\n",
    "    w[\"period\"] = \"WoW\"\n",
    "    frames.append(w)\n",
    "if M_FLAT.exists():\n",
    "    m = pd.read_csv(M_FLAT)\n",
    "    m[\"period\"] = \"MoM\"\n",
    "    frames.append(m)\n",
    "\n",
    "if frames:\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined.to_csv(C_FLAT, index=False)\n",
    "else:\n",
    "    pd.DataFrame(columns=[\"row_type\",\"period\"]).to_csv(C_FLAT, index=False)\n",
    "\n",
    "# --- 2) Build combined subject (uses theme if available) ---\n",
    "def read_text(p: Path) -> str:\n",
    "    try:\n",
    "        return (p.read_text(encoding=\"utf-8\") or \"\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def read_json(p: Path):\n",
    "    try:\n",
    "        import json\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "wj = read_json(W_JSON)\n",
    "mj = read_json(M_JSON)\n",
    "\n",
    "weekly_theme = (wj.get(\"theme\") or \"\").strip()\n",
    "month_name = \"\"\n",
    "try:\n",
    "    month_name = pd.to_datetime(mj.get(\"window\",{}).get(\"current\",{}).get(\"end\",\"\")).strftime(\"%B\")\n",
    "except Exception:\n",
    "    # fallback by looking at any MTD end in flat\n",
    "    try:\n",
    "        mf = pd.read_csv(M_FLAT)\n",
    "        if \"as_of_end\" in mf.columns and len(mf):\n",
    "            month_name = pd.to_datetime(mf[\"as_of_end\"].iloc[0]).strftime(\"%B\")\n",
    "    except Exception:\n",
    "        month_name = \"\"\n",
    "\n",
    "subject_week = read_text(W_SUBJ) or (weekly_theme and f\"{weekly_theme} â€” Weekly\") or \"Weekly Summary\"\n",
    "subject_month = read_text(M_SUBJ) or (month_name and f\"{month_name} MTD\") or \"Monthly MTD\"\n",
    "combined_subject = f\"{subject_week} | {subject_month}\"\n",
    "Path(C_SUBJ).write_text(combined_subject, encoding=\"utf-8\")\n",
    "\n",
    "# --- 3) Build combined HTML (reuses generated HTML blocks if present) ---\n",
    "def read_html(p: Path) -> str:\n",
    "    try:\n",
    "        return p.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "w_html = read_html(W_HTML)\n",
    "m_html = read_html(M_HTML)\n",
    "\n",
    "# Minimal wrapper that keeps each section's styling intact\n",
    "combined_html = []\n",
    "combined_html.append(\"<!doctype html><meta charset='utf-8'>\")\n",
    "combined_html.append(\"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\")\n",
    "combined_html.append(\"<h1 style='margin:0 0 12px 0;font-size:20px;'>Executive Summary â€” Weekly & Month-to-Date</h1>\")\n",
    "\n",
    "if w_html:\n",
    "    # Strip outer wrappers if present to avoid nested <html> tags\n",
    "    combined_html.append(\"<section style='margin-bottom:24px;border-bottom:1px solid #e5e7eb;padding-bottom:16px;'>\")\n",
    "    combined_html.append(w_html)\n",
    "    combined_html.append(\"</section>\")\n",
    "\n",
    "if m_html:\n",
    "    combined_html.append(\"<section style='margin-top:16px;'>\")\n",
    "    combined_html.append(m_html)\n",
    "    combined_html.append(\"</section>\")\n",
    "\n",
    "combined_html.append(\"</div>\")\n",
    "\n",
    "Path(C_HTML).write_text(\"\".join(combined_html), encoding=\"utf-8\")\n",
    "\n",
    "print(\n",
    "    \"ðŸ“¦ Combined artifacts:\\n\"\n",
    "    f\"- CSV:  {C_FLAT}\\n\"\n",
    "    f\"- HTML: {C_HTML}\\n\"\n",
    "    f\"- Subject: {C_SUBJ}\\n\"\n",
    "    f\"(Sources -> Weekly: {W_FLAT.name}, Monthly: {M_FLAT.name})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03f10b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Charts written â†’ weekly_spend_line.png weekly_top_categories_donut.png weekly_category_movement.png\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 16.5 (REPLACE): Exec-ready charts with wider figs, extra padding, percentile gradients ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P_LINE     = INSIGHTS_DIR / \"weekly_spend_line.png\"\n",
    "P_DONUT    = INSIGHTS_DIR / \"weekly_top_categories_donut.png\"\n",
    "P_MOVEMENT = INSIGHTS_DIR / \"weekly_category_movement.png\"\n",
    "\n",
    "# Fonts / styling (portable)\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Segoe UI\", \"DejaVu Sans\", \"Arial\", \"Liberation Sans\"]\n",
    "\n",
    "def fmt_usd(x):\n",
    "    try:\n",
    "        return f\"${float(x):,.0f}\"\n",
    "    except Exception:\n",
    "        return \"$0\"\n",
    "\n",
    "def spend_series(frame: pd.DataFrame, expenses_are_negative: bool) -> pd.Series:\n",
    "    a = frame[\"amount\"].astype(float)\n",
    "    return a.where(a < 0, 0).abs() if expenses_are_negative else a.where(a > 0, 0)\n",
    "\n",
    "def prefer_category_column(frame: pd.DataFrame) -> str:\n",
    "    for c in [\"category_display\",\"category\",\"category_final\",\"category_plaid\"]:\n",
    "        if c in frame.columns and frame[c].notna().any():\n",
    "            return c\n",
    "    return \"category\"\n",
    "\n",
    "# --- Common filters (mirror weekly digest logic) ---\n",
    "base = df.copy()\n",
    "for c in (\"display_name\",\"merchant_name\",\"name\"):\n",
    "    if c not in base.columns:\n",
    "        base[c] = \"\"\n",
    "txt_all = (base[\"display_name\"].astype(str) + \" \" +\n",
    "           base[\"merchant_name\"].astype(str) + \" \" +\n",
    "           base[\"name\"].astype(str)).str.upper()\n",
    "\n",
    "wealthfront_mask = txt_all.str.contains(r\"\\bWEALTHFRONT\\b\", na=False)\n",
    "applecash_mask   = txt_all.str.contains(r\"\\bAPPLE\\s+CASH\\b\", na=False)\n",
    "base = base.loc[~(wealthfront_mask & ~applecash_mask)].copy()\n",
    "\n",
    "if \"is_non_spend_flow\" in base.columns:\n",
    "    non_spend_mask = base[\"is_non_spend_flow\"].fillna(False).astype(bool)\n",
    "    keep_mask = (~non_spend_mask) | applecash_mask\n",
    "    base = base.loc[keep_mask].copy()\n",
    "\n",
    "base[\"date_only\"] = base[\"date\"].dt.date\n",
    "\n",
    "# Week window (Monâ€“Sun), consistent with Cell 14\n",
    "try:\n",
    "    _ = wk_start, wk_end\n",
    "except NameError:\n",
    "    try:\n",
    "        now = pd.Timestamp.now(tz=\"America/Los_Angeles\").normalize()\n",
    "    except Exception:\n",
    "        now = pd.Timestamp.now().normalize()\n",
    "    wd = int(now.weekday())  # Mon=0..Sun=6\n",
    "    days_to_last_sun = 7 if wd == 6 else (wd + 1)\n",
    "    wk_end   = (now - pd.Timedelta(days=days_to_last_sun)).date()\n",
    "    wk_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=6)).date()\n",
    "\n",
    "prev_wk_start = (pd.Timestamp(wk_start) - pd.Timedelta(days=7)).date()\n",
    "prev_wk_end   = (pd.Timestamp(wk_start) - pd.Timedelta(days=1)).date()\n",
    "\n",
    "# Polarity\n",
    "amt_all = base[\"amount\"].dropna()\n",
    "expenses_are_negative = (amt_all < 0).sum() > (amt_all > 0).sum()\n",
    "\n",
    "# ================= 1) Daily trend â€” last 28 days (classic style; wider fig & padding) =================\n",
    "plot_start = (pd.Timestamp(wk_end) - pd.Timedelta(days=27)).date()\n",
    "tw = base[(base[\"date_only\"] >= plot_start) & (base[\"date_only\"] <= wk_end)].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11.2, 5.8), dpi=144)  # wider\n",
    "if tw.empty:\n",
    "    ax.text(0.5, 0.5, \"No spend data (last 28 days)\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    tw[\"dt\"] = pd.to_datetime(tw[\"date_only\"])\n",
    "    daily = (tw.assign(spend=spend_series(tw, expenses_are_negative))\n",
    "               .groupby(\"dt\", dropna=False)[\"spend\"].sum()\n",
    "               .sort_index())\n",
    "    idx = pd.date_range(start=pd.to_datetime(plot_start), end=pd.to_datetime(wk_end), freq=\"D\")\n",
    "    daily = daily.reindex(idx, fill_value=0.0)\n",
    "    ma7 = daily.rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Classic lines\n",
    "    ax.plot(daily.index, daily.values, marker=\"o\", linewidth=2, label=\"Total Spend\")\n",
    "    ax.plot(ma7.index, ma7.values, linestyle=\"--\", linewidth=2, label=\"7-Day Average\")\n",
    "\n",
    "    # Currency on Y\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, p: fmt_usd(v)))\n",
    "\n",
    "    # X-axis like \"Aug-28\"\n",
    "    def _fmt_mmm_day(xv, pos):\n",
    "        try:\n",
    "            dt = mdates.num2date(xv)\n",
    "            return f\"{dt.strftime('%b')}-{dt.day}\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=6, maxticks=10))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(_fmt_mmm_day))\n",
    "\n",
    "    # Week-start dashed lines in light gray\n",
    "    for vline in [pd.to_datetime(prev_wk_start), pd.to_datetime(wk_start)]:\n",
    "        ax.axvline(vline, linestyle=\"--\", linewidth=1, alpha=0.8, color=\"#d1d5db\")\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=0.35)\n",
    "    ax.set_title(\"Total Spending â€” Last 28 Days\", pad=8, fontsize=12)\n",
    "    ax.legend(frameon=False, loc=\"upper left\")\n",
    "\n",
    "    # Extra breathing room\n",
    "    ax.set_xlim(daily.index.min(), daily.index.max())\n",
    "    ax.set_ylim(0, max(1.0, float(daily.values.max()) * 1.18))\n",
    "    ax.margins(x=0.03, y=0.14)\n",
    "\n",
    "fig.tight_layout(rect=[0.02, 0.02, 0.98, 0.98])\n",
    "fig.savefig(P_LINE, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Prep week frames\n",
    "cat_col = prefer_category_column(base)\n",
    "\n",
    "cur_w  = base[(base[\"date_only\"] >= wk_start) & (base[\"date_only\"] <= wk_end)].copy()\n",
    "prev_w = base[(base[\"date_only\"] >= prev_wk_start) & (base[\"date_only\"] <= prev_wk_end)].copy()\n",
    "\n",
    "# Spend-only frames\n",
    "if expenses_are_negative:\n",
    "    cur_exp  = cur_w[cur_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "    prev_exp = prev_w[prev_w[\"amount\"] < 0].assign(spend=lambda x: x[\"amount\"].abs())\n",
    "else:\n",
    "    cur_exp  = cur_w[cur_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "    prev_exp = prev_w[prev_w[\"amount\"] > 0].assign(spend=lambda x: x[\"amount\"])\n",
    "\n",
    "# Clean categories and exclude admin buckets\n",
    "EXCLUDE_CATS = {\"Transfers\",\"Income\",\"Debt Payments\",\"Fees\"}\n",
    "def _clean_cat(s):\n",
    "    s = (\"\" if pd.isna(s) else str(s)).strip()\n",
    "    return \"Uncategorized\" if s == \"\" or s.lower() in {\"none\",\"nan\"} else s\n",
    "\n",
    "cur_exp[cat_col]  = cur_exp[cat_col].apply(_clean_cat)\n",
    "prev_exp[cat_col] = prev_exp[cat_col].apply(_clean_cat)\n",
    "\n",
    "# ================= 2) Donut â€” Top Categories This Week (legend bottom; wider fig & bottom margin) =================\n",
    "top_cats = (cur_exp[~cur_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "            .groupby(cat_col, dropna=False)[\"spend\"].sum()\n",
    "            .sort_values(ascending=False).head(6))\n",
    "labels = [str(i) for i in top_cats.index]\n",
    "values = top_cats.values\n",
    "total  = float(top_cats.sum()) or 1.0\n",
    "\n",
    "# Bright palette\n",
    "bright_colors = [\"#6366F1\",\"#F59E0B\",\"#10B981\",\"#EF4444\",\"#3B82F6\",\"#A855F7\",\"#F97316\",\"#06B6D4\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.2, 6.6), dpi=144)  # wider\n",
    "if len(top_cats) == 0:\n",
    "    ax.text(0.5, 0.5, \"No category data for this week\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    ret = ax.pie(values, labels=None, autopct=None, startangle=90, colors=bright_colors[:len(values)])\n",
    "    wedges = ret[0]\n",
    "\n",
    "    # Donut hole\n",
    "    centre_circle = plt.Circle((0,0), 0.55, fc=\"white\")\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "\n",
    "    # Legend at the BOTTOM; no legend title\n",
    "    pretty_labels = [f\"{lab} â€” {fmt_usd(val)} ({val/total:,.0%})\" for lab, val in zip(labels, values)]\n",
    "    ax.legend(\n",
    "        wedges, pretty_labels,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.05),\n",
    "        ncol=min(3, len(pretty_labels)),\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Top Categories This Week\", pad=8, fontsize=12)\n",
    "    ax.axis('equal')\n",
    "\n",
    "# Leave extra bottom room for the legend\n",
    "fig.tight_layout(rect=[0.02, 0.10, 0.98, 0.98])\n",
    "fig.savefig(P_DONUT, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ================= 3) Category Movement WoW (This Week - Prior) =================\n",
    "cur_s  = (cur_exp[~cur_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "          .groupby(cat_col, dropna=False)[\"spend\"].sum())\n",
    "prev_s = (prev_exp[~prev_exp[cat_col].isin(EXCLUDE_CATS)]\n",
    "          .groupby(cat_col, dropna=False)[\"spend\"].sum())\n",
    "\n",
    "cats = sorted(set(cur_s.index) | set(prev_s.index))\n",
    "cur_s  = cur_s.reindex(cats, fill_value=0.0)\n",
    "prev_s = prev_s.reindex(cats, fill_value=0.0)\n",
    "delta = (cur_s - prev_s)\n",
    "\n",
    "# Keep top movers by absolute change\n",
    "delta = delta[delta != 0].sort_values(key=np.abs, ascending=False).head(10)\n",
    "labels_mv = list(delta.index)\n",
    "vals_mv   = delta.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11.8, 6.6), dpi=144)  # wider\n",
    "if len(delta) == 0:\n",
    "    ax.text(0.5, 0.5, \"No category movement WoW\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    ax.axis(\"off\")\n",
    "else:\n",
    "    abs_mv = np.abs(vals_mv)\n",
    "    max_abs = float(abs_mv.max()) if len(abs_mv) else 1.0\n",
    "\n",
    "    # ----- Percentile-based color intensity (robust to scale) -----\n",
    "    # Rank by absolute change (ascending); convert to 0..1 percentile\n",
    "    if len(abs_mv) > 1:\n",
    "        ranks = abs_mv.argsort().argsort() + 1  # 1..N\n",
    "        pct = ranks / float(len(abs_mv))        # 0..1\n",
    "    else:\n",
    "        pct = np.array([1.0])\n",
    "\n",
    "    def hex_to_rgb(h):\n",
    "        h = h.lstrip(\"#\")\n",
    "        return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        return \"#{:02X}{:02X}{:02X}\".format(*rgb)\n",
    "\n",
    "    def lerp(a, b, t):\n",
    "        return int(a + (b - a) * t)\n",
    "\n",
    "    def lerp_hex(c1, c2, t):\n",
    "        r1,g1,b1 = hex_to_rgb(c1); r2,g2,b2 = hex_to_rgb(c2)\n",
    "        return rgb_to_hex((lerp(r1,r2,t), lerp(g1,g2,t), lerp(b1,b2,t)))\n",
    "\n",
    "    # Tailwind-ish ramps (light -> dark)\n",
    "    RED_LIGHT   = \"#FECACA\"  # red-300\n",
    "    RED_DARK    = \"#B91C1C\"  # red-700\n",
    "    GREEN_LIGHT = \"#BBF7D0\"  # green-200\n",
    "    GREEN_DARK  = \"#065F46\"  # emerald-900\n",
    "\n",
    "    # Keep saturation between 0.35 and 1.0 to stay vivid\n",
    "    t = 0.35 + 0.65 * pct\n",
    "    colors = [\n",
    "        lerp_hex(GREEN_LIGHT, GREEN_DARK, ti) if v < 0 else lerp_hex(RED_LIGHT, RED_DARK, ti)\n",
    "        for v, ti in zip(vals_mv, t)\n",
    "    ]\n",
    "\n",
    "    # Bars\n",
    "    y = np.arange(len(labels_mv))[::-1]\n",
    "    ax.barh(y, vals_mv, height=0.55, color=colors, edgecolor=\"none\")\n",
    "\n",
    "    # Y labels\n",
    "    ax.set_yticks(y, labels_mv)\n",
    "\n",
    "    # Extra horizontal padding so labels never crowd the axis\n",
    "    pad_abs = max(30.0, (float(max_abs) if len(abs_mv) else 1.0) * 0.22)\n",
    "    xmin = -max_abs - pad_abs\n",
    "    xmax =  max_abs + pad_abs\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.margins(x=0.06)\n",
    "\n",
    "    # Annotate each bar with $ change, placed slightly outside the bar\n",
    "    for yi, v in zip(y, vals_mv):\n",
    "        offset = max(10.0, (float(max_abs) if len(abs_mv) else 1.0) * 0.06)\n",
    "        x_text = v + (offset if v >= 0 else -offset)\n",
    "        ha = \"left\" if v >= 0 else \"right\"\n",
    "        ax.text(x_text, yi, f\"{fmt_usd(v)}\", va=\"center\", ha=ha, fontsize=10)\n",
    "\n",
    "    # Zero reference line + grid\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1, alpha=0.7, color=\"#9ca3af\")\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: fmt_usd(x)))\n",
    "    ax.grid(axis=\"x\", linestyle=\":\", alpha=0.35)\n",
    "\n",
    "    ax.set_title(\"Category Movement WoW (This Week vs Prior)\", pad=10, fontsize=12)\n",
    "\n",
    "fig.tight_layout(rect=[0.03, 0.02, 0.99, 0.98])\n",
    "fig.savefig(P_MOVEMENT, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"ðŸ“Š Charts written â†’\", P_LINE.name, P_DONUT.name, P_MOVEMENT.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88c42a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“§ Email sent via STARTTLS to kosisonna.ugo@gmail.com â€” subject: Weekly Financial Update: Major Spending Reduction Achieved | September Financial Overview: Spending Down, Income Up!\n",
      "Inline images: ['weekly_spend_line.png', 'weekly_top_categories_donut.png', 'weekly_category_movement.png']\n",
      "Attachments: none\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 17: Email dispatch (inline images only, NO attachments) ---\n",
    "# Gmail-ready SMTP, kill switch, inline PNG charts via CID.\n",
    "# Removes all file attachments (CSVs and image fallbacks).\n",
    "\n",
    "import os, smtplib, ssl, re\n",
    "from pathlib import Path\n",
    "from email.message import EmailMessage\n",
    "\n",
    "def _mask(s):\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:3] + \"â€¦\" + s[-3:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "INSIGHTS_DIR = DATA_PROCESSED / \"insights\"\n",
    "INSIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Primary artifacts for body/subject\n",
    "C_HTML = INSIGHTS_DIR / \"digest_combined_email.html\"\n",
    "C_SUBJ = INSIGHTS_DIR / \"digest_combined_subject.txt\"\n",
    "W_HTML = INSIGHTS_DIR / \"digest_latest_email.html\"\n",
    "W_SUBJ = INSIGHTS_DIR / \"digest_latest_subject.txt\"\n",
    "M_HTML = INSIGHTS_DIR / \"digest_mom_email.html\"\n",
    "M_SUBJ = INSIGHTS_DIR / \"digest_mom_subject.txt\"\n",
    "\n",
    "# Chart PNGs (from Cell 16.5)\n",
    "WEEKLY_LINE_PATH = INSIGHTS_DIR / \"weekly_spend_line.png\"\n",
    "WEEKLY_PIE_PATH  = INSIGHTS_DIR / \"weekly_top_categories_pie.png\"\n",
    "\n",
    "# ---------------- Kill switch ----------------\n",
    "EMAIL_ENABLED = (os.getenv(\"EMAIL_ENABLED\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "EMAIL_KILL_FILE = STATE_DIR / \"EMAIL_KILL\"\n",
    "if EMAIL_KILL_FILE.exists():\n",
    "    EMAIL_ENABLED = False\n",
    "\n",
    "EMAIL_DRY_RUN = (os.getenv(\"EMAIL_DRY_RUN\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n",
    "\n",
    "if not EMAIL_ENABLED:\n",
    "    print(\"âœ‹ Email sending disabled (kill switch). Set EMAIL_ENABLED=1 and remove .state/EMAIL_KILL to re-enable.\")\n",
    "else:\n",
    "    # ---------------- SMTP (Gmail-ready) ----------------\n",
    "    SMTP_HOST = os.getenv(\"SMTP_HOST\", \"smtp.gmail.com\").strip()\n",
    "    SMTP_PORT = int(os.getenv(\"SMTP_PORT\", \"587\"))\n",
    "    SMTP_SSL_PORT = int(os.getenv(\"SMTP_SSL_PORT\", \"465\"))\n",
    "    SMTP_USERNAME = (os.getenv(\"SMTP_USERNAME\", \"\") or \"\").strip()\n",
    "    SMTP_PASSWORD = (os.getenv(\"SMTP_PASSWORD\", \"\") or \"\").replace(\" \", \"\")  # trim spaces Google shows\n",
    "    SMTP_STARTTLS = (os.getenv(\"SMTP_STARTTLS\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "\n",
    "    EMAIL_FROM = (os.getenv(\"EMAIL_FROM\", \"\") or \"\").strip()\n",
    "    EMAIL_TO   = (os.getenv(\"EMAIL_TO\", \"\") or \"\").strip()\n",
    "    EMAIL_CC   = (os.getenv(\"EMAIL_CC\", \"\") or \"\").strip()\n",
    "    EMAIL_BCC  = (os.getenv(\"EMAIL_BCC\", \"\") or \"\").strip()\n",
    "\n",
    "    SUBJECT_OVERRIDE = os.getenv(\"EMAIL_SUBJECT_OVERRIDE\", \"\").strip()\n",
    "    BODY_HTML_OVERRIDE_PATH = os.getenv(\"EMAIL_BODY_HTML_PATH\", \"\").strip()\n",
    "\n",
    "    # Minimal validation\n",
    "    missing = [k for k,v in {\n",
    "        \"SMTP_HOST\": SMTP_HOST,\n",
    "        \"SMTP_USERNAME\": SMTP_USERNAME,\n",
    "        \"SMTP_PASSWORD\": SMTP_PASSWORD,\n",
    "        \"EMAIL_FROM\": EMAIL_FROM,\n",
    "        \"EMAIL_TO\": EMAIL_TO,\n",
    "    }.items() if not v]\n",
    "    if missing:\n",
    "        raise RuntimeError(\"Email config missing: \" + \", \".join(missing))\n",
    "\n",
    "    # ---------------- Subject & HTML body ----------------\n",
    "    def _read_text(p: Path) -> str:\n",
    "        try: return (p.read_text(encoding=\"utf-8\") or \"\").strip()\n",
    "        except Exception: return \"\"\n",
    "\n",
    "    def _read_html(p: Path) -> str:\n",
    "        try: return p.read_text(encoding=\"utf-8\")\n",
    "        except Exception: return \"\"\n",
    "\n",
    "    subject = SUBJECT_OVERRIDE or _read_text(C_SUBJ) or _read_text(W_SUBJ) or _read_text(M_SUBJ) or \"AI Credit Card Dashboard â€” Digest\"\n",
    "\n",
    "    if BODY_HTML_OVERRIDE_PATH:\n",
    "        body_html = Path(BODY_HTML_OVERRIDE_PATH).read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        body_html = _read_html(C_HTML) or _read_html(W_HTML) or _read_html(M_HTML)\n",
    "\n",
    "    if not body_html:\n",
    "        body_html = (\n",
    "            \"<!doctype html><meta charset='utf-8'>\"\n",
    "            \"<div style='font-family:Segoe UI,system-ui,-apple-system;line-height:1.55;font-size:14px;color:#111827;'>\"\n",
    "            \"<h1 style='margin:0 0 8px 0;font-size:18px;'>AI Credit Card Dashboard â€” Digest</h1>\"\n",
    "            \"<p>No HTML digest was found this run. Check earlier cells for generation status.</p>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    # Plain-text fallback\n",
    "    def _html_to_text(h: str) -> str:\n",
    "        t = re.sub(r\"<(br|/p|/li)>\", \"\\n\", h, flags=re.IGNORECASE)\n",
    "        t = re.sub(r\"<[^>]+>\", \"\", t)\n",
    "        return re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    body_text = _html_to_text(body_html)\n",
    "\n",
    "    # ---------------- Prepare image refs & append <img> once ----------------\n",
    "    img_refs = []\n",
    "    if (INSIGHTS_DIR / \"weekly_spend_line.png\").exists():\n",
    "        img_refs.append((\"weekly_line\", INSIGHTS_DIR / \"weekly_spend_line.png\"))\n",
    "    if (INSIGHTS_DIR / \"weekly_top_categories_donut.png\").exists():\n",
    "        img_refs.append((\"weekly_top_categories_donut\", INSIGHTS_DIR / \"weekly_top_categories_donut.png\"))\n",
    "    if (INSIGHTS_DIR / \"weekly_category_movement.png\").exists():\n",
    "        img_refs.append((\"weekly_category_movement\", INSIGHTS_DIR / \"weekly_category_movement.png\"))\n",
    "\n",
    "    # ---------------- Build email ----------------\n",
    "    def _split_emails(s): return [e.strip() for e in s.split(\",\") if e.strip()]\n",
    "    rcpts = []\n",
    "    seen = set()\n",
    "    for e in _split_emails(EMAIL_TO) + _split_emails(EMAIL_CC) + _split_emails(EMAIL_BCC):\n",
    "        if e.lower() not in seen:\n",
    "            seen.add(e.lower()); rcpts.append(e)\n",
    "    if not rcpts:\n",
    "        raise RuntimeError(\"No recipients found (EMAIL_TO/CC/BCC).\")\n",
    "\n",
    "    msg = EmailMessage()\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = EMAIL_FROM\n",
    "    msg[\"To\"] = \", \".join(_split_emails(EMAIL_TO))\n",
    "    if EMAIL_CC: msg[\"Cc\"] = \", \".join(_split_emails(EMAIL_CC))\n",
    "\n",
    "    # Force multipart/alternative container and add HTML\n",
    "    msg.set_content(body_text)\n",
    "    msg.make_alternative()\n",
    "    msg.add_alternative(body_html, subtype=\"html\")\n",
    "    html_part = msg.get_body(preferencelist=(\"html\",))\n",
    "\n",
    "    # Inline embed only (no fallback attachments)\n",
    "    embedded = []\n",
    "    if html_part is not None and img_refs:\n",
    "        for cid, pth in img_refs:\n",
    "            try:\n",
    "                with open(pth, \"rb\") as f:\n",
    "                    html_part.add_related(\n",
    "                        f.read(),\n",
    "                        maintype=\"image\",\n",
    "                        subtype=\"png\",\n",
    "                        cid=f\"<{cid}>\",\n",
    "                        filename=pth.name\n",
    "                    )\n",
    "                embedded.append(pth.name)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Inline embed failed for {pth.name}: {e}. Skipping image (no attachments by policy).\")\n",
    "    elif img_refs:\n",
    "        print(\"âš ï¸ Could not locate HTML part; skipping inline images (no attachments by policy).\")\n",
    "\n",
    "    # ---------------- Send (STARTTLS then SSL fallback) ----------------\n",
    "    context = ssl.create_default_context()\n",
    "\n",
    "    def _try_starttls():\n",
    "        with smtplib.SMTP(SMTP_HOST, SMTP_PORT, timeout=60) as server:\n",
    "            server.ehlo()\n",
    "            if SMTP_STARTTLS: server.starttls(context=context); server.ehlo()\n",
    "            server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "            if EMAIL_DRY_RUN:\n",
    "                print(\"âœ… STARTTLS login OK (dry-run).\"); return\n",
    "            server.send_message(msg, to_addrs=rcpts)\n",
    "\n",
    "    def _try_ssl():\n",
    "        with smtplib.SMTP_SSL(SMTP_HOST, SMTP_SSL_PORT, context=context, timeout=60) as server:\n",
    "            server.ehlo()\n",
    "            server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "            if EMAIL_DRY_RUN:\n",
    "                print(\"âœ… SSL login OK (dry-run).\"); return\n",
    "            server.send_message(msg, to_addrs=rcpts)\n",
    "\n",
    "    try:\n",
    "        _try_starttls()\n",
    "        print(f\"ðŸ“§ Email sent via STARTTLS to {', '.join(rcpts)} â€” subject: {subject}\")\n",
    "    except smtplib.SMTPAuthenticationError as e:\n",
    "        print(\"âŒ STARTTLS auth failed:\", e.smtp_error.decode() if hasattr(e, \"smtp_error\") else str(e))\n",
    "        print(\"â€¦attempting SSL on port\", SMTP_SSL_PORT)\n",
    "        _try_ssl()\n",
    "        print(f\"ðŸ“§ Email sent via SSL to {', '.join(rcpts)} â€” subject: {subject}\")\n",
    "\n",
    "    print(\"Inline images:\", embedded)\n",
    "    print(\"Attachments: none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23894e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EMAIL DIAG ===\n",
      "EMAIL_ENABLED: True | kill-switch file present: False\n",
      "EMAIL_DRY_RUN: False\n",
      "SMTP_HOST: smtp.gmail.com  | STARTTLS: True\n",
      "SMTP_PORT: 587  | SMTP_SSL_PORT: 465\n",
      "SMTP_USERNAME: kosâ€¦com\n",
      "SMTP_PASSWORD: hwqâ€¦lvo\n",
      "EMAIL_FROM: kosisonna.ugo@gmail.com\n",
      "EMAIL_TO: kosisonna.ugo@gmail.com\n",
      "EMAIL_CC: \n",
      "EMAIL_BCC: \n",
      "Recipients parsed: ['kosisonna.ugo@gmail.com']\n",
      "TCP reach 587: True\n",
      "TCP reach 465 (ssl): True\n",
      "SMTP AUTH OK on STARTTLS\n",
      "SMTP AUTH OK on SSL\n",
      "=== END EMAIL DIAG ===\n"
     ]
    }
   ],
   "source": [
    "import os, socket, ssl, smtplib\n",
    "from pathlib import Path\n",
    "\n",
    "def _mask(s):\n",
    "    if not s: return \"<missing>\"\n",
    "    s = str(s)\n",
    "    return (s[:3] + \"â€¦\" + s[-3:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "STATE_DIR = Path(globals().get(\"STATE_DIR\", Path(\".state\")))\n",
    "kill = (STATE_DIR / \"EMAIL_KILL\").exists()\n",
    "\n",
    "SMTP_HOST = (os.getenv(\"SMTP_HOST\", \"smtp.gmail.com\") or \"\").strip()\n",
    "SMTP_PORT = int(os.getenv(\"SMTP_PORT\", \"587\"))\n",
    "SMTP_SSL_PORT = int(os.getenv(\"SMTP_SSL_PORT\", \"465\"))\n",
    "SMTP_USERNAME = (os.getenv(\"SMTP_USERNAME\", \"\") or \"\").strip()\n",
    "SMTP_PASSWORD = (os.getenv(\"SMTP_PASSWORD\", \"\") or \"\").strip()\n",
    "SMTP_STARTTLS = (os.getenv(\"SMTP_STARTTLS\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "\n",
    "EMAIL_ENABLED = (os.getenv(\"EMAIL_ENABLED\", \"1\") or \"1\").strip().lower() not in {\"0\",\"false\",\"no\",\"off\"}\n",
    "EMAIL_DRY_RUN = (os.getenv(\"EMAIL_DRY_RUN\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}\n",
    "EMAIL_FROM = (os.getenv(\"EMAIL_FROM\", \"\") or \"\").strip()\n",
    "EMAIL_TO   = (os.getenv(\"EMAIL_TO\", \"\") or \"\").strip()\n",
    "EMAIL_CC   = (os.getenv(\"EMAIL_CC\", \"\") or \"\").strip()\n",
    "EMAIL_BCC  = (os.getenv(\"EMAIL_BCC\", \"\") or \"\").strip()\n",
    "\n",
    "print(\"=== EMAIL DIAG ===\")\n",
    "print(\"EMAIL_ENABLED:\", EMAIL_ENABLED, \"| kill-switch file present:\", kill)\n",
    "print(\"EMAIL_DRY_RUN:\", EMAIL_DRY_RUN)\n",
    "print(\"SMTP_HOST:\", SMTP_HOST, \" | STARTTLS:\", SMTP_STARTTLS)\n",
    "print(\"SMTP_PORT:\", SMTP_PORT, \" | SMTP_SSL_PORT:\", SMTP_SSL_PORT)\n",
    "print(\"SMTP_USERNAME:\", _mask(SMTP_USERNAME))\n",
    "print(\"SMTP_PASSWORD:\", _mask(SMTP_PASSWORD))\n",
    "print(\"EMAIL_FROM:\", EMAIL_FROM)\n",
    "print(\"EMAIL_TO:\", EMAIL_TO)\n",
    "print(\"EMAIL_CC:\", EMAIL_CC)\n",
    "print(\"EMAIL_BCC:\", EMAIL_BCC)\n",
    "\n",
    "# quick recipient sanity\n",
    "rcpts = [e.strip() for e in (EMAIL_TO + \",\" + EMAIL_CC + \",\" + EMAIL_BCC).split(\",\") if e.strip()]\n",
    "print(\"Recipients parsed:\", rcpts)\n",
    "\n",
    "# check ports reachability\n",
    "def check_port(host, port, ssl_wrap=False):\n",
    "    try:\n",
    "        s = socket.create_connection((host, port), timeout=8)\n",
    "        if ssl_wrap:\n",
    "            ctx = ssl.create_default_context()\n",
    "            s = ctx.wrap_socket(s, server_hostname=host)\n",
    "        s.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Port check fail {host}:{port}{' (ssl)' if ssl_wrap else ''} ->\", repr(e))\n",
    "        return False\n",
    "\n",
    "print(\"TCP reach 587:\", check_port(SMTP_HOST, SMTP_PORT))\n",
    "print(\"TCP reach 465 (ssl):\", check_port(SMTP_HOST, SMTP_SSL_PORT, ssl_wrap=True))\n",
    "\n",
    "# optional: auth probe without sending\n",
    "try:\n",
    "    with smtplib.SMTP(SMTP_HOST, SMTP_PORT, timeout=15) as server:\n",
    "        server.ehlo()\n",
    "        if SMTP_STARTTLS:\n",
    "            import ssl as _ssl\n",
    "            server.starttls(context=_ssl.create_default_context()); server.ehlo()\n",
    "        server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "        print(\"SMTP AUTH OK on STARTTLS\")\n",
    "except smtplib.SMTPAuthenticationError as e:\n",
    "    print(\"SMTP AUTH FAIL on STARTTLS:\", e.smtp_error.decode() if hasattr(e, \"smtp_error\") else str(e))\n",
    "except Exception as e:\n",
    "    print(\"SMTP STARTTLS path error:\", repr(e))\n",
    "\n",
    "try:\n",
    "    with smtplib.SMTP_SSL(SMTP_HOST, SMTP_SSL_PORT, timeout=15) as server:\n",
    "        server.ehlo()\n",
    "        server.login(SMTP_USERNAME, SMTP_PASSWORD)\n",
    "        print(\"SMTP AUTH OK on SSL\")\n",
    "except smtplib.SMTPAuthenticationError as e:\n",
    "    print(\"SMTP AUTH FAIL on SSL:\", e.smtp_error.decode() if hasattr(e, \"smtp_error\") else str(e))\n",
    "except Exception as e:\n",
    "    print(\"SMTP SSL path error:\", repr(e))\n",
    "\n",
    "print(\"=== END EMAIL DIAG ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.870682,
   "end_time": "2025-09-13T23:16:17.727110",
   "environment_variables": {},
   "exception": null,
   "input_path": "scripts/enrich_transactions.ipynb",
   "output_path": "scripts/enrich_transactions.ipynb",
   "parameters": {},
   "start_time": "2025-09-13T23:16:02.856428",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
