{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d990ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK → PLAID_CLIENT_ID: 68bb…6689 | PLAID_ENV: production | OUTPUT_DIR: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw | TOKENS_PATH: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\.state\\access_tokens.json\n",
      "Loaded 3 token(s).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Env + paths + Plaid tokens (simple, robust) ---\n",
    "import os, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to load .env files if present (no hard dependency)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    for p in [Path.cwd() / \"scripts\" / \".env\", Path.cwd() / \".env\"]:\n",
    "        if p.exists():\n",
    "            load_dotenv(p, override=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s:\n",
    "        return \"<missing>\"\n",
    "    return (s[:4] + \"…\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# Resolve repo root (prefer GitHub workspace in CI)\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "if gw:\n",
    "    repo_root = Path(gw).resolve()\n",
    "else:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    repo_root = next((p for p in [cwd, *cwd.parents] if (p / \".git\").exists()), cwd)\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR  = Path(os.getenv(\"OUTPUT_DIR\", repo_root / \"data\" / \"raw\"))\n",
    "STATE_DIR   = Path(os.getenv(\"STATE_DIR\",  repo_root / \".state\"))\n",
    "TOKENS_PATH = Path(os.getenv(\"TOKENS_PATH\", STATE_DIR / \"access_tokens.json\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Basic Plaid env\n",
    "PLAID_CLIENT_ID = os.getenv(\"PLAID_CLIENT_ID\", \"\")\n",
    "PLAID_SECRET    = os.getenv(\"PLAID_SECRET\", \"\")\n",
    "PLAID_ENV       = (os.getenv(\"PLAID_ENV\", \"production\") or \"production\").strip().lower()\n",
    "PLAID_ENV = {\"prod\":\"production\",\"live\":\"production\",\"dev\":\"development\",\"devel\":\"development\",\"sb\":\"sandbox\"}.get(PLAID_ENV, PLAID_ENV)\n",
    "if PLAID_ENV not in {\"production\",\"development\",\"sandbox\"}:\n",
    "    PLAID_ENV = \"production\"\n",
    "\n",
    "# --- Load access tokens (env first, then .state/access_tokens.json) ---\n",
    "def _parse_tokens(blob) -> dict[str, str]:\n",
    "    if isinstance(blob, dict):\n",
    "        return {str(k): str(v).strip() for k, v in blob.items() if v}\n",
    "    if isinstance(blob, list):\n",
    "        out = {}\n",
    "        for item in blob:\n",
    "            if isinstance(item, dict):\n",
    "                name  = item.get(\"issuer\") or item.get(\"bank\") or item.get(\"name\")\n",
    "                token = item.get(\"access_token\") or item.get(\"token\")\n",
    "                if name and token:\n",
    "                    out[str(name)] = str(token).strip()\n",
    "        return out\n",
    "    if isinstance(blob, str):\n",
    "        s = blob.strip()\n",
    "        # Try JSON first\n",
    "        try:\n",
    "            return _parse_tokens(json.loads(s))\n",
    "        except Exception:\n",
    "            # Fallback: key=value or key:value pairs separated by , ; | or newlines\n",
    "            out = {}\n",
    "            for part in re.split(r\"[,\\n;|]+\", s):\n",
    "                part = part.strip()\n",
    "                if not part:\n",
    "                    continue\n",
    "                sep = \"=\" if \"=\" in part else (\":\" if \":\" in part else None)\n",
    "                if not sep:\n",
    "                    continue\n",
    "                k, v = part.split(sep, 1)\n",
    "                k, v = k.strip().strip('\"\\' '), v.strip().strip('\"\\' ')\n",
    "                if k and v:\n",
    "                    out[k] = v\n",
    "            return out\n",
    "    return {}\n",
    "\n",
    "def load_access_tokens() -> dict[str, str]:\n",
    "    env_blob = os.getenv(\"PLAID_ACCESS_TOKENS\", \"\").strip()\n",
    "    if env_blob:\n",
    "        t = _parse_tokens(env_blob)\n",
    "        if t:\n",
    "            return t\n",
    "    if TOKENS_PATH.exists():\n",
    "        try:\n",
    "            t = _parse_tokens(TOKENS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            if t:\n",
    "                return t\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise AssertionError(\n",
    "        f\"Could not load access tokens. Set PLAID_ACCESS_TOKENS or place JSON/kv pairs at {TOKENS_PATH}.\"\n",
    "    )\n",
    "\n",
    "ACCESS_TOKENS = load_access_tokens()\n",
    "\n",
    "# Gentle validation (non-fatal): tokens should match the selected environment\n",
    "expected_prefix = f\"access-{PLAID_ENV}-\"\n",
    "bad = [name for name, tok in ACCESS_TOKENS.items() if not str(tok).startswith(expected_prefix)]\n",
    "if bad:\n",
    "    print(f\"⚠️ Some tokens don’t look like '{expected_prefix}…' → {bad} (continuing anyway)\")\n",
    "\n",
    "print(\n",
    "    \"Env OK →\",\n",
    "    \"PLAID_CLIENT_ID:\", mask(PLAID_CLIENT_ID),\n",
    "    \"| PLAID_ENV:\", PLAID_ENV,\n",
    "    \"| OUTPUT_DIR:\", str(OUTPUT_DIR),\n",
    "    \"| TOKENS_PATH:\", str(TOKENS_PATH),\n",
    ")\n",
    "print(f\"Loaded {len(ACCESS_TOKENS)} token(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a22fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaid SDK: v10+ (plaid_api)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Plaid client init (v10+ preferred, legacy fallback) ---\n",
    "USE_PLAID_V10 = False\n",
    "client = None\n",
    "\n",
    "try:\n",
    "    # v10+ path\n",
    "    from plaid.api import plaid_api\n",
    "    from plaid.configuration import Configuration\n",
    "    try:\n",
    "        from plaid.configuration import Environment  # newer enum\n",
    "        env_host = {\n",
    "            \"production\":  Environment.Production,\n",
    "            \"development\": Environment.Development,\n",
    "            \"sandbox\":     Environment.Sandbox,\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=env_host)\n",
    "    except Exception:\n",
    "        # fallback if Environment enum not present\n",
    "        host_url = {\n",
    "            \"production\":  \"https://production.plaid.com\",\n",
    "            \"development\": \"https://development.plaid.com\",\n",
    "            \"sandbox\":     \"https://sandbox.plaid.com\",\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=host_url)\n",
    "\n",
    "    from plaid.api_client import ApiClient\n",
    "    config.api_key[\"clientId\"] = PLAID_CLIENT_ID\n",
    "    config.api_key[\"secret\"]   = PLAID_SECRET\n",
    "    api_client = ApiClient(config)\n",
    "    client = plaid_api.PlaidApi(api_client)\n",
    "    USE_PLAID_V10 = True\n",
    "    print(\"Plaid SDK: v10+ (plaid_api)\")\n",
    "except Exception as e_v10:\n",
    "    try:\n",
    "        # legacy path\n",
    "        from plaid import Client as LegacyClient\n",
    "        client = LegacyClient(\n",
    "            client_id=PLAID_CLIENT_ID,\n",
    "            secret=PLAID_SECRET,\n",
    "            environment=PLAID_ENV\n",
    "        )\n",
    "        USE_PLAID_V10 = False\n",
    "        print(\"Plaid SDK: legacy Client()\")\n",
    "    except Exception as e_legacy:\n",
    "        raise ImportError(\n",
    "            \"Could not initialize Plaid client. Ensure 'plaid-python' is installed. \"\n",
    "            f\"v10 error: {e_v10}\\nlegacy error: {e_legacy}\"\n",
    "        )\n",
    "\n",
    "# Optional quick probe (set PRECHECK=1 to enable)\n",
    "if os.getenv(\"PRECHECK\", \"0\") == \"1\" and USE_PLAID_V10:\n",
    "    from plaid.model.accounts_get_request import AccountsGetRequest\n",
    "    from plaid.api_client import ApiException\n",
    "    for issuer, tok in ACCESS_TOKENS.items():\n",
    "        try:\n",
    "            n = len(client.accounts_get(AccountsGetRequest(access_token=tok)).to_dict().get(\"accounts\", []))\n",
    "            print(f\"{issuer}: ✅ accounts_get OK ({n} accounts)\")\n",
    "        except ApiException as e:\n",
    "            print(f\"{issuer}: ❌ API {e.status} -> {getattr(e, 'body', e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd6aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SYNC Discover (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Petal (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Silver State Schools Credit Union (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 154\u001b[0m\n\u001b[0;32m    151\u001b[0m     combined \u001b[38;5;241m=\u001b[39m combined[\u001b[38;5;241m~\u001b[39mcombined[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39misin({\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_removed_ids})]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Append adds/mods, then dedupe by txn_key\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m new_mod \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m all_added_mod \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m all_added_mod \u001b[38;5;28;01melse\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_mod\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    156\u001b[0m     union_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(combined\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39munion(new_mod\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "File \u001b[1;32mc:\\Users\\kosis\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\kosis\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\kosis\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Pull & CONSOLIDATE (growing latest.csv, no rolling window) ---\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta  # <-- needed\n",
    "import hashlib                         # <-- needed for txn_uid/txn_key\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For GET fallback only — doesn't affect growth because we always union with prev\n",
    "DAYS_BACK = int(os.getenv(\"DAYS_BACK\", \"730\"))\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "CURSORS_PATH = STATE_DIR / \"plaid_cursors.json\"\n",
    "\n",
    "def load_cursors() -> dict:\n",
    "    if CURSORS_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CURSORS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def save_cursors(cur: dict):\n",
    "    CURSORS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    CURSORS_PATH.write_text(json.dumps(cur, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def normalize_category(x):\n",
    "    return \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "def ensure_txn_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    if \"amount\" in df.columns:\n",
    "        df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    # Fallback UID if not present\n",
    "    if \"txn_uid\" not in df.columns:\n",
    "        def _mk(row):\n",
    "            key = f\"{row.get('date')}_{row.get('name')}_{row.get('merchant_name')}_{row.get('amount')}_{row.get('bank_name')}\"\n",
    "            return hashlib.sha1(str(key).encode(\"utf-8\")).hexdigest()\n",
    "        df[\"txn_uid\"] = df.apply(_mk, axis=1)\n",
    "\n",
    "    # Normalize transaction_id and build stable key\n",
    "    if \"transaction_id\" not in df.columns:\n",
    "        df[\"transaction_id\"] = pd.Series([pd.NA] * len(df))\n",
    "    else:\n",
    "        df[\"transaction_id\"] = df[\"transaction_id\"].replace(\"\", pd.NA)\n",
    "\n",
    "    df[\"txn_key\"] = df[\"transaction_id\"].where(df[\"transaction_id\"].notna(), df[\"txn_uid\"]).astype(str)\n",
    "    return df\n",
    "\n",
    "# Convert raw txn dicts to our normalized schema\n",
    "def df_from_txns(txns: list[dict], bank_name: str) -> pd.DataFrame:\n",
    "    if not txns:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(txns)\n",
    "\n",
    "    expected_cols = [\n",
    "        \"name\",\"merchant_name\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"category\",\"date\",\"amount\"\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df[\"category\"] = df[\"category\"].apply(normalize_category)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"bank_name\"] = bank_name\n",
    "    df[\"card_name\"] = bank_name\n",
    "\n",
    "    keep_cols = [\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\"\n",
    "    ]\n",
    "    return df[[c for c in keep_cols if c in df.columns]].copy()\n",
    "\n",
    "# Load previous latest.csv (the growing archive)\n",
    "latest_csv_path = repo_root / \"data\" / \"raw\" / \"latest.csv\"\n",
    "prev = pd.DataFrame()\n",
    "if latest_csv_path.exists():\n",
    "    try:\n",
    "        prev = pd.read_csv(latest_csv_path)\n",
    "        for c in [\"transaction_id\",\"txn_uid\",\"txn_key\"]:\n",
    "            if c not in prev.columns:\n",
    "                prev[c] = pd.Series(dtype=object)\n",
    "        prev = ensure_txn_keys(prev)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read previous latest.csv: {e}\")\n",
    "\n",
    "cursors = load_cursors()\n",
    "all_added_mod = []\n",
    "all_removed_ids = set()\n",
    "\n",
    "use_sync = ('USE_PLAID_V10' in globals() and USE_PLAID_V10)\n",
    "\n",
    "if use_sync:\n",
    "    # --- Sync-first path using Plaid v10 client (deltas) ---\n",
    "    from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "\n",
    "    def sync_one(bank_name: str, access_token: str, cursor: str | None):\n",
    "        added, modified, removed_ids = [], [], []\n",
    "        next_cursor = cursor\n",
    "        while True:\n",
    "            req_kwargs = {\"access_token\": access_token, \"count\": 500}\n",
    "            if isinstance(next_cursor, str) and next_cursor:\n",
    "                req_kwargs[\"cursor\"] = next_cursor\n",
    "            req = TransactionsSyncRequest(**req_kwargs)\n",
    "\n",
    "            resp = client.transactions_sync(req).to_dict()\n",
    "            added.extend(resp.get(\"added\", []) or [])\n",
    "            modified.extend(resp.get(\"modified\", []) or [])\n",
    "\n",
    "            rem = resp.get(\"removed\", []) or []\n",
    "            for r in rem:\n",
    "                if isinstance(r, dict):\n",
    "                    rid = r.get(\"transaction_id\")\n",
    "                    if rid: removed_ids.append(rid)\n",
    "                elif isinstance(r, str):\n",
    "                    removed_ids.append(r)\n",
    "\n",
    "            next_cursor = resp.get(\"next_cursor\", next_cursor)\n",
    "            if not resp.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "        return (next_cursor if isinstance(next_cursor, str) and next_cursor else None,\n",
    "                added, modified, removed_ids)\n",
    "\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 SYNC {bank_name} (start: {'cursor-present' if cursors.get(token) else 'no-cursor'})\")\n",
    "        cur0 = cursors.get(token)\n",
    "        next_cur, added, modified, removed_ids = sync_one(bank_name, token, cur0)\n",
    "\n",
    "        df_add = df_from_txns(added, bank_name)\n",
    "        df_mod = df_from_txns(modified, bank_name)\n",
    "        all_added_mod.extend([df_add, df_mod])\n",
    "        all_removed_ids.update(removed_ids)\n",
    "\n",
    "        cursors[token] = next_cur\n",
    "        print(f\"   → added={len(df_add):,}, modified={len(df_mod):,}, removed={len(removed_ids):,}, next_cursor={'set' if next_cur else 'None'}\")\n",
    "\n",
    "    # Start from previous archive and apply deltas\n",
    "    combined = prev.copy()\n",
    "\n",
    "    # Remove deleted ids if present\n",
    "    if not combined.empty and all_removed_ids and \"transaction_id\" in combined.columns:\n",
    "        combined = combined[~combined[\"transaction_id\"].astype(str).isin({str(x) for x in all_removed_ids})]\n",
    "\n",
    "    # Append adds/mods, then dedupe by txn_key\n",
    "    new_mod = pd.concat([df for df in all_added_mod if df is not None and not df.empty], ignore_index=True) if all_added_mod else pd.DataFrame()\n",
    "    if not new_mod.empty:\n",
    "        union_cols = sorted(set(combined.columns).union(new_mod.columns))\n",
    "        combined = combined.reindex(columns=union_cols)\n",
    "        new_mod = new_mod.reindex(columns=union_cols)\n",
    "        combined = pd.concat([combined, new_mod], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    # --- GET fallback (windowed fetch), then UNION with previous archive ---\n",
    "    def fetch_transactions_get(bank_name: str, access_token: str) -> pd.DataFrame:\n",
    "        txns = []\n",
    "        offset = 0\n",
    "        while True:\n",
    "            resp = client.Transactions.get(\n",
    "                access_token=access_token,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                options={\"count\": 500, \"offset\": offset}\n",
    "            )\n",
    "            total = resp[\"total_transactions\"]\n",
    "            txns.extend(resp[\"transactions\"])\n",
    "            if len(txns) >= total:\n",
    "                break\n",
    "            offset = len(txns)\n",
    "            if offset > 200_000:\n",
    "                raise RuntimeError(f\"Pagination runaway for {bank_name}\")\n",
    "        return df_from_txns(txns, bank_name)\n",
    "\n",
    "    frames = []\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 GET {bank_name} ({start_date} → {end_date})…\")\n",
    "        frames.append(fetch_transactions_get(bank_name, token))\n",
    "    fetched = pd.concat([f for f in frames if f is not None and not f.empty], ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "    # UNION with previous archive (no trimming)\n",
    "    union_cols = sorted(set(prev.columns).union(fetched.columns))\n",
    "    prev_u = prev.reindex(columns=union_cols)\n",
    "    fetched_u = fetched.reindex(columns=union_cols)\n",
    "    combined = pd.concat([prev_u, fetched_u], ignore_index=True)\n",
    "\n",
    "# --- Final: ensure keys, dedupe by txn_key, sort by date (DESC), ready for save in later cell ---\n",
    "combined = ensure_txn_keys(combined)\n",
    "if not combined.empty:\n",
    "    # Keep most recent per txn_key (modified rows win)\n",
    "    if \"date\" in combined.columns:\n",
    "        combined = combined.sort_values(\"date\").drop_duplicates(subset=[\"txn_key\"], keep=\"last\")\n",
    "        combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        combined = combined.drop_duplicates(subset=[\"txn_key\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Persist updated cursor state (safe to keep)\n",
    "try:\n",
    "    save_cursors(cursors)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save cursors: {e}\")\n",
    "\n",
    "print(f\"✅ Consolidated {'SYNC' if use_sync else 'GET'} → rows={len(combined):,} across {len(ACCESS_TOKENS)} bank(s)\")\n",
    "if not combined.empty and \"date\" in combined.columns:\n",
    "    print(f\"Dates in latest.csv (post-merge): {str(pd.to_datetime(combined['date']).min())[:10]} → {str(pd.to_datetime(combined['date']).max())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Clean -> normalize schema ---\n",
    "if combined.empty:\n",
    "    # Create an empty but well-typed frame to keep Power BI stable\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"bank_name\",\"card_name\"\n",
    "    ])\n",
    "\n",
    "# Normalize category: Plaid sometimes returns list; make it a short string\n",
    "if \"category\" in combined.columns:\n",
    "    combined[\"category\"] = combined[\"category\"].apply(\n",
    "        lambda x: \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "    )\n",
    "\n",
    "# Ensure date type\n",
    "combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only expected columns (but don’t error if some are missing)\n",
    "keep_cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "    \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\"\n",
    "]\n",
    "combined = combined[[c for c in keep_cols if c in combined.columns]].copy()\n",
    "\n",
    "# Sort newest first\n",
    "combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Fill minimal NA for downstream friendliness\n",
    "for c in [\"name\",\"merchant_name\",\"category\",\"payment_channel\",\"bank_name\",\"card_name\"]:\n",
    "    if c in combined.columns:\n",
    "        combined[c] = combined[c].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: YAML helpers (merchant key, mapping, non-spend) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    s = (name or \"\").upper()\n",
    "    s = re.sub(r\"APPLE PAY ENDING IN \\d{4}\", \"\", s)\n",
    "    s = re.sub(r\"#\\d{2,}\", \"\", s)              # strip store numbers like #1234\n",
    "    s = re.sub(r\"\\d+\", \"\", s)                  # kill stray digits\n",
    "    s = re.sub(r\"[^A-Z&\\s]\", \" \", s)           # keep letters, ampersand, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def apply_yaml_mapping(df: pd.DataFrame, ymap: dict) -> pd.DataFrame:\n",
    "    if not ymap or df.empty:\n",
    "        # Still add standard columns so schema is stable\n",
    "        out = df.copy()\n",
    "        for c in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\"confidence_final\",\"source_final\"]:\n",
    "            if c not in out.columns:\n",
    "                out[c] = None\n",
    "        out[\"source_final\"] = out[\"source_final\"].fillna(\"raw\")\n",
    "        out[\"confidence_final\"] = out[\"confidence_final\"].fillna(\"raw\")\n",
    "        return out\n",
    "\n",
    "    look = {k.upper(): v for k, v in ymap.items()}\n",
    "    rows = []\n",
    "    # (kept for clarity; merge-based vectorization is overkill at this size)\n",
    "    for _, r in df.iterrows():\n",
    "        mk = r.get(\"merchant_key\", \"\")\n",
    "        m = look.get(mk, {})\n",
    "        rows.append({\n",
    "            **r,\n",
    "            \"display_name_final\": m.get(\"display_name\", r.get(\"merchant_name\") or r.get(\"name\")),\n",
    "            \"category_final\":     m.get(\"category\"),\n",
    "            \"subcategory_final\":  m.get(\"subcategory\"),\n",
    "            \"tags_final\":         \",\".join(m.get(\"tags\", [])) if isinstance(m.get(\"tags\", []), (list, tuple)) else m.get(\"tags\"),\n",
    "            \"confidence_final\":   m.get(\"confidence\", \"map\"),\n",
    "            \"source_final\":       \"yaml\" if m else \"raw\"\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def mark_non_spend_flows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    pats = [\n",
    "        r\"PAYMENT\", r\"TRANSFER\", r\"ACH\", r\"ZELLE\", r\"DIRECTPAY\", r\"CREDIT\",\n",
    "        r\"REFUND\", r\"REIMBURSE\", r\"ADJUSTMENT\", r\"REVERSAL\"\n",
    "    ]\n",
    "    pat = re.compile(\"|\".join(pats))\n",
    "    names = (df.get(\"name\", pd.Series(\"\", index=df.index)).fillna(\"\") + \" \" +\n",
    "             df.get(\"merchant_name\", pd.Series(\"\", index=df.index)).fillna(\"\")).str.upper()\n",
    "    df = df.copy()\n",
    "    df[\"is_non_spend_flow\"] = names.str.contains(pat)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Optional YAML enrichment, then finalize columns ---\n",
    "# Build a robust merchant key\n",
    "combined[\"merchant_key\"] = combined[\"merchant_name\"].where(\n",
    "    combined[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    combined[\"name\"]\n",
    ").map(merchant_key_from)\n",
    "\n",
    "# Load YAML map if exists (use repo_root)\n",
    "PATH_YAML = (repo_root / \"config\" / \"categories.yaml\")\n",
    "ymap = {}\n",
    "if PATH_YAML.exists():\n",
    "    with open(PATH_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "        ymap = yaml.safe_load(f) or {}\n",
    "\n",
    "# Apply mapping + mark non-spend flows\n",
    "enriched = apply_yaml_mapping(combined, ymap)\n",
    "enriched = mark_non_spend_flows(enriched)\n",
    "\n",
    "# ⚠️ FIXED: missing comma in your original list between card_name and display_name_final\n",
    "cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"merchant_key\",\"category\",\"amount\",\n",
    "    \"bank_name\",\"card_name\",\n",
    "    \"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\n",
    "    \"is_non_spend_flow\",\"confidence_final\",\"source_final\"\n",
    "]\n",
    "for c in cols:\n",
    "    if c not in enriched.columns:\n",
    "        enriched[c] = None\n",
    "enriched = enriched[cols].copy()\n",
    "\n",
    "# Keep dates as date (or datetime) for Power BI\n",
    "enriched[\"date\"] = pd.to_datetime(enriched[\"date\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest CSV saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv  rows=294\n",
      "\n",
      "Preview (top 10):\n",
      "      date                                                                                                                            name merchant_name                                                                             merchant_key      category  amount                         bank_name                         card_name                                                                                                              display_name_final category_final subcategory_final tags_final  is_non_spend_flow confidence_final source_final\n",
      "2025-09-10                                                                                                                 Microsoft Store     Microsoft                                                                                MICROSOFT Subscriptions    1.00                          Discover                          Discover                                                                                                                       Microsoft           None              None                         False              map          raw\n",
      "2025-09-10                                                                                              POS Signature Purchase using Token                                                                     POS SIGNATURE PURCHASE USING TOKEN                 18.61                             Petal                             Petal                                                                                              POS Signature Purchase using Token           None              None                         False              map          raw\n",
      "2025-09-10                                                                                                                 Microsoft Store     Microsoft                                                                                MICROSOFT Subscriptions   -1.00                          Discover                          Discover                                                                                                                       Microsoft           None              None                         False              map          raw\n",
      "2025-09-10                                                                                              POS Signature Purchase using Token                                                                     POS SIGNATURE PURCHASE USING TOKEN          Misc   18.61                             Petal                             Petal                                                                                              POS Signature Purchase using Token           None              None                         False              map          raw\n",
      "2025-09-10                                                                                                                 Microsoft Store     Microsoft                                                                                MICROSOFT                 -1.00                          Discover                          Discover                                                                                                                       Microsoft           None              None                         False              map          raw\n",
      "2025-09-10                                                                                                                 Microsoft Store     Microsoft                                                                                MICROSOFT                  1.00                          Discover                          Discover                                                                                                                       Microsoft           None              None                         False              map          raw\n",
      "2025-09-09                                                                                              POS Signature Purchase using Token                                                                     POS SIGNATURE PURCHASE USING TOKEN                 16.49                             Petal                             Petal                                                                                              POS Signature Purchase using Token           None              None                         False              map          raw\n",
      "2025-09-09 Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953               WITHDRAWAL ALLY TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ACH ECC WEB ACH TRACE                504.22 Silver State Schools Credit Union Silver State Schools Credit Union Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953           None              None                          True              map          raw\n",
      "2025-09-09 Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953               WITHDRAWAL ALLY TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ACH ECC WEB ACH TRACE     Transfers  504.22 Silver State Schools Credit Union Silver State Schools Credit Union Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953           None              None                          True              map          raw\n",
      "2025-09-09                                                                                              POS Signature Purchase using Token                                                                     POS SIGNATURE PURCHASE USING TOKEN          Misc   16.49                             Petal                             Petal                                                                                              POS Signature Purchase using Token           None              None                         False              map          raw\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Write latest.csv + preview ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "latest_path = OUTPUT_DIR / \"latest.csv\"\n",
    "\n",
    "# Write enriched directly (so Power BI gets the good stuff)\n",
    "enriched.to_csv(latest_path, index=False)\n",
    "\n",
    "# Sanity\n",
    "assert latest_path.exists(), \"latest.csv was not written.\"\n",
    "assert \"bank_name\" in enriched.columns, \"bank_name column missing.\"\n",
    "assert \"card_name\" in enriched.columns, \"card_name column missing.\"\n",
    "\n",
    "print(f\"✅ Latest CSV saved → {latest_path}  rows={len(enriched):,}\")\n",
    "try:\n",
    "    print(\"\\nPreview (top 10):\")\n",
    "    print(enriched.head(10).to_string(index=False))\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
