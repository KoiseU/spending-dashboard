{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d990ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK â†’ PLAID_CLIENT_ID: 68bbâ€¦6689 | PLAID_SECRET: a605â€¦7df5 | PLAID_ENV: production | OUTPUT_DIR: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw | TOKENS_PATH: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\.state\\access_tokens.json\n",
      "Loaded 3 token(s).\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb â€” Cell 1: Env, paths, .env loader, tokens (original) ---\n",
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Optional dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "    find_dotenv = None\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    return (s[:4] + \"â€¦\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# Resolve repo root (works from / or /scripts)\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next((p for p in [cwd, *cwd.parents] if (p / \".git\").exists() or p.name == \"spending-dashboard\"), cwd)\n",
    "\n",
    "# âœ… CI-safe override: prefer GitHub workspace path if present\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "if gw:\n",
    "    repo_root = Path(gw).resolve()\n",
    "\n",
    "# Load .env if present (scripts/.env preferred)\n",
    "def load_envs():\n",
    "    if load_dotenv is None:\n",
    "        return\n",
    "    abs_override = os.getenv(\"ENV_PATH\", str(repo_root / \"scripts\" / \".env\"))\n",
    "    if abs_override and Path(abs_override).exists():\n",
    "        try:\n",
    "            load_dotenv(abs_override, override=False, encoding=\"utf-8\")\n",
    "        except TypeError:\n",
    "            load_dotenv(abs_override, override=False)\n",
    "    for p in [\n",
    "        repo_root / \"scripts\" / \".env\",\n",
    "        repo_root / \".env\",\n",
    "        repo_root / \"config\" / \".env\",\n",
    "        cwd / \".env\",\n",
    "    ]:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                load_dotenv(str(p), override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(str(p), override=False)\n",
    "    if find_dotenv:\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        if found:\n",
    "            try:\n",
    "                load_dotenv(found, override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(found, override=False)\n",
    "\n",
    "load_envs()\n",
    "\n",
    "# Normalize env\n",
    "PLAID_CLIENT_ID = os.getenv(\"PLAID_CLIENT_ID\")\n",
    "PLAID_SECRET    = os.getenv(\"PLAID_SECRET\")\n",
    "PLAID_ENV       = (os.getenv(\"PLAID_ENV\", \"production\") or \"production\").strip().lower()\n",
    "alias = {\"prod\":\"production\",\"live\":\"production\",\"dev\":\"development\",\"devel\":\"development\",\"sb\":\"sandbox\"}\n",
    "PLAID_ENV = alias.get(PLAID_ENV, PLAID_ENV)\n",
    "if PLAID_ENV not in {\"production\",\"development\",\"sandbox\"}:\n",
    "    PLAID_ENV = \"production\"\n",
    "\n",
    "# Paths (env-overridable)\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", str(repo_root / \"data\" / \"raw\")))\n",
    "STATE_DIR  = Path(os.getenv(\"STATE_DIR\",  str(repo_root / \".state\")))\n",
    "TOKENS_PATH = Path(os.getenv(\"TOKENS_PATH\", str(STATE_DIR / \"access_tokens.json\")))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load & validate access tokens (env > file), canonical-only ---\n",
    "def _strip_bom(s: str) -> str:\n",
    "    return s.lstrip(\"\\ufeff\") if isinstance(s, str) else s\n",
    "\n",
    "def _parse_pairs_blob(blob: str) -> dict:\n",
    "    raw = [p.strip() for sep in [\"\\n\",\";\",\"|\",\",\"] for p in (blob.split(sep) if sep in blob else []) if p.strip()]\n",
    "    if not raw: raw = [blob.strip()]\n",
    "    out = {}\n",
    "    for p in raw:\n",
    "        if \"=\" in p:\n",
    "            k, v = p.split(\"=\", 1)\n",
    "        elif \":\" in p:\n",
    "            k, v = p.split(\":\", 1)\n",
    "        else:\n",
    "            continue\n",
    "        k = k.strip().strip('\"').strip(\"'\")\n",
    "        v = v.strip().strip('\"').strip(\"'\")\n",
    "        if k and v:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def _normalize_tokens(obj) -> dict:\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): str(v).strip() for k,v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        out = {}\n",
    "        for item in obj:\n",
    "            if isinstance(item, dict):\n",
    "                name = item.get(\"issuer\") or item.get(\"bank\") or item.get(\"name\")\n",
    "                token = item.get(\"access_token\") or item.get(\"token\")\n",
    "                if name and token:\n",
    "                    out[str(name)] = str(token).strip()\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        s = _strip_bom(obj).strip()\n",
    "        try:\n",
    "            parsed = json.loads(s)    # JSON first\n",
    "            return _normalize_tokens(parsed)\n",
    "        except Exception:\n",
    "            return _parse_pairs_blob(s)\n",
    "    return {}\n",
    "\n",
    "def load_access_tokens():\n",
    "    blob = os.getenv(\"PLAID_ACCESS_TOKENS\", \"\").strip()\n",
    "    if blob:\n",
    "        tokens = _normalize_tokens(blob)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    if TOKENS_PATH.exists():\n",
    "        raw = TOKENS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        tokens = _normalize_tokens(raw)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    raise AssertionError(\n",
    "        f\"Could not load access tokens. Provide PLAID_ACCESS_TOKENS env or a valid JSON at {TOKENS_PATH}.\"\n",
    "    )\n",
    "\n",
    "ACCESS_TOKENS = load_access_tokens()\n",
    "\n",
    "PAT = re.compile(r\"^access-(?:production|development|sandbox)-[a-z0-9\\-]+$\")\n",
    "expected_prefix = f\"access-{PLAID_ENV}-\"\n",
    "bad = [k for k,v in ACCESS_TOKENS.items() if not isinstance(v, str) or not v.startswith(expected_prefix) or not PAT.match(v)]\n",
    "assert not bad, f\"Non-canonical or wrong-env tokens for: {bad}. Ensure tokens look like '{expected_prefix}â€¦' (no '/', '+', '=').\"\n",
    "\n",
    "print(\n",
    "    \"Env OK â†’\",\n",
    "    \"PLAID_CLIENT_ID:\", mask(PLAID_CLIENT_ID),\n",
    "    \"| PLAID_SECRET:\", mask(PLAID_SECRET),\n",
    "    \"| PLAID_ENV:\", PLAID_ENV,\n",
    "    \"| OUTPUT_DIR:\", str(OUTPUT_DIR),\n",
    "    \"| TOKENS_PATH:\", str(TOKENS_PATH),\n",
    ")\n",
    "print(f\"Loaded {len(ACCESS_TOKENS)} token(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a22fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaid SDK: v10+ (plaid_api)\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb â€” Cell 2: Plaid client init (original) ---\n",
    "USE_PLAID_V10 = False\n",
    "client = None\n",
    "\n",
    "try:\n",
    "    # v10+ path\n",
    "    from plaid.api import plaid_api\n",
    "    from plaid.configuration import Configuration\n",
    "    try:\n",
    "        from plaid.configuration import Environment  # newer enum\n",
    "        env_host = {\n",
    "            \"production\":  Environment.Production,\n",
    "            \"development\": Environment.Development,\n",
    "            \"sandbox\":     Environment.Sandbox,\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=env_host)\n",
    "    except Exception:\n",
    "        # fallback if Environment enum not present\n",
    "        host_url = {\n",
    "            \"production\":  \"https://production.plaid.com\",\n",
    "            \"development\": \"https://development.plaid.com\",\n",
    "            \"sandbox\":     \"https://sandbox.plaid.com\",\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=host_url)\n",
    "\n",
    "    from plaid.api_client import ApiClient\n",
    "    config.api_key[\"clientId\"] = PLAID_CLIENT_ID\n",
    "    config.api_key[\"secret\"]   = PLAID_SECRET\n",
    "    api_client = ApiClient(config)\n",
    "    client = plaid_api.PlaidApi(api_client)\n",
    "    USE_PLAID_V10 = True\n",
    "    print(\"Plaid SDK: v10+ (plaid_api)\")\n",
    "except Exception as e_v10:\n",
    "    try:\n",
    "        # legacy path\n",
    "        from plaid import Client as LegacyClient\n",
    "        client = LegacyClient(\n",
    "            client_id=PLAID_CLIENT_ID,\n",
    "            secret=PLAID_SECRET,\n",
    "            environment=PLAID_ENV\n",
    "        )\n",
    "        USE_PLAID_V10 = False\n",
    "        print(\"Plaid SDK: legacy Client()\")\n",
    "    except Exception as e_legacy:\n",
    "        raise ImportError(\n",
    "            \"Could not initialize Plaid client. Ensure 'plaid-python' is installed. \"\n",
    "            f\"v10 error: {e_v10}\\nlegacy error: {e_legacy}\"\n",
    "        )\n",
    "\n",
    "# --- Expanded PRECHECK: list accounts per token when PRECHECK=1 ---\n",
    "if os.getenv(\"PRECHECK\", \"0\") == \"1\" and USE_PLAID_V10:\n",
    "    from plaid.model.accounts_get_request import AccountsGetRequest\n",
    "    from plaid.api_client import ApiException\n",
    "    print(\"\\n[PRECHECK] Listing accounts per token:\")\n",
    "    for issuer, tok in ACCESS_TOKENS.items():\n",
    "        try:\n",
    "            acc_resp = client.accounts_get(AccountsGetRequest(access_token=tok)).to_dict()\n",
    "            accounts = acc_resp.get(\"accounts\", []) or []\n",
    "            print(f\"- {issuer}: {len(accounts)} account(s)\")\n",
    "            for a in accounts:\n",
    "                name = a.get(\"name\") or a.get(\"official_name\") or \"\"\n",
    "                mask = a.get(\"mask\") or \"\"\n",
    "                subtype = (a.get(\"subtype\") or \"\").upper()\n",
    "                a_id = a.get(\"account_id\")\n",
    "                print(f\"    â€¢ {name}  (subtype={subtype}, mask={mask}, id={a_id})\")\n",
    "        except ApiException as e:\n",
    "            print(f\"- {issuer}: âŒ API {e.status} -> {getattr(e, 'body', e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dd6aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ SYNC Discover (start: no-cursor)\n",
      "   [sync diag] PFC in added: 28 / 28, modified: 0 / 0\n",
      "   â†’ added=28, modified=0, removed=0, next_cursor=set\n",
      "ðŸ”„ SYNC Petal (start: no-cursor)\n",
      "   [sync diag] PFC in added: 32 / 32, modified: 0 / 0\n",
      "   â†’ added=32, modified=0, removed=0, next_cursor=set\n",
      "ðŸ”„ SYNC Silver State Schools Credit Union (start: no-cursor)\n",
      "   [sync diag] PFC in added: 92 / 92, modified: 0 / 0\n",
      "   â†’ added=92, modified=0, removed=0, next_cursor=set\n",
      "âœ… Consolidated using SYNC â†’ rows=153 across 3 bank(s)\n",
      "Window: 2025-03-19 â†’ 2025-09-15 | DAYS_BACK=180\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb â€” Cell 3 (UPDATED): Pull & consolidate via /transactions/sync ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Rolling window (override with DAYS_BACK env if you like)\n",
    "DAYS_BACK = int(os.getenv(\"DAYS_BACK\", \"180\"))\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "CURSORS_PATH = STATE_DIR / \"plaid_cursors.json\"\n",
    "\n",
    "def load_cursors() -> dict:\n",
    "    if CURSORS_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CURSORS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def save_cursors(cur: dict):\n",
    "    CURSORS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    CURSORS_PATH.write_text(json.dumps(cur, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def normalize_category(x):\n",
    "    return \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "# Convert raw txn dicts to our normalized schema (KEEP PFC)\n",
    "def df_from_txns(txns: list[dict], bank_name: str) -> pd.DataFrame:\n",
    "    if not txns:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(txns)\n",
    "\n",
    "    # Ensure expected columns exist (include PFC so we can map it later)\n",
    "    expected_cols = [\n",
    "        \"name\",\"merchant_name\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"category\",\"date\",\"amount\",\n",
    "        \"personal_finance_category\",\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Normalize\n",
    "    df[\"category\"] = df[\"category\"].apply(normalize_category)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    # Bank & card\n",
    "    df[\"bank_name\"] = bank_name\n",
    "    df[\"card_name\"] = bank_name  # upgrade later via accounts_dim if desired\n",
    "\n",
    "    keep_cols = [\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\",\"personal_finance_category\"\n",
    "    ]\n",
    "    return df[[c for c in keep_cols if c in df.columns]].copy()\n",
    "\n",
    "# Read previous latest.csv (acts as our cache for SYNC deltas)\n",
    "latest_csv_path = (repo_root / \"data\" / \"raw\" / \"latest.csv\")\n",
    "prev = pd.DataFrame()\n",
    "if latest_csv_path.exists():\n",
    "    try:\n",
    "        prev = pd.read_csv(latest_csv_path)\n",
    "        prev[\"date\"] = pd.to_datetime(prev[\"date\"], errors=\"coerce\")\n",
    "        if \"transaction_id\" not in prev.columns:\n",
    "            prev[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not read previous latest.csv: {e}\")\n",
    "\n",
    "# Optional one-time full resync (set FORCE_FULL_RESYNC=1 in env)\n",
    "if (os.getenv(\"FORCE_FULL_RESYNC\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n",
    "    try:\n",
    "        if CURSORS_PATH.exists():\n",
    "            CURSORS_PATH.unlink()\n",
    "            print(\"âš ï¸ Reset: deleted .state/plaid_cursors.json to force a full resync.\")\n",
    "    except Exception as e:\n",
    "        print(\"Reset warning:\", e)\n",
    "    cursors = {}\n",
    "    prev = pd.DataFrame()\n",
    "else:\n",
    "    cursors = load_cursors()\n",
    "\n",
    "all_added_mod = []   # list of DataFrames of added/modified across banks\n",
    "all_removed_ids = set()\n",
    "\n",
    "if 'USE_PLAID_V10' in globals() and USE_PLAID_V10:\n",
    "    # --- Sync-first path using Plaid v10 client (simple + robust) ---\n",
    "    from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "\n",
    "    def sync_one(bank_name: str, access_token: str, cursor: str | None):\n",
    "        \"\"\"\n",
    "        Call /transactions/sync until has_more is False.\n",
    "        Explicitly requests Personal Finance Category (PFC) on every page.\n",
    "        Returns: (next_cursor, added, modified, removed_ids)\n",
    "        \"\"\"\n",
    "        added, modified, removed_ids = [], [], []\n",
    "        next_cursor = cursor\n",
    "\n",
    "        while True:\n",
    "            req_kwargs = {\"access_token\": access_token, \"count\": 500}\n",
    "            if isinstance(next_cursor, str) and next_cursor:\n",
    "                req_kwargs[\"cursor\"] = next_cursor\n",
    "\n",
    "            # Ask Plaid to include PFC (and original description for debugging)\n",
    "            try:\n",
    "                from plaid.model.transactions_sync_request_options import TransactionsSyncRequestOptions\n",
    "                req_kwargs[\"options\"] = TransactionsSyncRequestOptions(\n",
    "                    include_personal_finance_category=True,\n",
    "                    include_original_description=True\n",
    "                )\n",
    "            except Exception:\n",
    "                # Older SDKs may not expose the Options model; skip silently.\n",
    "                pass\n",
    "\n",
    "            req = TransactionsSyncRequest(**req_kwargs)\n",
    "            resp = client.transactions_sync(req).to_dict()\n",
    "\n",
    "            added.extend(resp.get(\"added\", []) or [])\n",
    "            modified.extend(resp.get(\"modified\", []) or [])\n",
    "\n",
    "            # removed may be list[str] or list[dict]\n",
    "            rem = resp.get(\"removed\", []) or []\n",
    "            for r in rem:\n",
    "                if isinstance(r, dict):\n",
    "                    rid = r.get(\"transaction_id\")\n",
    "                    if rid: removed_ids.append(rid)\n",
    "                elif isinstance(r, str):\n",
    "                    removed_ids.append(r)\n",
    "\n",
    "            next_cursor = resp.get(\"next_cursor\", next_cursor)\n",
    "            if not resp.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "        # Tiny diag: count how many returned txns actually have PFC payloads\n",
    "        def _pfc_count(lst):\n",
    "            c = 0\n",
    "            for t in lst:\n",
    "                pfc = t.get(\"personal_finance_category\")\n",
    "                if isinstance(pfc, dict) and (pfc.get(\"primary\") or pfc.get(\"detailed\")):\n",
    "                    c += 1\n",
    "            return c\n",
    "\n",
    "        print(f\"   [sync diag] PFC in added: {_pfc_count(added)} / {len(added)}, \"\n",
    "              f\"modified: {_pfc_count(modified)} / {len(modified)}\")\n",
    "\n",
    "        return (next_cursor if isinstance(next_cursor, str) and next_cursor else None,\n",
    "                added, modified, removed_ids)\n",
    "\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"ðŸ”„ SYNC {bank_name} (start: {'cursor-present' if cursors.get(token) else 'no-cursor'})\")\n",
    "        cur0 = cursors.get(token)\n",
    "        next_cur, added, modified, removed_ids = sync_one(bank_name, token, cur0)\n",
    "\n",
    "        df_add = df_from_txns(added, bank_name)\n",
    "        df_mod = df_from_txns(modified, bank_name)\n",
    "        all_added_mod.append(df_add)\n",
    "        all_added_mod.append(df_mod)\n",
    "        all_removed_ids.update(removed_ids)\n",
    "\n",
    "        cursors[token] = next_cur\n",
    "        print(f\"   â†’ added={len(df_add):,}, modified={len(df_mod):,}, removed={len(removed_ids):,}, next_cursor={'set' if next_cur else 'None'}\")\n",
    "\n",
    "    # Start from previous CSV and apply deltas\n",
    "    cur = prev.copy()\n",
    "\n",
    "    # Remove deleted transaction_ids (guard if column is present)\n",
    "    if not cur.empty and all_removed_ids and \"transaction_id\" in cur.columns:\n",
    "        cur = cur[~cur[\"transaction_id\"].astype(str).isin({str(x) for x in all_removed_ids})]\n",
    "    elif not cur.empty and all_removed_ids:\n",
    "        print(\"â„¹ï¸ Skipping delete-apply: previous cache lacks 'transaction_id' column.\")\n",
    "\n",
    "    # Replace modified ids and add new ones\n",
    "    if any(len(x) for x in all_added_mod):\n",
    "        new_mod = (pd.concat([df for df in all_added_mod if not df.empty], ignore_index=True)\n",
    "                   if all_added_mod else pd.DataFrame())\n",
    "        if not new_mod.empty and \"transaction_id\" not in new_mod.columns:\n",
    "            new_mod[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "\n",
    "        if not cur.empty and \"transaction_id\" in cur.columns and \"transaction_id\" in new_mod.columns:\n",
    "            mod_ids = set(new_mod[\"transaction_id\"].dropna().astype(str).tolist())\n",
    "            if mod_ids:\n",
    "                cur = cur[~cur[\"transaction_id\"].astype(str).isin(mod_ids)]\n",
    "        else:\n",
    "            if not cur.empty and not new_mod.empty:\n",
    "                print(\"â„¹ï¸ Skipping modify-replace: one frame lacks 'transaction_id'; appending only.\")\n",
    "\n",
    "        combined = pd.concat([cur, new_mod], ignore_index=True) if not new_mod.empty else cur.copy()\n",
    "    else:\n",
    "        combined = cur.copy()\n",
    "\n",
    "else:\n",
    "    # --- Fallback: windowed GET per item (legacy client) ---\n",
    "    def fetch_transactions_get(bank_name: str, access_token: str) -> pd.DataFrame:\n",
    "        txns = []\n",
    "        offset = 0\n",
    "        while True:\n",
    "            resp = client.Transactions.get(\n",
    "                access_token=access_token,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                options={\"count\": 500, \"offset\": offset}\n",
    "            )\n",
    "            total = resp[\"total_transactions\"]\n",
    "            txns.extend(resp[\"transactions\"])\n",
    "            if len(txns) >= total:\n",
    "                break\n",
    "            offset = len(txns)\n",
    "            if offset > 50_000:\n",
    "                raise RuntimeError(f\"Pagination runaway for {bank_name}\")\n",
    "        return df_from_txns(txns, bank_name)\n",
    "\n",
    "    frames = []\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"ðŸ”„ GET {bank_name} ({start_date} â†’ {end_date})â€¦\")\n",
    "        frames.append(fetch_transactions_get(bank_name, token))\n",
    "    combined = pd.concat([f for f in frames if f is not None and not f.empty], ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# Finalize for both paths: trim to window, sort, dedupe by best available key\n",
    "if combined is None or combined.empty:\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\",\"personal_finance_category\"\n",
    "    ])\n",
    "else:\n",
    "    combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "    combined = combined[\n",
    "        (combined[\"date\"] >= pd.Timestamp(start_date)) &\n",
    "        (combined[\"date\"] <= pd.Timestamp(end_date))\n",
    "    ]\n",
    "    # Primary dedupe: transaction_id; fallback: (account_id, date, amount, name)\n",
    "    if \"transaction_id\" in combined.columns and combined[\"transaction_id\"].notna().any():\n",
    "        combined = (combined\n",
    "                    .sort_values(\"date\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"transaction_id\"], keep=\"first\")\n",
    "                    .reset_index(drop=True))\n",
    "    else:\n",
    "        subset = [c for c in [\"account_id\",\"date\",\"amount\",\"name\"] if c in combined.columns]\n",
    "        if subset:\n",
    "            combined = (combined\n",
    "                        .sort_values(\"date\", ascending=False)\n",
    "                        .drop_duplicates(subset=subset, keep=\"first\")\n",
    "                        .reset_index(drop=True))\n",
    "        else:\n",
    "            combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Persist cursors (local/CI workspace)\n",
    "try:\n",
    "    save_cursors(cursors if 'cursors' in locals() else {})\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not save cursors: {e}\")\n",
    "\n",
    "print(f\"âœ… Consolidated using {'SYNC' if 'USE_PLAID_V10' in globals() and USE_PLAID_V10 else 'GET'} â†’ rows={len(combined):,} across {len(ACCESS_TOKENS)} bank(s)\")\n",
    "print(f\"Window: {start_date} â†’ {end_date} | DAYS_BACK={DAYS_BACK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6faf63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BUILD] Counters\n",
      "Rows: 153\n",
      "Has PFC primary: 152\n",
      "\n",
      "Top 15 category (post-PFC mapping):\n",
      "category\n",
      "Transfers                45\n",
      "Shopping                 31\n",
      "Debt Payments            29\n",
      "Dining                   17\n",
      "Uncategorized            16\n",
      "Transportation            7\n",
      "Entertainment             4\n",
      "Home Improvement          2\n",
      "Fees                      1\n",
      "Government/Non-Profit     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3.5: Extract PFC and map to friendly base category ---\n",
    "\n",
    "# Ensure the column exists even if missing\n",
    "if \"personal_finance_category\" not in combined.columns:\n",
    "    combined[\"personal_finance_category\"] = None\n",
    "\n",
    "# Extract PFC primary/detailed safely\n",
    "def _pfc_get(x, key):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(key)\n",
    "    # Some SDKs may serialize nested objects as strings; try to parse\n",
    "    if isinstance(x, str) and x.strip().startswith(\"{\"):\n",
    "        try:\n",
    "            d = json.loads(x)\n",
    "            if isinstance(d, dict):\n",
    "                return d.get(key)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "combined[\"category_pfc_primary\"]  = combined[\"personal_finance_category\"].apply(lambda x: _pfc_get(x, \"primary\"))\n",
    "combined[\"category_pfc_detailed\"] = combined[\"personal_finance_category\"].apply(lambda x: _pfc_get(x, \"detailed\"))\n",
    "\n",
    "# Friendly names for PFC primary\n",
    "_pfc_map = {\n",
    "    \"FOOD_AND_DRINK\":\"Dining\",\n",
    "    \"GROCERIES\":\"Groceries\",\n",
    "    \"GENERAL_MERCHANDISE\":\"Shopping\",\n",
    "    \"TRANSPORTATION\":\"Transportation\",\n",
    "    \"TRAVEL\":\"Travel\",\n",
    "    \"HEALTHCARE\":\"Health\",\n",
    "    \"ENTERTAINMENT\":\"Entertainment\",\n",
    "    \"HOME_IMPROVEMENT\":\"Home Improvement\",\n",
    "    \"RENT_AND_UTILITIES\":\"Utilities\",\n",
    "    \"SERVICE\":\"Services\",\n",
    "    \"GOVERNMENT_AND_NON_PROFIT\":\"Government/Non-Profit\",\n",
    "    \"BANK_FEES\":\"Fees\",\n",
    "    \"INCOME\":\"Income\",\n",
    "    \"TRANSFER_OUT\":\"Transfers\",\n",
    "    \"TRANSFER_IN\":\"Transfers\",\n",
    "    \"LOAN_PAYMENTS\":\"Debt Payments\",\n",
    "    \"SUBSCRIPTION\":\"Subscriptions\",\n",
    "    \"RECURRING_SUBSCRIPTIONS\":\"Subscriptions\",\n",
    "}\n",
    "\n",
    "# Preserve legacy for debugging\n",
    "combined[\"category_plaid_legacy\"] = combined.get(\"category\")\n",
    "\n",
    "# Final base 'category' that leaves build:\n",
    "combined[\"category\"] = (\n",
    "    combined.get(\"category_pfc_primary\")\n",
    "      .map(lambda s: _pfc_map.get(str(s).upper()) if pd.notna(s) and str(s).strip() else None)\n",
    "      .fillna(combined.get(\"category_plaid_legacy\"))\n",
    "      .fillna(\"Uncategorized\")\n",
    ")\n",
    "\n",
    "print(\"\\n[BUILD] Counters\")\n",
    "print(\"Rows:\", len(combined))\n",
    "print(\"Has PFC primary:\", int(combined[\"category_pfc_primary\"].notna().sum()))\n",
    "print(\"\\nTop 15 category (post-PFC mapping):\")\n",
    "print(combined[\"category\"].fillna(\"<<NULL>>\").value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03199b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BUILD DIAG v2] Columns present: ['date', 'name', 'merchant_name', 'merchant_key', 'display_name', 'category', 'subcategory', 'tags', 'amount', 'payment_channel', 'pending', 'account_id', 'transaction_id', 'bank_name', 'card_name', 'is_non_spend_flow', 'display_name_final', 'category_final', 'subcategory_final', 'tags_final', 'confidence_final', 'source_final', 'personal_finance_category', 'category_pfc_primary', 'category_pfc_detailed', 'category_plaid_legacy']\n",
      "Has personal_finance_category column: True\n",
      "Sample columns for PFC probe: ['name', 'merchant_name', 'category', 'personal_finance_category']\n",
      "Non-null PFC rows: 152\n",
      "Example PFC dicts (up to 3): [{'confidence_level': 'LOW', 'detailed': 'TRANSFER_IN_INVESTMENT_AND_RETIREMENT_FUNDS', 'primary': 'TRANSFER_IN'}, {'confidence_level': 'HIGH', 'detailed': 'TRANSFER_OUT_ACCOUNT_TRANSFER', 'primary': 'TRANSFER_OUT'}, {'confidence_level': 'HIGH', 'detailed': 'GENERAL_MERCHANDISE_ELECTRONICS', 'primary': 'GENERAL_MERCHANDISE'}]\n",
      "\n",
      "Legacy 'category' top 10 BEFORE any mapping:\n",
      "category\n",
      "Transfers                45\n",
      "Shopping                 31\n",
      "Debt Payments            29\n",
      "Dining                   17\n",
      "Uncategorized            16\n",
      "Transportation            7\n",
      "Entertainment             4\n",
      "Home Improvement          2\n",
      "Fees                      1\n",
      "Government/Non-Profit     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- DIAG: Do we actually have PFC fields in the raw frames? ---\n",
    "print(\"[BUILD DIAG v2] Columns present:\", list(combined.columns))\n",
    "\n",
    "has_pfc_col = \"personal_finance_category\" in combined.columns\n",
    "print(\"Has personal_finance_category column:\", has_pfc_col)\n",
    "\n",
    "# Peek a few raw dicts if we can (best-effort; may be empty if we already collapsed earlier)\n",
    "sample_cols = [c for c in combined.columns if \"personal_finance\" in c.lower() or c in (\"category\",\"name\",\"merchant_name\")]\n",
    "print(\"Sample columns for PFC probe:\", sample_cols[:8])\n",
    "\n",
    "# Try to show any non-null personal_finance_category values if the column exists\n",
    "if has_pfc_col:\n",
    "    nn = combined[\"personal_finance_category\"].dropna()\n",
    "    print(\"Non-null PFC rows:\", int(nn.shape[0]))\n",
    "    if not nn.empty:\n",
    "        print(\"Example PFC dicts (up to 3):\", nn.head(3).tolist())\n",
    "\n",
    "print(\"\\nLegacy 'category' top 10 BEFORE any mapping:\")\n",
    "print(combined.get(\"category\", pd.Series(dtype=object)).fillna(\"<<NULL>>\").value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e7e92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_latest.ipynb â€” Cell 4 ---\n",
    "if combined.empty:\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"bank_name\",\"card_name\"\n",
    "    ])\n",
    "\n",
    "# Normalize category list â†’ string\n",
    "if \"category\" in combined.columns:\n",
    "    combined[\"category\"] = combined[\"category\"].apply(\n",
    "        lambda x: \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "    )\n",
    "\n",
    "# Ensure date type\n",
    "combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only expected columns\n",
    "keep_cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "    \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\"\n",
    "]\n",
    "combined = combined[[c for c in keep_cols if c in combined.columns]].copy()\n",
    "\n",
    "# ðŸ”— Upgrade card_name from accounts_dim.csv if available\n",
    "acc_dim_path = repo_root / \"config\" / \"accounts_dim.csv\"\n",
    "if acc_dim_path.exists() and \"account_id\" in combined.columns:\n",
    "    try:\n",
    "        acc = pd.read_csv(acc_dim_path)\n",
    "        if {\"account_id\",\"card_name\"}.issubset(set(acc.columns)):\n",
    "            acc_small = acc[[\"account_id\",\"card_name\",\"bank_name\"]].drop_duplicates(\"account_id\")\n",
    "            combined = combined.merge(acc_small, on=\"account_id\", how=\"left\", suffixes=(\"\",\"_dim\"))\n",
    "            # prefer dim's card_name when present\n",
    "            combined[\"card_name\"] = combined[\"card_name_dim\"].fillna(combined[\"card_name\"])\n",
    "            # prefer dim bank_name only if bank_name missing\n",
    "            if \"bank_name_dim\" in combined.columns:\n",
    "                combined[\"bank_name\"] = combined[\"bank_name\"].fillna(combined[\"bank_name_dim\"])\n",
    "            combined.drop(columns=[c for c in [\"card_name_dim\",\"bank_name_dim\"] if c in combined.columns], inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ accounts_dim join skipped: {e}\")\n",
    "\n",
    "# Sort newest first\n",
    "combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# --- PFC extraction and friendly mapping (base category) ---\n",
    "if \"personal_finance_category\" not in combined.columns:\n",
    "    combined[\"personal_finance_category\"] = None\n",
    "\n",
    "combined[\"category_pfc_primary\"] = combined[\"personal_finance_category\"].apply(\n",
    "    lambda x: (x or {}).get(\"primary\") if isinstance(x, dict) else None\n",
    ")\n",
    "combined[\"category_pfc_detailed\"] = combined[\"personal_finance_category\"].apply(\n",
    "    lambda x: (x or {}).get(\"detailed\") if isinstance(x, dict) else None\n",
    ")\n",
    "\n",
    "_pfc_map = {\n",
    "    \"FOOD_AND_DRINK\":\"Dining\",\n",
    "    \"GROCERIES\":\"Groceries\",\n",
    "    \"GENERAL_MERCHANDISE\":\"Shopping\",\n",
    "    \"TRANSPORTATION\":\"Transportation\",\n",
    "    \"TRAVEL\":\"Travel\",\n",
    "    \"HEALTHCARE\":\"Health\",\n",
    "    \"ENTERTAINMENT\":\"Entertainment\",\n",
    "    \"HOME_IMPROVEMENT\":\"Home Improvement\",\n",
    "    \"RENT_AND_UTILITIES\":\"Utilities\",\n",
    "    \"SERVICE\":\"Services\",\n",
    "    \"GOVERNMENT_AND_NON_PROFIT\":\"Government/Non-Profit\",\n",
    "    \"BANK_FEES\":\"Fees\",\n",
    "    \"INCOME\":\"Income\",\n",
    "    \"TRANSFER_OUT\":\"Transfers\",\n",
    "    \"TRANSFER_IN\":\"Transfers\",\n",
    "    \"LOAN_PAYMENTS\":\"Debt Payments\",\n",
    "    \"SUBSCRIPTION\":\"Subscriptions\",\n",
    "    \"RECURRING_SUBSCRIPTIONS\":\"Subscriptions\",\n",
    "}\n",
    "\n",
    "# Keep legacy for debugging\n",
    "combined[\"category_plaid\"] = combined.get(\"category\")\n",
    "\n",
    "# Final base category: PFC (friendly) -> legacy -> Uncategorized\n",
    "combined[\"category\"] = (\n",
    "    combined[\"category_pfc_primary\"]\n",
    "      .map(lambda s: _pfc_map.get(str(s).upper()) if pd.notna(s) else None)\n",
    "      .fillna(combined[\"category_plaid\"])\n",
    "      .fillna(\"Uncategorized\")\n",
    ")\n",
    "\n",
    "# Fill minimal NA for downstream friendliness (do NOT blank 'category')\n",
    "for c in [\"name\",\"merchant_name\",\"payment_channel\",\"bank_name\",\"card_name\"]:\n",
    "    if c in combined.columns:\n",
    "        combined[c] = combined[c].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e16b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_latest.ipynb â€” Cell 5 (REVISED) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    s = (name or \"\").upper()\n",
    "    s = re.sub(r\"APPLE PAY ENDING IN \\d{4}\", \"\", s)\n",
    "    s = re.sub(r\"#\\d{2,}\", \"\", s)              # strip store numbers like #1234\n",
    "    s = re.sub(r\"\\d+\", \"\", s)                  # kill stray digits\n",
    "    s = re.sub(r\"[^A-Z&\\s]\", \" \", s)           # keep letters, ampersand, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _extract_mapping_dict(loaded_yaml: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Accepts multiple YAML shapes:\n",
    "    - Flat: { \"MICROSOFT\": {...}, \"ARCO\": {...} }\n",
    "    - Nested: { \"MAPPING\": { \"MICROSOFT\": {...}, ... }, \"DEFAULTS\": {...}, ... }\n",
    "    - Case-insensitive key for the section name.\n",
    "    Returns a dict mapping merchant_key â†’ mapping.\n",
    "    \"\"\"\n",
    "    if not isinstance(loaded_yaml, dict):\n",
    "        return {}\n",
    "\n",
    "    # Try well-known section names, case-insensitive\n",
    "    for key in [\"MAPPING\", \"MERCHANTS\", \"MERCHANT_MAP\", \"MAP\"]:\n",
    "        for k in loaded_yaml.keys():\n",
    "            if str(k).strip().upper() == key and isinstance(loaded_yaml[k], dict):\n",
    "                return loaded_yaml[k]\n",
    "\n",
    "    # Fall back to flat if keys look like merchant names (values are dicts)\n",
    "    values_are_dicts = all(isinstance(v, dict) for v in loaded_yaml.values())\n",
    "    if values_are_dicts:\n",
    "        # But exclude obvious meta sections (CATEGORIES, DEFAULTS, etc.)\n",
    "        meta = {\"CATEGORIES\",\"DEFAULTS\",\"NECESSITY_FLAGS\",\"NON_SPEND_CATEGORIES\",\"VERSION\"}\n",
    "        if not any(str(k).strip().upper() in meta for k in loaded_yaml.keys()):\n",
    "            return loaded_yaml\n",
    "\n",
    "    return {}\n",
    "\n",
    "def _normalize_yaml_keys(mapping: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize YAML keys with the same merchant_key_from() used for data,\n",
    "    so exact matches succeed.\n",
    "    \"\"\"\n",
    "    norm = {}\n",
    "    for k, v in mapping.items():\n",
    "        mk = merchant_key_from(str(k))\n",
    "        if mk:\n",
    "            norm[mk] = v if isinstance(v, dict) else {}\n",
    "    return norm\n",
    "\n",
    "def apply_yaml_mapping(df: pd.DataFrame, yobj: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply YAML merchant mapping with precedence:\n",
    "    1) Exact merchant_key in mapping\n",
    "    2) Token buckets under mapping.<bucket>.tokens (substring match)\n",
    "    Produces *_final columns; leaves originals intact.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for c in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\"confidence_final\",\"source_final\"]:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "\n",
    "    # Extract 'mapping' block or flat map\n",
    "    ymap = _extract_mapping_dict(yobj)\n",
    "    if not isinstance(ymap, dict) or not ymap:\n",
    "        out[\"source_final\"] = out[\"source_final\"].fillna(\"raw\")\n",
    "        out[\"confidence_final\"] = out[\"confidence_final\"].fillna(\"raw\")\n",
    "        return out\n",
    "\n",
    "    # Split into exact-key map vs token buckets\n",
    "    exact_map = {}\n",
    "    token_buckets = []  # list of dicts: {bucket, tokens[], category, subcategory, display_name, tags}\n",
    "    for k, v in ymap.items():\n",
    "        if not isinstance(v, dict):\n",
    "            continue\n",
    "        if \"tokens\" in v and isinstance(v[\"tokens\"], list):\n",
    "            toks = [str(t).strip().upper() for t in v[\"tokens\"] if str(t).strip()]\n",
    "            if toks:\n",
    "                token_buckets.append({\n",
    "                    \"bucket\": str(k),\n",
    "                    \"tokens\": toks,\n",
    "                    \"category\": v.get(\"category\"),\n",
    "                    \"subcategory\": v.get(\"subcategory\"),\n",
    "                    \"display_name\": v.get(\"display_name\"),\n",
    "                    \"tags\": v.get(\"tags\"),\n",
    "                })\n",
    "        else:\n",
    "            exact_map[k] = v\n",
    "\n",
    "    exact_map_norm = _normalize_yaml_keys(exact_map)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in out.iterrows():\n",
    "        mk = str(r.get(\"merchant_key\") or \"\").upper()\n",
    "\n",
    "        # 1) Exact match\n",
    "        m = exact_map_norm.get(mk)\n",
    "        if m:\n",
    "            disp = m.get(\"display_name\") or r.get(\"merchant_name\") or r.get(\"name\") or mk\n",
    "            cat  = m.get(\"category\")\n",
    "            sub  = m.get(\"subcategory\")\n",
    "            tags = m.get(\"tags\")\n",
    "            if isinstance(tags, (list, tuple)): tags = \",\".join(str(t) for t in tags)\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": disp,\n",
    "                \"category_final\": cat,\n",
    "                \"subcategory_final\": sub,\n",
    "                \"tags_final\": tags,\n",
    "                \"confidence_final\": m.get(\"confidence\", \"map\"),\n",
    "                \"source_final\": \"yaml\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 2) Token buckets (first hit wins)\n",
    "        hit = None\n",
    "        for b in token_buckets:\n",
    "            if any(t in mk for t in b[\"tokens\"]):\n",
    "                hit = b; break\n",
    "\n",
    "        if hit:\n",
    "            disp = hit.get(\"display_name\") or r.get(\"merchant_name\") or r.get(\"name\") or mk\n",
    "            tags = hit.get(\"tags\")\n",
    "            if isinstance(tags, (list, tuple)): tags = \",\".join(str(t) for t in tags)\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": disp,\n",
    "                \"category_final\": hit.get(\"category\"),\n",
    "                \"subcategory_final\": hit.get(\"subcategory\"),\n",
    "                \"tags_final\": tags,\n",
    "                \"confidence_final\": \"bucket\",\n",
    "                \"source_final\": \"yaml-token\",\n",
    "            })\n",
    "        else:\n",
    "            # raw passthrough\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": r.get(\"display_name_final\") or r.get(\"merchant_name\") or r.get(\"name\"),\n",
    "                \"category_final\": r.get(\"category_final\"),\n",
    "                \"subcategory_final\": r.get(\"subcategory_final\"),\n",
    "                \"tags_final\": r.get(\"tags_final\"),\n",
    "                \"confidence_final\": r.get(\"confidence_final\") or \"raw\",\n",
    "                \"source_final\": r.get(\"source_final\") or \"raw\",\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def mark_non_spend_flows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    pats = [\n",
    "        r\"PAYMENT\", r\"TRANSFER\", r\"DIRECTPAY\", r\"CREDIT\",\n",
    "        r\"REFUND\", r\"REIMBURSE\", r\"ADJUSTMENT\", r\"REVERSAL\",\n",
    "        r\"ACH(?!.*APPLE\\s+CASH)\",\n",
    "        r\"WEALTHFRONT\"\n",
    "    ]\n",
    "    pat = re.compile(\"|\".join(pats))\n",
    "    names = (df.get(\"name\", pd.Series(\"\", index=df.index)).fillna(\"\") + \" \" +\n",
    "             df.get(\"merchant_name\", pd.Series(\"\", index=df.index)).fillna(\"\")).str.upper()\n",
    "    out = df.copy()\n",
    "    out[\"is_non_spend_flow\"] = names.str.contains(pat)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6e3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DIAG 2] YAML top-level keys: ['version', 'defaults', 'categories', 'non_spend_categories', 'necessity_flags', 'mapping']\n",
      "[DIAG 2] Has nested 'mapping' block: True\n",
      "[DIAG 2] mapping categories (first 20): ['dining', 'groceries', 'shopping', 'transportation', 'travel', 'auto_service', 'health_wellness', 'fitness_sports', 'government_fees', 'credit_card_payment', 'refunds_income']\n",
      "\n",
      "[DIAG 2] token counts per category (first 10):\n",
      " - dining: 9 tokens\n",
      " - groceries: 6 tokens\n",
      " - shopping: 10 tokens\n",
      " - transportation: 5 tokens\n",
      " - travel: 2 tokens\n",
      " - auto_service: 3 tokens\n",
      " - health_wellness: 3 tokens\n",
      " - fitness_sports: 3 tokens\n",
      " - government_fees: 6 tokens\n",
      " - credit_card_payment: 8 tokens\n",
      "\n",
      "[DIAG 2] Top tokenâ†’merchant_key matches (first 20):\n",
      "  ('shopping', 'TARGET', 2)\n",
      "  ('auto_service', 'FLETCHER JONES', 1)\n",
      "  ('credit_card_payment', 'CASHBACK BONUS REDEMPTION', 1)\n",
      "  ('credit_card_payment', 'PAYMENT THANK', 1)\n",
      "  ('fitness_sports', 'JIU JITSU', 1)\n",
      "  ('fitness_sports', 'SPECTATION SPORTS', 1)\n",
      "  ('government_fees', 'BULLET LEGAL SERVI', 1)\n",
      "  ('government_fees', 'DMV', 1)\n",
      "  ('government_fees', 'SOUTHERN NV HEALTH DIST', 1)\n",
      "  ('refunds_income', 'CASHBACK BONUS REDEMPTION', 1)\n",
      "  ('transportation', 'ARCO', 1)\n",
      "  ('transportation', 'S&S', 1)\n",
      "Wrote â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\debug_yaml_token_hits.csv\n",
      "YAML mapping hits: 0\n",
      "Examples of YAML-mapped rows:\n",
      "Empty DataFrame\n",
      "Columns: [merchant_key, display_name, category, subcategory, tags]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb â€” Cell 6 (REVISED) ---\n",
    "# Build merchant_key\n",
    "combined[\"merchant_key\"] = combined[\"merchant_name\"].where(\n",
    "    combined[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    combined[\"name\"]\n",
    ").map(merchant_key_from)\n",
    "\n",
    "# Load YAML (repo_root/config/categories.yaml)\n",
    "PATH_YAML = (repo_root / \"config\" / \"categories.yaml\")\n",
    "ymap_obj = {}\n",
    "if PATH_YAML.exists():\n",
    "    try:\n",
    "        with open(PATH_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "            ymap_obj = yaml.safe_load(f) or {}\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ YAML parse error: {e}\")\n",
    "        \n",
    "# === DIAG 2: Inspect YAML structure & see whether any tokens would match merchant_key ===\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"\\n[DIAG 2] YAML top-level keys:\", list((ymap_obj or {}).keys()))\n",
    "mapping_block = None\n",
    "for k in (\"mapping\",\"MAPPING\"):\n",
    "    if isinstance((ymap_obj or {}).get(k), dict):\n",
    "        mapping_block = ymap_obj[k]\n",
    "        break\n",
    "print(\"[DIAG 2] Has nested 'mapping' block:\", isinstance(mapping_block, dict))\n",
    "if isinstance(mapping_block, dict):\n",
    "    print(\"[DIAG 2] mapping categories (first 20):\", list(mapping_block.keys())[:20])\n",
    "\n",
    "def _strings_in(obj):\n",
    "    out=[]\n",
    "    if isinstance(obj, str): out.append(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        for x in obj: out.extend(_strings_in(x))\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values(): out.extend(_strings_in(v))\n",
    "    return out\n",
    "\n",
    "cat_tokens = {}\n",
    "if isinstance(mapping_block, dict):\n",
    "    for cat, node in mapping_block.items():\n",
    "        toks = [s for s in _strings_in(node) if isinstance(s, str)]\n",
    "        cat_tokens[cat] = [t for t in toks if len(str(t).strip()) >= 3]\n",
    "\n",
    "print(\"\\n[DIAG 2] token counts per category (first 10):\")\n",
    "for cat in list(cat_tokens.keys())[:10]:\n",
    "    print(f\" - {cat}: {len(cat_tokens[cat])} tokens\")\n",
    "\n",
    "mks = combined[\"merchant_key\"].astype(str).str.upper().unique() if \"merchant_key\" in combined.columns else []\n",
    "hits = []\n",
    "for cat, toks in cat_tokens.items():\n",
    "    for tok in toks:\n",
    "        T = str(tok).upper().strip()\n",
    "        if not T: continue\n",
    "        count = sum(1 for mk in mks if T in mk)\n",
    "        if count:\n",
    "            hits.append((cat, T, count))\n",
    "\n",
    "hits = sorted(hits, key=lambda x: (-x[2], x[0], x[1]))\n",
    "print(\"\\n[DIAG 2] Top tokenâ†’merchant_key matches (first 20):\")\n",
    "for row in hits[:20]:\n",
    "    print(\" \", row)\n",
    "\n",
    "DEBUG_DIR = (repo_root / \"data\" / \"processed\")\n",
    "DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame(hits, columns=[\"category\",\"token\",\"merchant_key_hits\"]).to_csv(DEBUG_DIR / \"debug_yaml_token_hits.csv\", index=False)\n",
    "print(\"Wrote â†’\", DEBUG_DIR / \"debug_yaml_token_hits.csv\")\n",
    "\n",
    "# Apply mapping + non-spend\n",
    "enriched = apply_yaml_mapping(combined, ymap_obj)\n",
    "enriched = mark_non_spend_flows(enriched)\n",
    "\n",
    "# COALESCE: canonical columns for downstream visuals\n",
    "enriched[\"display_name\"] = (\n",
    "    enriched.get(\"display_name_final\")\n",
    "            .fillna(enriched.get(\"merchant_name\"))\n",
    "            .fillna(enriched.get(\"name\"))\n",
    ")\n",
    "enriched[\"category\"]    = enriched.get(\"category_final\").fillna(enriched.get(\"category\"))\n",
    "enriched[\"subcategory\"] = enriched.get(\"subcategory_final\")\n",
    "enriched[\"tags\"]        = enriched.get(\"tags_final\").fillna(\"\")\n",
    "\n",
    "# Final export columns (canonical first; keep finals for debug)\n",
    "cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"merchant_key\",\n",
    "    \"display_name\",\"category\",\"subcategory\",\"tags\",\n",
    "    \"amount\",\"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\",\n",
    "    \"is_non_spend_flow\",\n",
    "    \"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\n",
    "    \"confidence_final\",\"source_final\"\n",
    "]\n",
    "for c in cols:\n",
    "    if c not in enriched.columns:\n",
    "        enriched[c] = None\n",
    "enriched = enriched[cols].copy()\n",
    "\n",
    "# Types\n",
    "enriched[\"date\"] = pd.to_datetime(enriched[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Quick visibility on YAML usage\n",
    "print(\"YAML mapping hits:\", int((enriched[\"source_final\"] == \"yaml\").sum()))\n",
    "print(\"Examples of YAML-mapped rows:\")\n",
    "print(enriched.loc[enriched[\"source_final\"] == \"yaml\",\n",
    "                   [\"merchant_key\",\"display_name\",\"category\",\"subcategory\",\"tags\"]].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "114c6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Latest CSV saved â†’ C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv  rows=153\n",
      "\n",
      "Preview (top 10):\n",
      "      date                                                                                                                            name  merchant_name                                                                             merchant_key                                                                                                                    display_name         category subcategory tags   amount payment_channel  pending                             account_id                        transaction_id                         bank_name                         card_name  is_non_spend_flow                                                                                                              display_name_final category_final subcategory_final tags_final confidence_final source_final\n",
      "2025-09-15                                 Deposit Kiosk / WEALTHFRONT BROKERAGE LLC/WELLS FARGO BANK, NA/75f7133df98441c1be9cff39c3a57f1a    Wealthfront                                                                              WEALTHFRONT                                                                                                                     Wealthfront        Transfers        None      -1000.00           other      0.0  gN6YZzQRNzSopryjzYN1IBPAD8pzr1t8D5wwJ XNvYQj4DNjSyAq0RY689CVwaLBOmJVHxQmrqp Silver State Schools Credit Union Silver State Schools Credit Union               True                                                                                                                     Wealthfront           None              None       None              raw          raw\n",
      "2025-09-14                                                                                              POS Signature Purchase using Apple                                                                      POS SIGNATURE PURCHASE USING APPLE                                                                                              POS Signature Purchase using Apple         Shopping        None         11.06        in store      0.0  gNvLDRDj5jt8kzgKweR5UBRg5rVrKDU6dgO1x xzYxNoNEdEFqbz1QnVybtwO9PqQpn0hgrBO9e                             Petal                             Petal              False                                                                                              POS Signature Purchase using Apple           None              None       None              raw          raw\n",
      "2025-09-14                                                                                              POS Signature Purchase using Token                                                                      POS SIGNATURE PURCHASE USING TOKEN                                                                                              POS Signature Purchase using Token        Transfers        None          5.99        in store      0.0  gNvLDRDj5jt8kzgKweR5UBRg5rVrKDU6dgO1x Joez5a54n4txBe3NY7VBsXkPKnVNR0I5oEmej                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "2025-09-13                                                                                              POS Signature Purchase using Token                                                                      POS SIGNATURE PURCHASE USING TOKEN                                                                                              POS Signature Purchase using Token        Transfers        None         12.90        in store      0.0  gNvLDRDj5jt8kzgKweR5UBRg5rVrKDU6dgO1x vz94YJYENEF1Lnpe6PaRuqxMjePdR9uVggmzk                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "2025-09-13                                                                                              POS Signature Purchase using Token                                                                      POS SIGNATURE PURCHASE USING TOKEN                                                                                              POS Signature Purchase using Token    Uncategorized        None         12.90                      NaN                                    NaN                                   NaN                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "2025-09-12                                                                                                                      Home Depot The Home Depot                                                                           THE HOME DEPOT                                                                                                                  The Home Depot Home Improvement        None          5.39        in store      0.0 MeB44vqbEwfQ5YJEbVR8UqrD3J9VKwFge99waB 89xZZYjnV7Iev7YzbN38srgOd1kjX3CrRYzQx                          Discover                          Discover              False                                                                                                                  The Home Depot           None              None       None              raw          raw\n",
      "2025-09-10                                                                                                                 Microsoft Store      Microsoft                                                                                MICROSOFT                                                                                                                       Microsoft         Shopping        None         -1.00           other      1.0 MeB44vqbEwfQ5YJEbVR8UqrD3J9VKwFge99waB LeqrrAPYodf7n6q5kejaf0Xbw00j9LH8wXNEw                          Discover                          Discover              False                                                                                                                       Microsoft           None              None       None              raw          raw\n",
      "2025-09-10                                                                                                                 Microsoft Store      Microsoft                                                                                MICROSOFT                                                                                                                       Microsoft         Shopping        None          1.00        in store      1.0 MeB44vqbEwfQ5YJEbVR8UqrD3J9VKwFge99waB Dex33174ovfDx8KrEoXnF3zr533y7XUg1KMAD                          Discover                          Discover              False                                                                                                                       Microsoft           None              None       None              raw          raw\n",
      "2025-09-10                                                                                              POS Signature Purchase using Token                                                                      POS SIGNATURE PURCHASE USING TOKEN                                                                                              POS Signature Purchase using Token        Transfers        None         18.61        in store      0.0  gNvLDRDj5jt8kzgKweR5UBRg5rVrKDU6dgO1x 07JpKyKkmkCmB94AkvE4SYxxLebrKehXEK75O                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "2025-09-09 Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953                WITHDRAWAL ALLY TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ACH ECC WEB ACH TRACE Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953    Debt Payments        None        504.22           other      0.0  gN6YZzQRNzSopryjzYN1IBPAD8pzr1t8D5wwJ ON1Y8Q39NQSwm78MOj53CmqdMPeVXjSp4jo7N Silver State Schools Credit Union Silver State Schools Credit Union               True Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000021948953           None              None       None              raw          raw\n",
      "\n",
      "Value counts â€” source_final:\n",
      "source_final\n",
      "raw    153\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb â€” Cell 7 ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "latest_path = OUTPUT_DIR / \"latest.csv\"\n",
    "\n",
    "# Write enriched (canonical columns included)\n",
    "enriched.to_csv(latest_path, index=False)\n",
    "\n",
    "assert latest_path.exists(), \"latest.csv was not written.\"\n",
    "assert \"bank_name\" in enriched.columns, \"bank_name column missing.\"\n",
    "assert \"card_name\" in enriched.columns, \"card_name column missing.\"\n",
    "\n",
    "print(f\"âœ… Latest CSV saved â†’ {latest_path}  rows={len(enriched):,}\")\n",
    "try:\n",
    "    print(\"\\nPreview (top 10):\")\n",
    "    print(enriched.head(10).to_string(index=False))\n",
    "    print(\"\\nValue counts â€” source_final:\")\n",
    "    print(enriched[\"source_final\"].value_counts(dropna=False).head(10))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
