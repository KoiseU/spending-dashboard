{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d990ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK → PLAID_CLIENT_ID: 68bb…6689 | PLAID_SECRET: a605…7df5 | PLAID_ENV: production | OUTPUT_DIR: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw | TOKENS_PATH: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\.state\\access_tokens.json\n",
      "Loaded 3 token(s).\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb — Cell 1: Env, paths, .env loader, tokens (original) ---\n",
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Optional dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "    find_dotenv = None\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    return (s[:4] + \"…\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# Resolve repo root (works from / or /scripts)\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next((p for p in [cwd, *cwd.parents] if (p / \".git\").exists() or p.name == \"spending-dashboard\"), cwd)\n",
    "\n",
    "# ✅ CI-safe override: prefer GitHub workspace path if present\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "if gw:\n",
    "    repo_root = Path(gw).resolve()\n",
    "\n",
    "# Load .env if present (scripts/.env preferred)\n",
    "def load_envs():\n",
    "    if load_dotenv is None:\n",
    "        return\n",
    "    abs_override = os.getenv(\"ENV_PATH\", str(repo_root / \"scripts\" / \".env\"))\n",
    "    if abs_override and Path(abs_override).exists():\n",
    "        try:\n",
    "            load_dotenv(abs_override, override=False, encoding=\"utf-8\")\n",
    "        except TypeError:\n",
    "            load_dotenv(abs_override, override=False)\n",
    "    for p in [\n",
    "        repo_root / \"scripts\" / \".env\",\n",
    "        repo_root / \".env\",\n",
    "        repo_root / \"config\" / \".env\",\n",
    "        cwd / \".env\",\n",
    "    ]:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                load_dotenv(str(p), override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(str(p), override=False)\n",
    "    if find_dotenv:\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        if found:\n",
    "            try:\n",
    "                load_dotenv(found, override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(found, override=False)\n",
    "\n",
    "load_envs()\n",
    "\n",
    "# Normalize env\n",
    "PLAID_CLIENT_ID = os.getenv(\"PLAID_CLIENT_ID\")\n",
    "PLAID_SECRET    = os.getenv(\"PLAID_SECRET\")\n",
    "PLAID_ENV       = (os.getenv(\"PLAID_ENV\", \"production\") or \"production\").strip().lower()\n",
    "alias = {\"prod\":\"production\",\"live\":\"production\",\"dev\":\"development\",\"devel\":\"development\",\"sb\":\"sandbox\"}\n",
    "PLAID_ENV = alias.get(PLAID_ENV, PLAID_ENV)\n",
    "if PLAID_ENV not in {\"production\",\"development\",\"sandbox\"}:\n",
    "    PLAID_ENV = \"production\"\n",
    "\n",
    "# Paths (env-overridable)\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", str(repo_root / \"data\" / \"raw\")))\n",
    "STATE_DIR  = Path(os.getenv(\"STATE_DIR\",  str(repo_root / \".state\")))\n",
    "TOKENS_PATH = Path(os.getenv(\"TOKENS_PATH\", str(STATE_DIR / \"access_tokens.json\")))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load & validate access tokens (env > file), canonical-only ---\n",
    "def _strip_bom(s: str) -> str:\n",
    "    return s.lstrip(\"\\ufeff\") if isinstance(s, str) else s\n",
    "\n",
    "def _parse_pairs_blob(blob: str) -> dict:\n",
    "    raw = [p.strip() for sep in [\"\\n\",\";\",\"|\",\",\"] for p in (blob.split(sep) if sep in blob else []) if p.strip()]\n",
    "    if not raw: raw = [blob.strip()]\n",
    "    out = {}\n",
    "    for p in raw:\n",
    "        if \"=\" in p:\n",
    "            k, v = p.split(\"=\", 1)\n",
    "        elif \":\" in p:\n",
    "            k, v = p.split(\":\", 1)\n",
    "        else:\n",
    "            continue\n",
    "        k = k.strip().strip('\"').strip(\"'\")\n",
    "        v = v.strip().strip('\"').strip(\"'\")\n",
    "        if k and v:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def _normalize_tokens(obj) -> dict:\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): str(v).strip() for k,v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        out = {}\n",
    "        for item in obj:\n",
    "            if isinstance(item, dict):\n",
    "                name = item.get(\"issuer\") or item.get(\"bank\") or item.get(\"name\")\n",
    "                token = item.get(\"access_token\") or item.get(\"token\")\n",
    "                if name and token:\n",
    "                    out[str(name)] = str(token).strip()\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        s = _strip_bom(obj).strip()\n",
    "        try:\n",
    "            parsed = json.loads(s)    # JSON first\n",
    "            return _normalize_tokens(parsed)\n",
    "        except Exception:\n",
    "            return _parse_pairs_blob(s)\n",
    "    return {}\n",
    "\n",
    "def load_access_tokens():\n",
    "    blob = os.getenv(\"PLAID_ACCESS_TOKENS\", \"\").strip()\n",
    "    if blob:\n",
    "        tokens = _normalize_tokens(blob)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    if TOKENS_PATH.exists():\n",
    "        raw = TOKENS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        tokens = _normalize_tokens(raw)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    raise AssertionError(\n",
    "        f\"Could not load access tokens. Provide PLAID_ACCESS_TOKENS env or a valid JSON at {TOKENS_PATH}.\"\n",
    "    )\n",
    "\n",
    "ACCESS_TOKENS = load_access_tokens()\n",
    "\n",
    "PAT = re.compile(r\"^access-(?:production|development|sandbox)-[a-z0-9\\-]+$\")\n",
    "expected_prefix = f\"access-{PLAID_ENV}-\"\n",
    "bad = [k for k,v in ACCESS_TOKENS.items() if not isinstance(v, str) or not v.startswith(expected_prefix) or not PAT.match(v)]\n",
    "assert not bad, f\"Non-canonical or wrong-env tokens for: {bad}. Ensure tokens look like '{expected_prefix}…' (no '/', '+', '=').\"\n",
    "\n",
    "print(\n",
    "    \"Env OK →\",\n",
    "    \"PLAID_CLIENT_ID:\", mask(PLAID_CLIENT_ID),\n",
    "    \"| PLAID_SECRET:\", mask(PLAID_SECRET),\n",
    "    \"| PLAID_ENV:\", PLAID_ENV,\n",
    "    \"| OUTPUT_DIR:\", str(OUTPUT_DIR),\n",
    "    \"| TOKENS_PATH:\", str(TOKENS_PATH),\n",
    ")\n",
    "print(f\"Loaded {len(ACCESS_TOKENS)} token(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a22fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaid SDK: v10+ (plaid_api)\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb — Cell 2: Plaid client init (original) ---\n",
    "USE_PLAID_V10 = False\n",
    "client = None\n",
    "\n",
    "try:\n",
    "    # v10+ path\n",
    "    from plaid.api import plaid_api\n",
    "    from plaid.configuration import Configuration\n",
    "    try:\n",
    "        from plaid.configuration import Environment  # newer enum\n",
    "        env_host = {\n",
    "            \"production\":  Environment.Production,\n",
    "            \"development\": Environment.Development,\n",
    "            \"sandbox\":     Environment.Sandbox,\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=env_host)\n",
    "    except Exception:\n",
    "        # fallback if Environment enum not present\n",
    "        host_url = {\n",
    "            \"production\":  \"https://production.plaid.com\",\n",
    "            \"development\": \"https://development.plaid.com\",\n",
    "            \"sandbox\":     \"https://sandbox.plaid.com\",\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=host_url)\n",
    "\n",
    "    from plaid.api_client import ApiClient\n",
    "    config.api_key[\"clientId\"] = PLAID_CLIENT_ID\n",
    "    config.api_key[\"secret\"]   = PLAID_SECRET\n",
    "    api_client = ApiClient(config)\n",
    "    client = plaid_api.PlaidApi(api_client)\n",
    "    USE_PLAID_V10 = True\n",
    "    print(\"Plaid SDK: v10+ (plaid_api)\")\n",
    "except Exception as e_v10:\n",
    "    try:\n",
    "        # legacy path\n",
    "        from plaid import Client as LegacyClient\n",
    "        client = LegacyClient(\n",
    "            client_id=PLAID_CLIENT_ID,\n",
    "            secret=PLAID_SECRET,\n",
    "            environment=PLAID_ENV\n",
    "        )\n",
    "        USE_PLAID_V10 = False\n",
    "        print(\"Plaid SDK: legacy Client()\")\n",
    "    except Exception as e_legacy:\n",
    "        raise ImportError(\n",
    "            \"Could not initialize Plaid client. Ensure 'plaid-python' is installed. \"\n",
    "            f\"v10 error: {e_v10}\\nlegacy error: {e_legacy}\"\n",
    "        )\n",
    "\n",
    "# --- Expanded PRECHECK: list accounts per token when PRECHECK=1 ---\n",
    "if os.getenv(\"PRECHECK\", \"0\") == \"1\" and USE_PLAID_V10:\n",
    "    from plaid.model.accounts_get_request import AccountsGetRequest\n",
    "    from plaid.api_client import ApiException\n",
    "    print(\"\\n[PRECHECK] Listing accounts per token:\")\n",
    "    for issuer, tok in ACCESS_TOKENS.items():\n",
    "        try:\n",
    "            acc_resp = client.accounts_get(AccountsGetRequest(access_token=tok)).to_dict()\n",
    "            accounts = acc_resp.get(\"accounts\", []) or []\n",
    "            print(f\"- {issuer}: {len(accounts)} account(s)\")\n",
    "            for a in accounts:\n",
    "                name = a.get(\"name\") or a.get(\"official_name\") or \"\"\n",
    "                mask = a.get(\"mask\") or \"\"\n",
    "                subtype = (a.get(\"subtype\") or \"\").upper()\n",
    "                a_id = a.get(\"account_id\")\n",
    "                print(f\"    • {name}  (subtype={subtype}, mask={mask}, id={a_id})\")\n",
    "        except ApiException as e:\n",
    "            print(f\"- {issuer}: ❌ API {e.status} -> {getattr(e, 'body', e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd6aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SYNC Discover (start: cursor-present)\n",
      "   [sync diag] PFC in added: 0 / 0, modified: 0 / 0\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Petal (start: cursor-present)\n",
      "   [sync diag] PFC in added: 0 / 0, modified: 0 / 0\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Silver State Schools Credit Union (start: cursor-present)\n",
      "   [sync diag] PFC in added: 0 / 0, modified: 0 / 0\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "✅ Consolidated using SYNC → rows=156 across 3 bank(s)\n",
      "Window: 2025-03-21 → 2025-09-17 | DAYS_BACK=180\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb — Cell 3 (UPDATED): Pull & consolidate via /transactions/sync ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Rolling window (override with DAYS_BACK env if you like)\n",
    "DAYS_BACK = int(os.getenv(\"DAYS_BACK\", \"180\"))\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "CURSORS_PATH = STATE_DIR / \"plaid_cursors.json\"\n",
    "\n",
    "def load_cursors() -> dict:\n",
    "    if CURSORS_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CURSORS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def save_cursors(cur: dict):\n",
    "    CURSORS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    CURSORS_PATH.write_text(json.dumps(cur, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def normalize_category(x):\n",
    "    return \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "# Convert raw txn dicts to our normalized schema (KEEP PFC)\n",
    "def df_from_txns(txns: list[dict], bank_name: str) -> pd.DataFrame:\n",
    "    if not txns:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(txns)\n",
    "\n",
    "    # Ensure expected columns exist (include PFC so we can map it later)\n",
    "    expected_cols = [\n",
    "        \"name\",\"merchant_name\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"category\",\"date\",\"amount\",\n",
    "        \"personal_finance_category\",\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Normalize\n",
    "    df[\"category\"] = df[\"category\"].apply(normalize_category)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    # Bank & card\n",
    "    df[\"bank_name\"] = bank_name\n",
    "    df[\"card_name\"] = bank_name  # upgrade later via accounts_dim if desired\n",
    "\n",
    "    keep_cols = [\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\",\"personal_finance_category\"\n",
    "    ]\n",
    "    return df[[c for c in keep_cols if c in df.columns]].copy()\n",
    "\n",
    "# Read previous latest.csv (acts as our cache for SYNC deltas)\n",
    "latest_csv_path = (repo_root / \"data\" / \"raw\" / \"latest.csv\")\n",
    "prev = pd.DataFrame()\n",
    "if latest_csv_path.exists():\n",
    "    try:\n",
    "        prev = pd.read_csv(latest_csv_path)\n",
    "        prev[\"date\"] = pd.to_datetime(prev[\"date\"], errors=\"coerce\")\n",
    "        if \"transaction_id\" not in prev.columns:\n",
    "            prev[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read previous latest.csv: {e}\")\n",
    "\n",
    "# Optional one-time full resync (set FORCE_FULL_RESYNC=1 in env)\n",
    "if (os.getenv(\"FORCE_FULL_RESYNC\", \"0\") or \"0\").strip().lower() in {\"1\",\"true\",\"yes\",\"on\"}:\n",
    "    try:\n",
    "        if CURSORS_PATH.exists():\n",
    "            CURSORS_PATH.unlink()\n",
    "            print(\"⚠️ Reset: deleted .state/plaid_cursors.json to force a full resync.\")\n",
    "    except Exception as e:\n",
    "        print(\"Reset warning:\", e)\n",
    "    cursors = {}\n",
    "    prev = pd.DataFrame()\n",
    "else:\n",
    "    cursors = load_cursors()\n",
    "\n",
    "all_added_mod = []   # list of DataFrames of added/modified across banks\n",
    "all_removed_ids = set()\n",
    "\n",
    "if 'USE_PLAID_V10' in globals() and USE_PLAID_V10:\n",
    "    # --- Sync-first path using Plaid v10 client (simple + robust) ---\n",
    "    from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "\n",
    "    def sync_one(bank_name: str, access_token: str, cursor: str | None):\n",
    "        \"\"\"\n",
    "        Call /transactions/sync until has_more is False.\n",
    "        Explicitly requests Personal Finance Category (PFC) on every page.\n",
    "        Returns: (next_cursor, added, modified, removed_ids)\n",
    "        \"\"\"\n",
    "        added, modified, removed_ids = [], [], []\n",
    "        next_cursor = cursor\n",
    "\n",
    "        while True:\n",
    "            req_kwargs = {\"access_token\": access_token, \"count\": 500}\n",
    "            if isinstance(next_cursor, str) and next_cursor:\n",
    "                req_kwargs[\"cursor\"] = next_cursor\n",
    "\n",
    "            # Ask Plaid to include PFC (and original description for debugging)\n",
    "            try:\n",
    "                from plaid.model.transactions_sync_request_options import TransactionsSyncRequestOptions\n",
    "                req_kwargs[\"options\"] = TransactionsSyncRequestOptions(\n",
    "                    include_personal_finance_category=True,\n",
    "                    include_original_description=True\n",
    "                )\n",
    "            except Exception:\n",
    "                # Older SDKs may not expose the Options model; skip silently.\n",
    "                pass\n",
    "\n",
    "            req = TransactionsSyncRequest(**req_kwargs)\n",
    "            resp = client.transactions_sync(req).to_dict()\n",
    "\n",
    "            added.extend(resp.get(\"added\", []) or [])\n",
    "            modified.extend(resp.get(\"modified\", []) or [])\n",
    "\n",
    "            # removed may be list[str] or list[dict]\n",
    "            rem = resp.get(\"removed\", []) or []\n",
    "            for r in rem:\n",
    "                if isinstance(r, dict):\n",
    "                    rid = r.get(\"transaction_id\")\n",
    "                    if rid: removed_ids.append(rid)\n",
    "                elif isinstance(r, str):\n",
    "                    removed_ids.append(r)\n",
    "\n",
    "            next_cursor = resp.get(\"next_cursor\", next_cursor)\n",
    "            if not resp.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "        # Tiny diag: count how many returned txns actually have PFC payloads\n",
    "        def _pfc_count(lst):\n",
    "            c = 0\n",
    "            for t in lst:\n",
    "                pfc = t.get(\"personal_finance_category\")\n",
    "                if isinstance(pfc, dict) and (pfc.get(\"primary\") or pfc.get(\"detailed\")):\n",
    "                    c += 1\n",
    "            return c\n",
    "\n",
    "        print(f\"   [sync diag] PFC in added: {_pfc_count(added)} / {len(added)}, \"\n",
    "              f\"modified: {_pfc_count(modified)} / {len(modified)}\")\n",
    "\n",
    "        return (next_cursor if isinstance(next_cursor, str) and next_cursor else None,\n",
    "                added, modified, removed_ids)\n",
    "\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 SYNC {bank_name} (start: {'cursor-present' if cursors.get(token) else 'no-cursor'})\")\n",
    "        cur0 = cursors.get(token)\n",
    "        next_cur, added, modified, removed_ids = sync_one(bank_name, token, cur0)\n",
    "\n",
    "        df_add = df_from_txns(added, bank_name)\n",
    "        df_mod = df_from_txns(modified, bank_name)\n",
    "        all_added_mod.append(df_add)\n",
    "        all_added_mod.append(df_mod)\n",
    "        all_removed_ids.update(removed_ids)\n",
    "\n",
    "        cursors[token] = next_cur\n",
    "        print(f\"   → added={len(df_add):,}, modified={len(df_mod):,}, removed={len(removed_ids):,}, next_cursor={'set' if next_cur else 'None'}\")\n",
    "\n",
    "    # Start from previous CSV and apply deltas\n",
    "    cur = prev.copy()\n",
    "\n",
    "    # Remove deleted transaction_ids (guard if column is present)\n",
    "    if not cur.empty and all_removed_ids and \"transaction_id\" in cur.columns:\n",
    "        cur = cur[~cur[\"transaction_id\"].astype(str).isin({str(x) for x in all_removed_ids})]\n",
    "    elif not cur.empty and all_removed_ids:\n",
    "        print(\"ℹ️ Skipping delete-apply: previous cache lacks 'transaction_id' column.\")\n",
    "\n",
    "    # Replace modified ids and add new ones\n",
    "    if any(len(x) for x in all_added_mod):\n",
    "        new_mod = (pd.concat([df for df in all_added_mod if not df.empty], ignore_index=True)\n",
    "                   if all_added_mod else pd.DataFrame())\n",
    "        if not new_mod.empty and \"transaction_id\" not in new_mod.columns:\n",
    "            new_mod[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "\n",
    "        if not cur.empty and \"transaction_id\" in cur.columns and \"transaction_id\" in new_mod.columns:\n",
    "            mod_ids = set(new_mod[\"transaction_id\"].dropna().astype(str).tolist())\n",
    "            if mod_ids:\n",
    "                cur = cur[~cur[\"transaction_id\"].astype(str).isin(mod_ids)]\n",
    "        else:\n",
    "            if not cur.empty and not new_mod.empty:\n",
    "                print(\"ℹ️ Skipping modify-replace: one frame lacks 'transaction_id'; appending only.\")\n",
    "\n",
    "        combined = pd.concat([cur, new_mod], ignore_index=True) if not new_mod.empty else cur.copy()\n",
    "    else:\n",
    "        combined = cur.copy()\n",
    "\n",
    "else:\n",
    "    # --- Fallback: windowed GET per item (legacy client) ---\n",
    "    def fetch_transactions_get(bank_name: str, access_token: str) -> pd.DataFrame:\n",
    "        txns = []\n",
    "        offset = 0\n",
    "        while True:\n",
    "            resp = client.Transactions.get(\n",
    "                access_token=access_token,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                options={\"count\": 500, \"offset\": offset}\n",
    "            )\n",
    "            total = resp[\"total_transactions\"]\n",
    "            txns.extend(resp[\"transactions\"])\n",
    "            if len(txns) >= total:\n",
    "                break\n",
    "            offset = len(txns)\n",
    "            if offset > 50_000:\n",
    "                raise RuntimeError(f\"Pagination runaway for {bank_name}\")\n",
    "        return df_from_txns(txns, bank_name)\n",
    "\n",
    "    frames = []\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 GET {bank_name} ({start_date} → {end_date})…\")\n",
    "        frames.append(fetch_transactions_get(bank_name, token))\n",
    "    combined = pd.concat([f for f in frames if f is not None and not f.empty], ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# Finalize for both paths: trim to window, sort, dedupe by best available key\n",
    "if combined is None or combined.empty:\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\",\"personal_finance_category\"\n",
    "    ])\n",
    "else:\n",
    "    combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "    combined = combined[\n",
    "        (combined[\"date\"] >= pd.Timestamp(start_date)) &\n",
    "        (combined[\"date\"] <= pd.Timestamp(end_date))\n",
    "    ]\n",
    "    # Primary dedupe: transaction_id; fallback: (account_id, date, amount, name)\n",
    "    if \"transaction_id\" in combined.columns and combined[\"transaction_id\"].notna().any():\n",
    "        combined = (combined\n",
    "                    .sort_values(\"date\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"transaction_id\"], keep=\"first\")\n",
    "                    .reset_index(drop=True))\n",
    "    else:\n",
    "        subset = [c for c in [\"account_id\",\"date\",\"amount\",\"name\"] if c in combined.columns]\n",
    "        if subset:\n",
    "            combined = (combined\n",
    "                        .sort_values(\"date\", ascending=False)\n",
    "                        .drop_duplicates(subset=subset, keep=\"first\")\n",
    "                        .reset_index(drop=True))\n",
    "        else:\n",
    "            combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Persist cursors (local/CI workspace)\n",
    "try:\n",
    "    save_cursors(cursors if 'cursors' in locals() else {})\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save cursors: {e}\")\n",
    "\n",
    "print(f\"✅ Consolidated using {'SYNC' if 'USE_PLAID_V10' in globals() and USE_PLAID_V10 else 'GET'} → rows={len(combined):,} across {len(ACCESS_TOKENS)} bank(s)\")\n",
    "print(f\"Window: {start_date} → {end_date} | DAYS_BACK={DAYS_BACK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6faf63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BUILD] Counters\n",
      "Rows: 156\n",
      "Has PFC primary: 0\n",
      "\n",
      "Top 15 category (post-PFC mapping):\n",
      "category\n",
      "Transfers                46\n",
      "Debt Payments            32\n",
      "Shopping                 30\n",
      "Dining                   17\n",
      "Services                 14\n",
      "Transportation            7\n",
      "Entertainment             4\n",
      "Home Improvement          2\n",
      "Uncategorized             1\n",
      "Fees                      1\n",
      "Health                    1\n",
      "Government/Non-Profit     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3.5: Extract PFC and map to friendly base category ---\n",
    "\n",
    "# Ensure the column exists even if missing\n",
    "if \"personal_finance_category\" not in combined.columns:\n",
    "    combined[\"personal_finance_category\"] = None\n",
    "\n",
    "# Extract PFC primary/detailed safely\n",
    "def _pfc_get(x, key):\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(key)\n",
    "    # Some SDKs may serialize nested objects as strings; try to parse\n",
    "    if isinstance(x, str) and x.strip().startswith(\"{\"):\n",
    "        try:\n",
    "            d = json.loads(x)\n",
    "            if isinstance(d, dict):\n",
    "                return d.get(key)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "combined[\"category_pfc_primary\"]  = combined[\"personal_finance_category\"].apply(lambda x: _pfc_get(x, \"primary\"))\n",
    "combined[\"category_pfc_detailed\"] = combined[\"personal_finance_category\"].apply(lambda x: _pfc_get(x, \"detailed\"))\n",
    "\n",
    "# Friendly names for PFC primary\n",
    "_pfc_map = {\n",
    "    \"FOOD_AND_DRINK\":\"Dining\",\n",
    "    \"GROCERIES\":\"Groceries\",\n",
    "    \"GENERAL_MERCHANDISE\":\"Shopping\",\n",
    "    \"TRANSPORTATION\":\"Transportation\",\n",
    "    \"TRAVEL\":\"Travel\",\n",
    "    \"HEALTHCARE\":\"Health\",\n",
    "    \"ENTERTAINMENT\":\"Entertainment\",\n",
    "    \"HOME_IMPROVEMENT\":\"Home Improvement\",\n",
    "    \"RENT_AND_UTILITIES\":\"Utilities\",\n",
    "    \"SERVICE\":\"Services\",\n",
    "    \"GOVERNMENT_AND_NON_PROFIT\":\"Government/Non-Profit\",\n",
    "    \"BANK_FEES\":\"Fees\",\n",
    "    \"INCOME\":\"Income\",\n",
    "    \"TRANSFER_OUT\":\"Transfers\",\n",
    "    \"TRANSFER_IN\":\"Transfers\",\n",
    "    \"LOAN_PAYMENTS\":\"Debt Payments\",\n",
    "    \"SUBSCRIPTION\":\"Subscriptions\",\n",
    "    \"RECURRING_SUBSCRIPTIONS\":\"Subscriptions\",\n",
    "}\n",
    "\n",
    "# Preserve legacy for debugging\n",
    "combined[\"category_plaid_legacy\"] = combined.get(\"category\")\n",
    "\n",
    "# Final base 'category' that leaves build:\n",
    "combined[\"category\"] = (\n",
    "    combined.get(\"category_pfc_primary\")\n",
    "      .map(lambda s: _pfc_map.get(str(s).upper()) if pd.notna(s) and str(s).strip() else None)\n",
    "      .fillna(combined.get(\"category_plaid_legacy\"))\n",
    "      .fillna(\"Uncategorized\")\n",
    ")\n",
    "\n",
    "print(\"\\n[BUILD] Counters\")\n",
    "print(\"Rows:\", len(combined))\n",
    "print(\"Has PFC primary:\", int(combined[\"category_pfc_primary\"].notna().sum()))\n",
    "print(\"\\nTop 15 category (post-PFC mapping):\")\n",
    "print(combined[\"category\"].fillna(\"<<NULL>>\").value_counts().head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03199b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BUILD DIAG v2] Columns present: ['date', 'name', 'merchant_name', 'merchant_key', 'display_name', 'category', 'subcategory', 'tags', 'amount', 'payment_channel', 'pending', 'account_id', 'transaction_id', 'bank_name', 'card_name', 'is_non_spend_flow', 'display_name_final', 'category_final', 'subcategory_final', 'tags_final', 'confidence_final', 'source_final', 'personal_finance_category', 'category_pfc_primary', 'category_pfc_detailed', 'category_plaid_legacy']\n",
      "Has personal_finance_category column: True\n",
      "Sample columns for PFC probe: ['name', 'merchant_name', 'category', 'personal_finance_category']\n",
      "Non-null PFC rows: 0\n",
      "\n",
      "Legacy 'category' top 10 BEFORE any mapping:\n",
      "category\n",
      "Transfers           46\n",
      "Debt Payments       32\n",
      "Shopping            30\n",
      "Dining              17\n",
      "Services            14\n",
      "Transportation       7\n",
      "Entertainment        4\n",
      "Home Improvement     2\n",
      "Uncategorized        1\n",
      "Fees                 1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- DIAG: Do we actually have PFC fields in the raw frames? ---\n",
    "print(\"[BUILD DIAG v2] Columns present:\", list(combined.columns))\n",
    "\n",
    "has_pfc_col = \"personal_finance_category\" in combined.columns\n",
    "print(\"Has personal_finance_category column:\", has_pfc_col)\n",
    "\n",
    "# Peek a few raw dicts if we can (best-effort; may be empty if we already collapsed earlier)\n",
    "sample_cols = [c for c in combined.columns if \"personal_finance\" in c.lower() or c in (\"category\",\"name\",\"merchant_name\")]\n",
    "print(\"Sample columns for PFC probe:\", sample_cols[:8])\n",
    "\n",
    "# Try to show any non-null personal_finance_category values if the column exists\n",
    "if has_pfc_col:\n",
    "    nn = combined[\"personal_finance_category\"].dropna()\n",
    "    print(\"Non-null PFC rows:\", int(nn.shape[0]))\n",
    "    if not nn.empty:\n",
    "        print(\"Example PFC dicts (up to 3):\", nn.head(3).tolist())\n",
    "\n",
    "print(\"\\nLegacy 'category' top 10 BEFORE any mapping:\")\n",
    "print(combined.get(\"category\", pd.Series(dtype=object)).fillna(\"<<NULL>>\").value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7e92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_latest.ipynb — Cell 4 ---\n",
    "if combined.empty:\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"bank_name\",\"card_name\"\n",
    "    ])\n",
    "\n",
    "# Normalize category list → string\n",
    "if \"category\" in combined.columns:\n",
    "    combined[\"category\"] = combined[\"category\"].apply(\n",
    "        lambda x: \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "    )\n",
    "\n",
    "# Ensure date type\n",
    "combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only expected columns\n",
    "keep_cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "    \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\"\n",
    "]\n",
    "combined = combined[[c for c in keep_cols if c in combined.columns]].copy()\n",
    "\n",
    "# 🔗 Upgrade card_name from accounts_dim.csv if available\n",
    "acc_dim_path = repo_root / \"config\" / \"accounts_dim.csv\"\n",
    "if acc_dim_path.exists() and \"account_id\" in combined.columns:\n",
    "    try:\n",
    "        acc = pd.read_csv(acc_dim_path)\n",
    "        if {\"account_id\",\"card_name\"}.issubset(set(acc.columns)):\n",
    "            acc_small = acc[[\"account_id\",\"card_name\",\"bank_name\"]].drop_duplicates(\"account_id\")\n",
    "            combined = combined.merge(acc_small, on=\"account_id\", how=\"left\", suffixes=(\"\",\"_dim\"))\n",
    "            # prefer dim's card_name when present\n",
    "            combined[\"card_name\"] = combined[\"card_name_dim\"].fillna(combined[\"card_name\"])\n",
    "            # prefer dim bank_name only if bank_name missing\n",
    "            if \"bank_name_dim\" in combined.columns:\n",
    "                combined[\"bank_name\"] = combined[\"bank_name\"].fillna(combined[\"bank_name_dim\"])\n",
    "            combined.drop(columns=[c for c in [\"card_name_dim\",\"bank_name_dim\"] if c in combined.columns], inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ accounts_dim join skipped: {e}\")\n",
    "\n",
    "# Sort newest first\n",
    "combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# --- PFC extraction and friendly mapping (base category) ---\n",
    "if \"personal_finance_category\" not in combined.columns:\n",
    "    combined[\"personal_finance_category\"] = None\n",
    "\n",
    "combined[\"category_pfc_primary\"] = combined[\"personal_finance_category\"].apply(\n",
    "    lambda x: (x or {}).get(\"primary\") if isinstance(x, dict) else None\n",
    ")\n",
    "combined[\"category_pfc_detailed\"] = combined[\"personal_finance_category\"].apply(\n",
    "    lambda x: (x or {}).get(\"detailed\") if isinstance(x, dict) else None\n",
    ")\n",
    "\n",
    "_pfc_map = {\n",
    "    \"FOOD_AND_DRINK\":\"Dining\",\n",
    "    \"GROCERIES\":\"Groceries\",\n",
    "    \"GENERAL_MERCHANDISE\":\"Shopping\",\n",
    "    \"TRANSPORTATION\":\"Transportation\",\n",
    "    \"TRAVEL\":\"Travel\",\n",
    "    \"HEALTHCARE\":\"Health\",\n",
    "    \"ENTERTAINMENT\":\"Entertainment\",\n",
    "    \"HOME_IMPROVEMENT\":\"Home Improvement\",\n",
    "    \"RENT_AND_UTILITIES\":\"Utilities\",\n",
    "    \"SERVICE\":\"Services\",\n",
    "    \"GOVERNMENT_AND_NON_PROFIT\":\"Government/Non-Profit\",\n",
    "    \"BANK_FEES\":\"Fees\",\n",
    "    \"INCOME\":\"Income\",\n",
    "    \"TRANSFER_OUT\":\"Transfers\",\n",
    "    \"TRANSFER_IN\":\"Transfers\",\n",
    "    \"LOAN_PAYMENTS\":\"Debt Payments\",\n",
    "    \"SUBSCRIPTION\":\"Subscriptions\",\n",
    "    \"RECURRING_SUBSCRIPTIONS\":\"Subscriptions\",\n",
    "}\n",
    "\n",
    "# Keep legacy for debugging\n",
    "combined[\"category_plaid\"] = combined.get(\"category\")\n",
    "\n",
    "# Final base category: PFC (friendly) -> legacy -> Uncategorized\n",
    "combined[\"category\"] = (\n",
    "    combined[\"category_pfc_primary\"]\n",
    "      .map(lambda s: _pfc_map.get(str(s).upper()) if pd.notna(s) else None)\n",
    "      .fillna(combined[\"category_plaid\"])\n",
    "      .fillna(\"Uncategorized\")\n",
    ")\n",
    "\n",
    "# Fill minimal NA for downstream friendliness (do NOT blank 'category')\n",
    "for c in [\"name\",\"merchant_name\",\"payment_channel\",\"bank_name\",\"card_name\"]:\n",
    "    if c in combined.columns:\n",
    "        combined[c] = combined[c].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_latest.ipynb — Cell 5 (REVISED) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalization for merchant identity:\n",
    "    - Canonicalize brand patterns (AMZN/AMAZON, PAYPAL, SQUARE, APPLE.COM/BILL, GOOGLE*)\n",
    "    - Strip bank noise (POS/DEBIT/CHECK CRD/ACH/ZELLE/TRANSFER/etc.)\n",
    "    - Remove store numbers/digits/punctuation; keep letters, &, spaces, and '/' '.' for brand URLs\n",
    "    - Collapse whitespace; fallback to 'UNKNOWN'\n",
    "    \"\"\"\n",
    "    u = (name or \"\").upper()\n",
    "\n",
    "    # Canonical brand replacements (before stripping)\n",
    "    canon = [\n",
    "        (r\"AMZN\\s+MKTPL?C?E?|AMAZON\\.?\\s*COM\", \"AMAZON\"),\n",
    "        (r\"APPLE\\.?\\s*COM/?BILL\", \"APPLE.COM/BILL\"),\n",
    "        (r\"\\bGOOGLE\\*\", \"GOOGLE \"),\n",
    "        (r\"\\bSQC?\\*\", \"SQUARE \"),\n",
    "        (r\"\\bPAYPAL\\*?\", \"PAYPAL \"),\n",
    "    ]\n",
    "    for pat, repl in canon:\n",
    "        u = re.sub(pat, repl, u)\n",
    "\n",
    "    # Strip common bank/payments noise tokens\n",
    "    noise = [\n",
    "        r\"APPLE PAY ENDING IN \\d{4}\",\n",
    "        r\"POS(?:\\s+PURCHASE)?\",\n",
    "        r\"DEBIT(?:\\s+CARD)?(?:\\s+PURCHASE)?\",\n",
    "        r\"CHECK ?CRD\",\n",
    "        r\"VISA(?:\\s+POS)?\", r\"MASTERCARD\", r\"DISCOVER\", r\"AMEX\",\n",
    "        r\"ACH(?:\\s+(CREDIT|DEBIT))?\", r\"WEB AUTHORIZED PMT\", r\"ONLINE PMT\",\n",
    "        r\"ZELLE(?:\\s+PAYMENT)?\", r\"VENMO(?:\\s+PAYMENT)?\",\n",
    "        r\"XFER\", r\"TRANSFER\",\n",
    "        r\"PURCHASE\", r\"PENDING\", r\"REVERSAL\", r\"ADJ(?:USTMENT)?\",\n",
    "        r\"ID[: ]?\\d+\",\n",
    "    ]\n",
    "    for pat in noise:\n",
    "        u = re.sub(rf\"\\b{pat}\\b\", \" \", u)\n",
    "\n",
    "    # Remove store numbers & digits\n",
    "    u = re.sub(r\"#\\d{2,}\", \" \", u)\n",
    "    u = re.sub(r\"\\d+\", \" \", u)\n",
    "\n",
    "    # Keep letters, '&', spaces, plus '/' '.' for URLish brands; collapse spaces\n",
    "    u = re.sub(r\"[^A-Z&\\s\\./]\", \" \", u)\n",
    "    u = re.sub(r\"\\s+\", \" \", u).strip()\n",
    "\n",
    "    # Post-canon tidy\n",
    "    u = u.replace(\"APPLE COM BILL\", \"APPLE.COM/BILL\").strip()\n",
    "    return u or \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def _extract_mapping_dict(loaded_yaml: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Accepts multiple YAML shapes:\n",
    "    - Flat: { \"MICROSOFT\": {...}, \"ARCO\": {...} }\n",
    "    - Nested: { \"MAPPING\": { \"MICROSOFT\": {...}, ... }, \"DEFAULTS\": {...}, ... }\n",
    "    - Case-insensitive key for the section name.\n",
    "    Returns a dict mapping merchant_key → mapping.\n",
    "    \"\"\"\n",
    "    if not isinstance(loaded_yaml, dict):\n",
    "        return {}\n",
    "\n",
    "    # Try well-known section names, case-insensitive\n",
    "    for key in [\"MAPPING\", \"MERCHANTS\", \"MERCHANT_MAP\", \"MAP\"]:\n",
    "        for k in loaded_yaml.keys():\n",
    "            if str(k).strip().upper() == key and isinstance(loaded_yaml[k], dict):\n",
    "                return loaded_yaml[k]\n",
    "\n",
    "    # Fall back to flat if keys look like merchant names (values are dicts)\n",
    "    values_are_dicts = all(isinstance(v, dict) for v in loaded_yaml.values())\n",
    "    if values_are_dicts:\n",
    "        # But exclude obvious meta sections (CATEGORIES, DEFAULTS, etc.)\n",
    "        meta = {\"CATEGORIES\",\"DEFAULTS\",\"NECESSITY_FLAGS\",\"NON_SPEND_CATEGORIES\",\"VERSION\"}\n",
    "        if not any(str(k).strip().upper() in meta for k in loaded_yaml.keys()):\n",
    "            return loaded_yaml\n",
    "\n",
    "    return {}\n",
    "\n",
    "def _normalize_yaml_keys(mapping: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize YAML keys with the same merchant_key_from() used for data,\n",
    "    so exact matches succeed.\n",
    "    \"\"\"\n",
    "    norm = {}\n",
    "    for k, v in mapping.items():\n",
    "        mk = merchant_key_from(str(k))\n",
    "        if mk:\n",
    "            norm[mk] = v if isinstance(v, dict) else {}\n",
    "    return norm\n",
    "\n",
    "def apply_yaml_mapping(df: pd.DataFrame, yobj: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply YAML merchant mapping with precedence:\n",
    "    1) Exact merchant_key in mapping\n",
    "    2) Token buckets under mapping.<bucket>.tokens (substring match)\n",
    "    Produces *_final columns; leaves originals intact.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for c in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\"confidence_final\",\"source_final\"]:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "\n",
    "    # Extract 'mapping' block or flat map\n",
    "    ymap = _extract_mapping_dict(yobj)\n",
    "    if not isinstance(ymap, dict) or not ymap:\n",
    "        out[\"source_final\"] = out[\"source_final\"].fillna(\"raw\")\n",
    "        out[\"confidence_final\"] = out[\"confidence_final\"].fillna(\"raw\")\n",
    "        return out\n",
    "\n",
    "    # Split into exact-key map vs token buckets\n",
    "    exact_map = {}\n",
    "    token_buckets = []  # list of dicts: {bucket, tokens[], category, subcategory, display_name, tags}\n",
    "    for k, v in ymap.items():\n",
    "        if not isinstance(v, dict):\n",
    "            continue\n",
    "        if \"tokens\" in v and isinstance(v[\"tokens\"], list):\n",
    "            toks = [str(t).strip().upper() for t in v[\"tokens\"] if str(t).strip()]\n",
    "            if toks:\n",
    "                token_buckets.append({\n",
    "                    \"bucket\": str(k),\n",
    "                    \"tokens\": toks,\n",
    "                    \"category\": v.get(\"category\"),\n",
    "                    \"subcategory\": v.get(\"subcategory\"),\n",
    "                    \"display_name\": v.get(\"display_name\"),\n",
    "                    \"tags\": v.get(\"tags\"),\n",
    "                })\n",
    "        else:\n",
    "            exact_map[k] = v\n",
    "\n",
    "    exact_map_norm = _normalize_yaml_keys(exact_map)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in out.iterrows():\n",
    "        mk = str(r.get(\"merchant_key\") or \"\").upper()\n",
    "\n",
    "        # 1) Exact match\n",
    "        m = exact_map_norm.get(mk)\n",
    "        if m:\n",
    "            disp = m.get(\"display_name\") or r.get(\"merchant_name\") or r.get(\"name\") or mk\n",
    "            cat  = m.get(\"category\")\n",
    "            sub  = m.get(\"subcategory\")\n",
    "            tags = m.get(\"tags\")\n",
    "            if isinstance(tags, (list, tuple)): tags = \",\".join(str(t) for t in tags)\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": disp,\n",
    "                \"category_final\": cat,\n",
    "                \"subcategory_final\": sub,\n",
    "                \"tags_final\": tags,\n",
    "                \"confidence_final\": m.get(\"confidence\", \"map\"),\n",
    "                \"source_final\": \"yaml\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 2) Token buckets (first hit wins)\n",
    "        hit = None\n",
    "        for b in token_buckets:\n",
    "            if any(t in mk for t in b[\"tokens\"]):\n",
    "                hit = b; break\n",
    "\n",
    "        if hit:\n",
    "            disp = hit.get(\"display_name\") or r.get(\"merchant_name\") or r.get(\"name\") or mk\n",
    "            tags = hit.get(\"tags\")\n",
    "            if isinstance(tags, (list, tuple)): tags = \",\".join(str(t) for t in tags)\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": disp,\n",
    "                \"category_final\": hit.get(\"category\"),\n",
    "                \"subcategory_final\": hit.get(\"subcategory\"),\n",
    "                \"tags_final\": tags,\n",
    "                \"confidence_final\": \"bucket\",\n",
    "                \"source_final\": \"yaml-token\",\n",
    "            })\n",
    "        else:\n",
    "            # raw passthrough\n",
    "            rows.append({**r,\n",
    "                \"display_name_final\": r.get(\"display_name_final\") or r.get(\"merchant_name\") or r.get(\"name\"),\n",
    "                \"category_final\": r.get(\"category_final\"),\n",
    "                \"subcategory_final\": r.get(\"subcategory_final\"),\n",
    "                \"tags_final\": r.get(\"tags_final\"),\n",
    "                \"confidence_final\": r.get(\"confidence_final\") or \"raw\",\n",
    "                \"source_final\": r.get(\"source_final\") or \"raw\",\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def mark_non_spend_flows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    pats = [\n",
    "        r\"PAYMENT\", r\"TRANSFER\", r\"DIRECTPAY\", r\"CREDIT\",\n",
    "        r\"REFUND\", r\"REIMBURSE\", r\"ADJUSTMENT\", r\"REVERSAL\",\n",
    "        r\"ACH(?!.*APPLE\\s+CASH)\",\n",
    "        r\"WEALTHFRONT\"\n",
    "    ]\n",
    "    pat = re.compile(\"|\".join(pats))\n",
    "    names = (df.get(\"name\", pd.Series(\"\", index=df.index)).fillna(\"\") + \" \" +\n",
    "             df.get(\"merchant_name\", pd.Series(\"\", index=df.index)).fillna(\"\")).str.upper()\n",
    "    out = df.copy()\n",
    "    out[\"is_non_spend_flow\"] = names.str.contains(pat)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6e3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DIAG 2] YAML top-level keys: ['version', 'defaults', 'categories', 'non_spend_categories', 'necessity_flags', 'mapping']\n",
      "[DIAG 2] Has nested 'mapping' block: True\n",
      "[DIAG 2] mapping categories (first 20): ['dining', 'groceries', 'shopping', 'transportation', 'travel', 'auto_service', 'health_wellness', 'fitness_sports', 'government_fees', 'credit_card_payment', 'refunds_income']\n",
      "\n",
      "[DIAG 2] token counts per category (first 10):\n",
      " - dining: 9 tokens\n",
      " - groceries: 6 tokens\n",
      " - shopping: 10 tokens\n",
      " - transportation: 5 tokens\n",
      " - travel: 2 tokens\n",
      " - auto_service: 3 tokens\n",
      " - health_wellness: 3 tokens\n",
      " - fitness_sports: 3 tokens\n",
      " - government_fees: 6 tokens\n",
      " - credit_card_payment: 8 tokens\n",
      "\n",
      "[DIAG 2] Top token→merchant_key matches (first 20):\n",
      "  ('shopping', 'TARGET', 2)\n",
      "  ('auto_service', 'FLETCHER JONES', 1)\n",
      "  ('credit_card_payment', 'CASHBACK BONUS REDEMPTION', 1)\n",
      "  ('credit_card_payment', 'PAYMENT THANK', 1)\n",
      "  ('fitness_sports', 'JIU JITSU', 1)\n",
      "  ('fitness_sports', 'SPECTATION SPORTS', 1)\n",
      "  ('government_fees', 'BULLET LEGAL SERVI', 1)\n",
      "  ('government_fees', 'DMV', 1)\n",
      "  ('government_fees', 'SOUTHERN NV HEALTH DIST', 1)\n",
      "  ('refunds_income', 'CASHBACK BONUS REDEMPTION', 1)\n",
      "  ('transportation', 'ARCO', 1)\n",
      "  ('transportation', 'S&S', 1)\n",
      "Wrote → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\debug_yaml_token_hits.csv\n",
      "YAML mapping hits: 0\n",
      "Examples of YAML-mapped rows:\n",
      "Empty DataFrame\n",
      "Columns: [merchant_key, display_name, category, subcategory, tags]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb — Cell 6 (REVISED) ---\n",
    "# Build merchant_key\n",
    "combined[\"merchant_key\"] = combined[\"merchant_name\"].where(\n",
    "    combined[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    combined[\"name\"]\n",
    ").map(merchant_key_from)\n",
    "\n",
    "# Load YAML (repo_root/config/categories.yaml)\n",
    "PATH_YAML = (repo_root / \"config\" / \"categories.yaml\")\n",
    "ymap_obj = {}\n",
    "if PATH_YAML.exists():\n",
    "    try:\n",
    "        with open(PATH_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "            ymap_obj = yaml.safe_load(f) or {}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ YAML parse error: {e}\")\n",
    "        \n",
    "# === DIAG 2: Inspect YAML structure & see whether any tokens would match merchant_key ===\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"\\n[DIAG 2] YAML top-level keys:\", list((ymap_obj or {}).keys()))\n",
    "mapping_block = None\n",
    "for k in (\"mapping\",\"MAPPING\"):\n",
    "    if isinstance((ymap_obj or {}).get(k), dict):\n",
    "        mapping_block = ymap_obj[k]\n",
    "        break\n",
    "print(\"[DIAG 2] Has nested 'mapping' block:\", isinstance(mapping_block, dict))\n",
    "if isinstance(mapping_block, dict):\n",
    "    print(\"[DIAG 2] mapping categories (first 20):\", list(mapping_block.keys())[:20])\n",
    "\n",
    "def _strings_in(obj):\n",
    "    out=[]\n",
    "    if isinstance(obj, str): out.append(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        for x in obj: out.extend(_strings_in(x))\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values(): out.extend(_strings_in(v))\n",
    "    return out\n",
    "\n",
    "cat_tokens = {}\n",
    "if isinstance(mapping_block, dict):\n",
    "    for cat, node in mapping_block.items():\n",
    "        toks = [s for s in _strings_in(node) if isinstance(s, str)]\n",
    "        cat_tokens[cat] = [t for t in toks if len(str(t).strip()) >= 3]\n",
    "\n",
    "print(\"\\n[DIAG 2] token counts per category (first 10):\")\n",
    "for cat in list(cat_tokens.keys())[:10]:\n",
    "    print(f\" - {cat}: {len(cat_tokens[cat])} tokens\")\n",
    "\n",
    "mks = combined[\"merchant_key\"].astype(str).str.upper().unique() if \"merchant_key\" in combined.columns else []\n",
    "hits = []\n",
    "for cat, toks in cat_tokens.items():\n",
    "    for tok in toks:\n",
    "        T = str(tok).upper().strip()\n",
    "        if not T: continue\n",
    "        count = sum(1 for mk in mks if T in mk)\n",
    "        if count:\n",
    "            hits.append((cat, T, count))\n",
    "\n",
    "hits = sorted(hits, key=lambda x: (-x[2], x[0], x[1]))\n",
    "print(\"\\n[DIAG 2] Top token→merchant_key matches (first 20):\")\n",
    "for row in hits[:20]:\n",
    "    print(\" \", row)\n",
    "\n",
    "DEBUG_DIR = (repo_root / \"data\" / \"processed\")\n",
    "DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame(hits, columns=[\"category\",\"token\",\"merchant_key_hits\"]).to_csv(DEBUG_DIR / \"debug_yaml_token_hits.csv\", index=False)\n",
    "print(\"Wrote →\", DEBUG_DIR / \"debug_yaml_token_hits.csv\")\n",
    "\n",
    "# Apply mapping + non-spend\n",
    "enriched = apply_yaml_mapping(combined, ymap_obj)\n",
    "enriched = mark_non_spend_flows(enriched)\n",
    "\n",
    "# COALESCE: canonical columns for downstream visuals\n",
    "enriched[\"display_name\"] = (\n",
    "    enriched.get(\"display_name_final\")\n",
    "            .fillna(enriched.get(\"merchant_name\"))\n",
    "            .fillna(enriched.get(\"name\"))\n",
    ")\n",
    "enriched[\"category\"]    = enriched.get(\"category_final\").fillna(enriched.get(\"category\"))\n",
    "enriched[\"subcategory\"] = enriched.get(\"subcategory_final\")\n",
    "enriched[\"tags\"]        = enriched.get(\"tags_final\").fillna(\"\")\n",
    "\n",
    "# Final export columns (canonical first; keep finals for debug)\n",
    "cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"merchant_key\",\n",
    "    \"display_name\",\"category\",\"subcategory\",\"tags\",\n",
    "    \"amount\",\"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\",\n",
    "    \"is_non_spend_flow\",\n",
    "    \"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\n",
    "    \"confidence_final\",\"source_final\"\n",
    "]\n",
    "for c in cols:\n",
    "    if c not in enriched.columns:\n",
    "        enriched[c] = None\n",
    "enriched = enriched[cols].copy()\n",
    "\n",
    "# Types\n",
    "enriched[\"date\"] = pd.to_datetime(enriched[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Quick visibility on YAML usage\n",
    "print(\"YAML mapping hits:\", int((enriched[\"source_final\"] == \"yaml\").sum()))\n",
    "print(\"Examples of YAML-mapped rows:\")\n",
    "print(enriched.loc[enriched[\"source_final\"] == \"yaml\",\n",
    "                   [\"merchant_key\",\"display_name\",\"category\",\"subcategory\",\"tags\"]].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114c6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest CSV saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv  rows=156\n",
      "\n",
      "Preview (top 10):\n",
      "      date                                                                                                                            name merchant_name                                                                       merchant_key                                                                                                                    display_name      category subcategory tags   amount payment_channel  pending  account_id  transaction_id                         bank_name                         card_name  is_non_spend_flow                                                                                                              display_name_final category_final subcategory_final tags_final confidence_final source_final\n",
      "2025-09-16 Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000023255802               WITHDRAWAL ALLY / TYPE ALLY PAYMT ID CO ALLY NAME KOSISONNA UGOCHUKW ECC WEB TRACE Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000023255802 Uncategorized        None        485.78                      NaN         NaN             NaN Silver State Schools Credit Union Silver State Schools Credit Union               True Withdrawal ALLY / TYPE: ALLY PAYMT ID: 9833122002 CO: ALLY NAME: Kosisonna Ugochukw %% ACH ECC WEB %% ACH Trace 021000023255802           None              None       None              raw          raw\n",
      "2025-09-16       Withdrawal Petal Card, Inc TYPE: PAYMENTS ID: 8565523001 CO: Petal Card, Inc. %% ACH ECC PPD %% ACH Trace 021000021953967         Petal                                                                              PETAL                                                                                                                           Petal Debt Payments        None        712.96                      NaN         NaN             NaN Silver State Schools Credit Union Silver State Schools Credit Union               True                                                                                                                           Petal           None              None       None              raw          raw\n",
      "2025-09-15                                                                                        CASHBACK BONUS REDEMPTION PYMT/STMT CRDT                                                         CASHBACK BONUS REDEMPTION PYMT/STMT CRDT                                                                                        CASHBACK BONUS REDEMPTION PYMT/STMT CRDT     Transfers        None        -22.65                      NaN         NaN             NaN                          Discover                          Discover              False                                                                                        CASHBACK BONUS REDEMPTION PYMT/STMT CRDT           None              None       None              raw          raw\n",
      "2025-09-15                                                                                              POS Signature Purchase using Apple                                                                            SIGNATURE USING APPLE                                                                                              POS Signature Purchase using Apple      Shopping        None         16.69                      NaN         NaN             NaN                             Petal                             Petal              False                                                                                              POS Signature Purchase using Apple           None              None       None              raw          raw\n",
      "2025-09-15                                 Deposit Kiosk / WEALTHFRONT BROKERAGE LLC/WELLS FARGO BANK, NA/75f7133df98441c1be9cff39c3a57f1a   Wealthfront                                                                        WEALTHFRONT                                                                                                                     Wealthfront     Transfers        None      -1000.00                      NaN         NaN             NaN Silver State Schools Credit Union Silver State Schools Credit Union               True                                                                                                                     Wealthfront           None              None       None              raw          raw\n",
      "2025-09-15                                                                                                    INTERNET PAYMENT - THANK YOU                                                                       INTERNET PAYMENT THANK YOU                                                                                                    INTERNET PAYMENT - THANK YOU Debt Payments        None       -527.57                      NaN         NaN             NaN                          Discover                          Discover               True                                                                                                    INTERNET PAYMENT - THANK YOU           None              None       None              raw          raw\n",
      "2025-09-15                                                                                            ACH Payment from account ending 6212                                                                      PAYMENT FROM ACCOUNT ENDING                                                                                            ACH Payment from account ending 6212 Debt Payments        None       -712.96                      NaN         NaN             NaN                             Petal                             Petal               True                                                                                            ACH Payment from account ending 6212           None              None       None              raw          raw\n",
      "2025-09-14                                                                                              POS Signature Purchase using Token                                                                            SIGNATURE USING TOKEN                                                                                              POS Signature Purchase using Token     Transfers        None          5.99                      NaN         NaN             NaN                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "2025-09-14                                                                                              POS Signature Purchase using Apple                                                                            SIGNATURE USING APPLE                                                                                              POS Signature Purchase using Apple      Shopping        None         11.06                      NaN         NaN             NaN                             Petal                             Petal              False                                                                                              POS Signature Purchase using Apple           None              None       None              raw          raw\n",
      "2025-09-13                                                                                              POS Signature Purchase using Token                                                                            SIGNATURE USING TOKEN                                                                                              POS Signature Purchase using Token     Transfers        None         12.90                      NaN         NaN             NaN                             Petal                             Petal              False                                                                                              POS Signature Purchase using Token           None              None       None              raw          raw\n",
      "\n",
      "Value counts — source_final:\n",
      "source_final\n",
      "raw    156\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- build_latest.ipynb — Cell 7 ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "latest_path = OUTPUT_DIR / \"latest.csv\"\n",
    "\n",
    "# Write enriched (canonical columns included)\n",
    "enriched.to_csv(latest_path, index=False)\n",
    "\n",
    "assert latest_path.exists(), \"latest.csv was not written.\"\n",
    "assert \"bank_name\" in enriched.columns, \"bank_name column missing.\"\n",
    "assert \"card_name\" in enriched.columns, \"card_name column missing.\"\n",
    "\n",
    "print(f\"✅ Latest CSV saved → {latest_path}  rows={len(enriched):,}\")\n",
    "try:\n",
    "    print(\"\\nPreview (top 10):\")\n",
    "    print(enriched.head(10).to_string(index=False))\n",
    "    print(\"\\nValue counts — source_final:\")\n",
    "    print(enriched[\"source_final\"].value_counts(dropna=False).head(10))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
