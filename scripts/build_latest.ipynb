{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d990ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env OK → PLAID_CLIENT_ID: 68bb…6689 | PLAID_SECRET: a605…7df5 | PLAID_ENV: production | OUTPUT_DIR: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw | TOKENS_PATH: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\.state\\access_tokens.json\n",
      "Loaded 3 token(s).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Env, paths, .env loader, tokens (robust) ---\n",
    "import os, json, re, time\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Optional dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "    find_dotenv = None\n",
    "\n",
    "def mask(s: str | None) -> str:\n",
    "    if not s: return \"<missing>\"\n",
    "    return (s[:4] + \"…\" + s[-4:]) if len(s) > 8 else \"***\"\n",
    "\n",
    "# Resolve repo root (works from / or /scripts)\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next((p for p in [cwd, *cwd.parents] if (p / \".git\").exists() or p.name == \"spending-dashboard\"), cwd)\n",
    "\n",
    "# ✅ CI-safe override: prefer GitHub workspace path if present\n",
    "gw = os.getenv(\"GITHUB_WORKSPACE\")\n",
    "if gw:\n",
    "    repo_root = Path(gw).resolve()\n",
    "\n",
    "# Load .env if present (scripts/.env preferred)\n",
    "def load_envs():\n",
    "    if load_dotenv is None:\n",
    "        return\n",
    "    abs_override = os.getenv(\"ENV_PATH\", str(repo_root / \"scripts\" / \".env\"))\n",
    "    if abs_override and Path(abs_override).exists():\n",
    "        try:\n",
    "            load_dotenv(abs_override, override=False, encoding=\"utf-8\")\n",
    "        except TypeError:\n",
    "            load_dotenv(abs_override, override=False)\n",
    "    for p in [\n",
    "        repo_root / \"scripts\" / \".env\",\n",
    "        repo_root / \".env\",\n",
    "        repo_root / \"config\" / \".env\",\n",
    "        cwd / \".env\",\n",
    "    ]:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                load_dotenv(str(p), override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(str(p), override=False)\n",
    "    if find_dotenv:\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        if found:\n",
    "            try:\n",
    "                load_dotenv(found, override=False, encoding=\"utf-8\")\n",
    "            except TypeError:\n",
    "                load_dotenv(found, override=False)\n",
    "\n",
    "load_envs()\n",
    "\n",
    "# Normalize env\n",
    "PLAID_CLIENT_ID = os.getenv(\"PLAID_CLIENT_ID\")\n",
    "PLAID_SECRET    = os.getenv(\"PLAID_SECRET\")\n",
    "PLAID_ENV       = (os.getenv(\"PLAID_ENV\", \"production\") or \"production\").strip().lower()\n",
    "alias = {\"prod\":\"production\",\"live\":\"production\",\"dev\":\"development\",\"devel\":\"development\",\"sb\":\"sandbox\"}\n",
    "PLAID_ENV = alias.get(PLAID_ENV, PLAID_ENV)\n",
    "if PLAID_ENV not in {\"production\",\"development\",\"sandbox\"}:\n",
    "    PLAID_ENV = \"production\"\n",
    "\n",
    "# Paths (env-overridable)\n",
    "OUTPUT_DIR = Path(os.getenv(\"OUTPUT_DIR\", str(repo_root / \"data\" / \"raw\")))\n",
    "STATE_DIR  = Path(os.getenv(\"STATE_DIR\",  str(repo_root / \".state\")))\n",
    "TOKENS_PATH = Path(os.getenv(\"TOKENS_PATH\", str(STATE_DIR / \"access_tokens.json\")))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load & validate access tokens (env > file), canonical-only ---\n",
    "def _strip_bom(s: str) -> str:\n",
    "    return s.lstrip(\"\\ufeff\") if isinstance(s, str) else s\n",
    "\n",
    "def _parse_pairs_blob(blob: str) -> dict:\n",
    "    raw = [p.strip() for sep in [\"\\n\",\";\",\"|\",\",\"] for p in (blob.split(sep) if sep in blob else []) if p.strip()]\n",
    "    if not raw: raw = [blob.strip()]\n",
    "    out = {}\n",
    "    for p in raw:\n",
    "        if \"=\" in p:\n",
    "            k, v = p.split(\"=\", 1)\n",
    "        elif \":\" in p:\n",
    "            k, v = p.split(\":\", 1)\n",
    "        else:\n",
    "            continue\n",
    "        k = k.strip().strip('\"').strip(\"'\")\n",
    "        v = v.strip().strip('\"').strip(\"'\")\n",
    "        if k and v:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def _normalize_tokens(obj) -> dict:\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): str(v).strip() for k,v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        out = {}\n",
    "        for item in obj:\n",
    "            if isinstance(item, dict):\n",
    "                name = item.get(\"issuer\") or item.get(\"bank\") or item.get(\"name\")\n",
    "                token = item.get(\"access_token\") or item.get(\"token\")\n",
    "                if name and token:\n",
    "                    out[str(name)] = str(token).strip()\n",
    "        return out\n",
    "    if isinstance(obj, str):\n",
    "        s = _strip_bom(obj).strip()\n",
    "        try:\n",
    "            parsed = json.loads(s)    # JSON first\n",
    "            return _normalize_tokens(parsed)\n",
    "        except Exception:\n",
    "            return _parse_pairs_blob(s)\n",
    "    return {}\n",
    "\n",
    "def load_access_tokens():\n",
    "    blob = os.getenv(\"PLAID_ACCESS_TOKENS\", \"\").strip()\n",
    "    if blob:\n",
    "        tokens = _normalize_tokens(blob)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    if TOKENS_PATH.exists():\n",
    "        raw = TOKENS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        tokens = _normalize_tokens(raw)\n",
    "        if tokens:\n",
    "            return tokens\n",
    "    raise AssertionError(\n",
    "        f\"Could not load access tokens. Provide PLAID_ACCESS_TOKENS env or a valid JSON at {TOKENS_PATH}.\"\n",
    "    )\n",
    "\n",
    "ACCESS_TOKENS = load_access_tokens()\n",
    "\n",
    "PAT = re.compile(r\"^access-(?:production|development|sandbox)-[a-z0-9\\-]+$\")\n",
    "expected_prefix = f\"access-{PLAID_ENV}-\"\n",
    "bad = [k for k,v in ACCESS_TOKENS.items() if not isinstance(v, str) or not v.startswith(expected_prefix) or not PAT.match(v)]\n",
    "assert not bad, f\"Non-canonical or wrong-env tokens for: {bad}. Ensure tokens look like '{expected_prefix}…' (no '/', '+', '=').\"\n",
    "\n",
    "print(\n",
    "    \"Env OK →\",\n",
    "    \"PLAID_CLIENT_ID:\", mask(PLAID_CLIENT_ID),\n",
    "    \"| PLAID_SECRET:\", mask(PLAID_SECRET),\n",
    "    \"| PLAID_ENV:\", PLAID_ENV,\n",
    "    \"| OUTPUT_DIR:\", str(OUTPUT_DIR),\n",
    "    \"| TOKENS_PATH:\", str(TOKENS_PATH),\n",
    ")\n",
    "print(f\"Loaded {len(ACCESS_TOKENS)} token(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a22fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaid SDK: v10+ (plaid_api)\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Plaid client init (v10+ preferred, legacy fallback) ---\n",
    "USE_PLAID_V10 = False\n",
    "client = None\n",
    "\n",
    "try:\n",
    "    # v10+ path\n",
    "    from plaid.api import plaid_api\n",
    "    from plaid.configuration import Configuration\n",
    "    try:\n",
    "        from plaid.configuration import Environment  # newer enum\n",
    "        env_host = {\n",
    "            \"production\":  Environment.Production,\n",
    "            \"development\": Environment.Development,\n",
    "            \"sandbox\":     Environment.Sandbox,\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=env_host)\n",
    "    except Exception:\n",
    "        # fallback if Environment enum not present\n",
    "        host_url = {\n",
    "            \"production\":  \"https://production.plaid.com\",\n",
    "            \"development\": \"https://development.plaid.com\",\n",
    "            \"sandbox\":     \"https://sandbox.plaid.com\",\n",
    "        }[PLAID_ENV]\n",
    "        config = Configuration(host=host_url)\n",
    "\n",
    "    from plaid.api_client import ApiClient\n",
    "    config.api_key[\"clientId\"] = PLAID_CLIENT_ID\n",
    "    config.api_key[\"secret\"]   = PLAID_SECRET\n",
    "    api_client = ApiClient(config)\n",
    "    client = plaid_api.PlaidApi(api_client)\n",
    "    USE_PLAID_V10 = True\n",
    "    print(\"Plaid SDK: v10+ (plaid_api)\")\n",
    "except Exception as e_v10:\n",
    "    try:\n",
    "        # legacy path\n",
    "        from plaid import Client as LegacyClient\n",
    "        client = LegacyClient(\n",
    "            client_id=PLAID_CLIENT_ID,\n",
    "            secret=PLAID_SECRET,\n",
    "            environment=PLAID_ENV\n",
    "        )\n",
    "        USE_PLAID_V10 = False\n",
    "        print(\"Plaid SDK: legacy Client()\")\n",
    "    except Exception as e_legacy:\n",
    "        raise ImportError(\n",
    "            \"Could not initialize Plaid client. Ensure 'plaid-python' is installed. \"\n",
    "            f\"v10 error: {e_v10}\\nlegacy error: {e_legacy}\"\n",
    "        )\n",
    "\n",
    "# Optional quick probe (set PRECHECK=1 to enable)\n",
    "if os.getenv(\"PRECHECK\", \"0\") == \"1\" and USE_PLAID_V10:\n",
    "    from plaid.model.accounts_get_request import AccountsGetRequest\n",
    "    from plaid.api_client import ApiException\n",
    "    for issuer, tok in ACCESS_TOKENS.items():\n",
    "        try:\n",
    "            n = len(client.accounts_get(AccountsGetRequest(access_token=tok)).to_dict().get(\"accounts\", []))\n",
    "            print(f\"{issuer}: ✅ accounts_get OK ({n} accounts)\")\n",
    "        except ApiException as e:\n",
    "            print(f\"{issuer}: ❌ API {e.status} -> {getattr(e, 'body', e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dd6aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 SYNC Discover (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Petal (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "🔄 SYNC Silver State Schools Credit Union (start: cursor-present)\n",
      "   → added=0, modified=0, removed=0, next_cursor=set\n",
      "✅ Consolidated using SYNC → rows=0 across 3 bank(s)\n",
      "Window: 2025-03-16 → 2025-09-12 | DAYS_BACK=180\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Pull & consolidate via /transactions/sync (fallback to GET) ---\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Rolling window (change 90 → 180/365 if you want a bigger window)\n",
    "DAYS_BACK = int(os.getenv(\"DAYS_BACK\", \"180\"))\n",
    "end_date = date.today()\n",
    "start_date = end_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "CURSORS_PATH = STATE_DIR / \"plaid_cursors.json\"\n",
    "\n",
    "def load_cursors() -> dict:\n",
    "    if CURSORS_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(CURSORS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def save_cursors(cur: dict):\n",
    "    CURSORS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    CURSORS_PATH.write_text(json.dumps(cur, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def normalize_category(x):\n",
    "    return \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "\n",
    "# Convert raw txn dicts to our normalized schema\n",
    "def df_from_txns(txns: list[dict], bank_name: str) -> pd.DataFrame:\n",
    "    if not txns:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(txns)\n",
    "\n",
    "    # Ensure expected columns exist\n",
    "    expected_cols = [\n",
    "        \"name\",\"merchant_name\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"category\",\"date\",\"amount\"\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Normalize\n",
    "    df[\"category\"] = df[\"category\"].apply(normalize_category)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    # Bank & card\n",
    "    df[\"bank_name\"] = bank_name\n",
    "    df[\"card_name\"] = bank_name  # upgrade later via accounts_dim\n",
    "\n",
    "    keep_cols = [\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\"\n",
    "    ]\n",
    "    return df[[c for c in keep_cols if c in df.columns]].copy()\n",
    "\n",
    "# Read previous latest.csv (acts as our on-repo cache)\n",
    "latest_csv_path = (repo_root / \"data\" / \"raw\" / \"latest.csv\")\n",
    "prev = pd.DataFrame()\n",
    "if latest_csv_path.exists():\n",
    "    try:\n",
    "        prev = pd.read_csv(latest_csv_path)\n",
    "        prev[\"date\"] = pd.to_datetime(prev[\"date\"], errors=\"coerce\")\n",
    "        # Ensure we always have the column even if an older file omitted it\n",
    "        if \"transaction_id\" not in prev.columns:\n",
    "            prev[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read previous latest.csv: {e}\")\n",
    "\n",
    "cursors = load_cursors()\n",
    "all_added_mod = []   # list of DataFrames of added/modified across banks\n",
    "all_removed_ids = set()\n",
    "\n",
    "if 'USE_PLAID_V10' in globals() and USE_PLAID_V10:\n",
    "    # --- Sync-first path using Plaid v10 client ---\n",
    "    from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "\n",
    "    def sync_one(bank_name: str, access_token: str, cursor: str | None):\n",
    "        \"\"\"Call /transactions/sync until has_more is False. Omit 'cursor' on first call.\"\"\"\n",
    "        added, modified, removed_ids = [], [], []\n",
    "        next_cursor = cursor\n",
    "        while True:\n",
    "            req_kwargs = {\"access_token\": access_token, \"count\": 500}\n",
    "            if isinstance(next_cursor, str) and next_cursor:\n",
    "                req_kwargs[\"cursor\"] = next_cursor\n",
    "            req = TransactionsSyncRequest(**req_kwargs)\n",
    "\n",
    "            resp = client.transactions_sync(req).to_dict()\n",
    "            added.extend(resp.get(\"added\", []) or [])\n",
    "            modified.extend(resp.get(\"modified\", []) or [])\n",
    "\n",
    "            # removed may be list[str] or list[dict]\n",
    "            rem = resp.get(\"removed\", []) or []\n",
    "            for r in rem:\n",
    "                if isinstance(r, dict):\n",
    "                    rid = r.get(\"transaction_id\")\n",
    "                    if rid: removed_ids.append(rid)\n",
    "                elif isinstance(r, str):\n",
    "                    removed_ids.append(r)\n",
    "\n",
    "            next_cursor = resp.get(\"next_cursor\", next_cursor)\n",
    "            if not resp.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "        return (next_cursor if isinstance(next_cursor, str) and next_cursor else None,\n",
    "                added, modified, removed_ids)\n",
    "\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 SYNC {bank_name} (start: {'cursor-present' if cursors.get(token) else 'no-cursor'})\")\n",
    "        cur0 = cursors.get(token)\n",
    "        next_cur, added, modified, removed_ids = sync_one(bank_name, token, cur0)\n",
    "\n",
    "        df_add = df_from_txns(added, bank_name)\n",
    "        df_mod = df_from_txns(modified, bank_name)\n",
    "        all_added_mod.append(df_add)\n",
    "        all_added_mod.append(df_mod)\n",
    "        all_removed_ids.update(removed_ids)\n",
    "\n",
    "        cursors[token] = next_cur\n",
    "        print(f\"   → added={len(df_add):,}, modified={len(df_mod):,}, removed={len(removed_ids):,}, next_cursor={'set' if next_cur else 'None'}\")\n",
    "\n",
    "    # Start from previous CSV and apply deltas\n",
    "    cur = prev.copy()\n",
    "\n",
    "    # Remove deleted transaction_ids (guard if column is present)\n",
    "    if not cur.empty and all_removed_ids and \"transaction_id\" in cur.columns:\n",
    "        cur = cur[~cur[\"transaction_id\"].astype(str).isin({str(x) for x in all_removed_ids})]\n",
    "    elif not cur.empty and all_removed_ids:\n",
    "        print(\"ℹ️ Skipping delete-apply: previous cache lacks 'transaction_id' column.\")\n",
    "\n",
    "    # Replace modified ids and add new ones\n",
    "    if any(len(x) for x in all_added_mod):\n",
    "        new_mod = (pd.concat([df for df in all_added_mod if not df.empty], ignore_index=True)\n",
    "                   if all_added_mod else pd.DataFrame())\n",
    "        if not new_mod.empty and \"transaction_id\" not in new_mod.columns:\n",
    "            new_mod[\"transaction_id\"] = pd.Series(dtype=object)\n",
    "\n",
    "        if not cur.empty and \"transaction_id\" in cur.columns and \"transaction_id\" in new_mod.columns:\n",
    "            mod_ids = set(new_mod[\"transaction_id\"].dropna().astype(str).tolist())\n",
    "            if mod_ids:\n",
    "                cur = cur[~cur[\"transaction_id\"].astype(str).isin(mod_ids)]\n",
    "        else:\n",
    "            if not cur.empty and not new_mod.empty:\n",
    "                print(\"ℹ️ Skipping modify-replace: one frame lacks 'transaction_id'; appending only.\")\n",
    "\n",
    "        cur = pd.concat([cur, new_mod], ignore_index=True) if not new_mod.empty else cur\n",
    "\n",
    "    combined = cur.copy()\n",
    "\n",
    "else:\n",
    "    # --- Fallback: windowed GET per item (legacy client) ---\n",
    "    def fetch_transactions_get(bank_name: str, access_token: str) -> pd.DataFrame:\n",
    "        txns = []\n",
    "        offset = 0\n",
    "        while True:\n",
    "            resp = client.Transactions.get(\n",
    "                access_token=access_token,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                options={\"count\": 500, \"offset\": offset}\n",
    "            )\n",
    "            total = resp[\"total_transactions\"]\n",
    "            txns.extend(resp[\"transactions\"])\n",
    "            if len(txns) >= total:\n",
    "                break\n",
    "            offset = len(txns)\n",
    "            if offset > 50_000:\n",
    "                raise RuntimeError(f\"Pagination runaway for {bank_name}\")\n",
    "        return df_from_txns(txns, bank_name)\n",
    "\n",
    "    frames = []\n",
    "    for bank_name, token in ACCESS_TOKENS.items():\n",
    "        print(f\"🔄 GET {bank_name} ({start_date} → {end_date})…\")\n",
    "        frames.append(fetch_transactions_get(bank_name, token))\n",
    "    combined = pd.concat([f for f in frames if f is not None and not f.empty], ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# Finalize for both paths: trim to window, sort, dedupe by transaction_id if present\n",
    "if combined is None or combined.empty:\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "        \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "        \"bank_name\",\"card_name\"\n",
    "    ])\n",
    "else:\n",
    "    combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "    combined = combined[\n",
    "        (combined[\"date\"] >= pd.Timestamp(start_date)) &\n",
    "        (combined[\"date\"] <= pd.Timestamp(end_date))\n",
    "    ]\n",
    "    if \"transaction_id\" in combined.columns:\n",
    "        combined = (combined\n",
    "                    .sort_values(\"date\", ascending=False)\n",
    "                    .drop_duplicates(subset=[\"transaction_id\"], keep=\"first\")\n",
    "                    .reset_index(drop=True))\n",
    "    else:\n",
    "        combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save cursors (persist locally; CI keeps it in the workspace unless you choose to cache/commit)\n",
    "try:\n",
    "    save_cursors(cursors)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save cursors: {e}\")\n",
    "\n",
    "print(f\"✅ Consolidated using {'SYNC' if 'USE_PLAID_V10' in globals() and USE_PLAID_V10 else 'GET'} → rows={len(combined):,} across {len(ACCESS_TOKENS)} bank(s)\")\n",
    "print(f\"Window: {start_date} → {end_date} | DAYS_BACK={DAYS_BACK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e7e92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Clean -> normalize schema ---\n",
    "if combined.empty:\n",
    "    # Create an empty but well-typed frame to keep Power BI stable\n",
    "    combined = pd.DataFrame(columns=[\n",
    "        \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\"payment_channel\",\"pending\",\n",
    "        \"account_id\",\"transaction_id\",\"bank_name\",\"card_name\"\n",
    "    ])\n",
    "\n",
    "# Normalize category: Plaid sometimes returns list; make it a short string\n",
    "if \"category\" in combined.columns:\n",
    "    combined[\"category\"] = combined[\"category\"].apply(\n",
    "        lambda x: \" > \".join(x) if isinstance(x, (list, tuple)) else x\n",
    "    )\n",
    "\n",
    "# Ensure date type\n",
    "combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only expected columns (but don’t error if some are missing)\n",
    "keep_cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"category\",\"amount\",\n",
    "    \"payment_channel\",\"pending\",\"account_id\",\"transaction_id\",\n",
    "    \"bank_name\",\"card_name\"\n",
    "]\n",
    "combined = combined[[c for c in keep_cols if c in combined.columns]].copy()\n",
    "\n",
    "# Sort newest first\n",
    "combined = combined.sort_values(\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Fill minimal NA for downstream friendliness\n",
    "for c in [\"name\",\"merchant_name\",\"category\",\"payment_channel\",\"bank_name\",\"card_name\"]:\n",
    "    if c in combined.columns:\n",
    "        combined[c] = combined[c].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e16b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: YAML helpers (merchant key, mapping, non-spend) ---\n",
    "from pathlib import Path\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def merchant_key_from(name: str) -> str:\n",
    "    s = (name or \"\").upper()\n",
    "    s = re.sub(r\"APPLE PAY ENDING IN \\d{4}\", \"\", s)\n",
    "    s = re.sub(r\"#\\d{2,}\", \"\", s)              # strip store numbers like #1234\n",
    "    s = re.sub(r\"\\d+\", \"\", s)                  # kill stray digits\n",
    "    s = re.sub(r\"[^A-Z&\\s]\", \" \", s)           # keep letters, ampersand, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def apply_yaml_mapping(df: pd.DataFrame, ymap: dict) -> pd.DataFrame:\n",
    "    if not ymap or df.empty:\n",
    "        # Still add standard columns so schema is stable\n",
    "        out = df.copy()\n",
    "        for c in [\"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\"confidence_final\",\"source_final\"]:\n",
    "            if c not in out.columns:\n",
    "                out[c] = None\n",
    "        out[\"source_final\"] = out[\"source_final\"].fillna(\"raw\")\n",
    "        out[\"confidence_final\"] = out[\"confidence_final\"].fillna(\"raw\")\n",
    "        return out\n",
    "\n",
    "    look = {k.upper(): v for k, v in ymap.items()}\n",
    "    rows = []\n",
    "    # (kept for clarity; merge-based vectorization is overkill at this size)\n",
    "    for _, r in df.iterrows():\n",
    "        mk = r.get(\"merchant_key\", \"\")\n",
    "        m = look.get(mk, {})\n",
    "        rows.append({\n",
    "            **r,\n",
    "            \"display_name_final\": m.get(\"display_name\", r.get(\"merchant_name\") or r.get(\"name\")),\n",
    "            \"category_final\":     m.get(\"category\"),\n",
    "            \"subcategory_final\":  m.get(\"subcategory\"),\n",
    "            \"tags_final\":         \",\".join(m.get(\"tags\", [])) if isinstance(m.get(\"tags\", []), (list, tuple)) else m.get(\"tags\"),\n",
    "            \"confidence_final\":   m.get(\"confidence\", \"map\"),\n",
    "            \"source_final\":       \"yaml\" if m else \"raw\"\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def mark_non_spend_flows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    pats = [\n",
    "        r\"PAYMENT\", r\"TRANSFER\", r\"ACH\", r\"ZELLE\", r\"DIRECTPAY\", r\"CREDIT\",\n",
    "        r\"REFUND\", r\"REIMBURSE\", r\"ADJUSTMENT\", r\"REVERSAL\"\n",
    "    ]\n",
    "    pat = re.compile(\"|\".join(pats))\n",
    "    names = (df.get(\"name\", pd.Series(\"\", index=df.index)).fillna(\"\") + \" \" +\n",
    "             df.get(\"merchant_name\", pd.Series(\"\", index=df.index)).fillna(\"\")).str.upper()\n",
    "    df = df.copy()\n",
    "    df[\"is_non_spend_flow\"] = names.str.contains(pat)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec6e3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Optional YAML enrichment, then finalize columns ---\n",
    "# Build a robust merchant key\n",
    "combined[\"merchant_key\"] = combined[\"merchant_name\"].where(\n",
    "    combined[\"merchant_name\"].astype(str).str.len() > 0,\n",
    "    combined[\"name\"]\n",
    ").map(merchant_key_from)\n",
    "\n",
    "# Load YAML map if exists (use repo_root)\n",
    "PATH_YAML = (repo_root / \"config\" / \"categories.yaml\")\n",
    "ymap = {}\n",
    "if PATH_YAML.exists():\n",
    "    with open(PATH_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "        ymap = yaml.safe_load(f) or {}\n",
    "\n",
    "# Apply mapping + mark non-spend flows\n",
    "enriched = apply_yaml_mapping(combined, ymap)\n",
    "enriched = mark_non_spend_flows(enriched)\n",
    "\n",
    "# ⚠️ FIXED: missing comma in your original list between card_name and display_name_final\n",
    "cols = [\n",
    "    \"date\",\"name\",\"merchant_name\",\"merchant_key\",\"category\",\"amount\",\n",
    "    \"bank_name\",\"card_name\",\n",
    "    \"display_name_final\",\"category_final\",\"subcategory_final\",\"tags_final\",\n",
    "    \"is_non_spend_flow\",\"confidence_final\",\"source_final\"\n",
    "]\n",
    "for c in cols:\n",
    "    if c not in enriched.columns:\n",
    "        enriched[c] = None\n",
    "enriched = enriched[cols].copy()\n",
    "\n",
    "# Keep dates as date (or datetime) for Power BI\n",
    "enriched[\"date\"] = pd.to_datetime(enriched[\"date\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "114c6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest CSV saved → C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\raw\\latest.csv  rows=0\n",
      "\n",
      "Preview (top 10):\n",
      "Empty DataFrame\n",
      "Columns: [date, name, merchant_name, merchant_key, category, amount, bank_name, card_name, display_name_final, category_final, subcategory_final, tags_final, is_non_spend_flow, confidence_final, source_final]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Write latest.csv + preview ---\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "latest_path = OUTPUT_DIR / \"latest.csv\"\n",
    "\n",
    "# Write enriched directly (so Power BI gets the good stuff)\n",
    "enriched.to_csv(latest_path, index=False)\n",
    "\n",
    "# Sanity\n",
    "assert latest_path.exists(), \"latest.csv was not written.\"\n",
    "assert \"bank_name\" in enriched.columns, \"bank_name column missing.\"\n",
    "assert \"card_name\" in enriched.columns, \"card_name column missing.\"\n",
    "\n",
    "print(f\"✅ Latest CSV saved → {latest_path}  rows={len(enriched):,}\")\n",
    "try:\n",
    "    print(\"\\nPreview (top 10):\")\n",
    "    print(enriched.head(10).to_string(index=False))\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
