{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a90b082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Sandbox Item created\n",
      "✔ Wrote: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\plaid_items.json\n",
      "✔ Wrote: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\plaid_cursors.json\n",
      "item_id: V6NJvaWpwgtPd56vnMAbFKxraAkqKbTWDQ5lk\n",
      "access_token (prefix): access-sandbox-023548ca-0020…\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------\n",
    "# Load environment\n",
    "# -------------------------\n",
    "REPO = Path(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\").resolve()\n",
    "load_dotenv(REPO / \".env\")\n",
    "\n",
    "# -------------------------\n",
    "# Plaid imports\n",
    "# -------------------------\n",
    "from plaid.configuration import Configuration, Environment\n",
    "from plaid.api import plaid_api\n",
    "from plaid import ApiClient\n",
    "from plaid.model.products import Products\n",
    "from plaid.model.sandbox_public_token_create_request import SandboxPublicTokenCreateRequest\n",
    "from plaid.model.item_public_token_exchange_request import ItemPublicTokenExchangeRequest\n",
    "\n",
    "# -------------------------\n",
    "# Plaid client (sandbox)\n",
    "# -------------------------\n",
    "configuration = Configuration(\n",
    "    host=Environment.Sandbox,\n",
    "    api_key={\n",
    "        \"clientId\": os.getenv(\"PLAID_CLIENT_ID\"),\n",
    "        \"secret\": os.getenv(\"PLAID_SECRET\"),\n",
    "    },\n",
    ")\n",
    "api_client = ApiClient(configuration)\n",
    "client = plaid_api.PlaidApi(api_client)\n",
    "\n",
    "# -------------------------\n",
    "# File paths\n",
    "# -------------------------\n",
    "CONFIG_DIR = REPO / \"config\"\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ITEMS_JSON = CONFIG_DIR / \"plaid_items.json\"\n",
    "CURSORS_JSON = CONFIG_DIR / \"plaid_cursors.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Create sandbox Item\n",
    "# -------------------------\n",
    "try:\n",
    "    prod_tx = Products(\"transactions\")\n",
    "except Exception:\n",
    "    prod_tx = Products.TRANSACTIONS\n",
    "\n",
    "def create_sandbox_item(institution_id=\"ins_3\"):  # First Platypus Bank\n",
    "    req = SandboxPublicTokenCreateRequest(\n",
    "        institution_id=institution_id,\n",
    "        initial_products=[prod_tx],\n",
    "    )\n",
    "    pub = client.sandbox_public_token_create(req)\n",
    "    exch = client.item_public_token_exchange(\n",
    "        ItemPublicTokenExchangeRequest(public_token=pub[\"public_token\"])\n",
    "    )\n",
    "    return {\"item_id\": exch[\"item_id\"], \"access_token\": exch[\"access_token\"]}\n",
    "\n",
    "item = create_sandbox_item()\n",
    "\n",
    "# -------------------------\n",
    "# Save configs\n",
    "# -------------------------\n",
    "with ITEMS_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"items\": [item]}, f, indent=2)\n",
    "\n",
    "with CURSORS_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"transactions\": {item[\"item_id\"]: \"\"}}, f, indent=2)\n",
    "\n",
    "print(\"✔ Sandbox Item created\")\n",
    "print(\"✔ Wrote:\", ITEMS_JSON)\n",
    "print(\"✔ Wrote:\", CURSORS_JSON)\n",
    "print(\"item_id:\", item[\"item_id\"])\n",
    "print(\"access_token (prefix):\", item[\"access_token\"][:28] + \"…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cefae658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Item V6NJvaWpwgtPd56vnMAbFKxraAkqKbTWDQ5lk ===\n",
      "starting cursor: ''\n",
      "added=0 | modified=0 | removed=0\n",
      "next_cursor (NOT saved): \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from plaid.configuration import Configuration, Environment\n",
    "from plaid.api import plaid_api\n",
    "from plaid import ApiClient\n",
    "from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "from plaid.model.transactions_sync_request_options import TransactionsSyncRequestOptions\n",
    "\n",
    "# --- setup ---\n",
    "REPO = Path(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\").resolve()\n",
    "load_dotenv(REPO / \".env\")\n",
    "cfg_dir = REPO / \"config\"\n",
    "items = json.loads((cfg_dir/\"plaid_items.json\").read_text())[\"items\"]\n",
    "cursors = json.loads((cfg_dir/\"plaid_cursors.json\").read_text()).get(\"transactions\", {})\n",
    "\n",
    "api = plaid_api.PlaidApi(ApiClient(Configuration(\n",
    "    host=Environment.Sandbox,\n",
    "    api_key={\"clientId\": os.getenv(\"PLAID_CLIENT_ID\"), \"secret\": os.getenv(\"PLAID_SECRET\")},\n",
    ")))\n",
    "\n",
    "for it in items:\n",
    "    item_id = it[\"item_id\"]\n",
    "    token   = it[\"access_token\"]\n",
    "    cursor  = cursors.get(item_id, \"\")\n",
    "    print(f\"\\n=== Item {item_id} ===\")\n",
    "    print(\"starting cursor:\", repr(cursor))\n",
    "\n",
    "    added = modified = removed = 0\n",
    "    latest_cursor = cursor\n",
    "    has_more = True\n",
    "\n",
    "    while has_more:\n",
    "        # build request kwargs; only include 'cursor' if non-empty\n",
    "        req_kwargs = dict(access_token=token, count=500,\n",
    "                          options=TransactionsSyncRequestOptions(include_personal_finance_category=False))\n",
    "        if latest_cursor:\n",
    "            req_kwargs[\"cursor\"] = latest_cursor\n",
    "\n",
    "        resp = api.transactions_sync(TransactionsSyncRequest(**req_kwargs))\n",
    "        added    += len(resp[\"added\"])\n",
    "        modified += len(resp[\"modified\"])\n",
    "        removed  += len(resp[\"removed\"])\n",
    "        latest_cursor = resp[\"next_cursor\"]\n",
    "        has_more      = resp[\"has_more\"]\n",
    "\n",
    "    print(f\"added={added} | modified={modified} | removed={removed}\")\n",
    "    print(\"next_cursor (NOT saved):\", latest_cursor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92a8a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V6NJvaWpwgtPd56vnMAbFKxraAkqKbTWDQ5lk] added=0 modified=0 removed=0 | next_cursor set.\n",
      "✔ Silver written: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\interim\\transactions_canonical.csv | rows: 386\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from plaid.configuration import Configuration, Environment\n",
    "from plaid.api import plaid_api\n",
    "from plaid import ApiClient\n",
    "from plaid.model.transactions_sync_request import TransactionsSyncRequest\n",
    "from plaid.model.transactions_sync_request_options import TransactionsSyncRequestOptions\n",
    "\n",
    "# --- paths/env\n",
    "REPO = Path(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\").resolve()\n",
    "DATA_DIR = REPO / \"data\"\n",
    "INTERIM = DATA_DIR / \"interim\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "SILVER = INTERIM / \"transactions_canonical.csv\"\n",
    "\n",
    "CFG = REPO / \"config\"\n",
    "ITEMS_JSON = CFG / \"plaid_items.json\"\n",
    "CURSORS_JSON = CFG / \"plaid_cursors.json\"\n",
    "\n",
    "load_dotenv(REPO / \".env\")\n",
    "\n",
    "# --- plaid client\n",
    "client = plaid_api.PlaidApi(ApiClient(Configuration(\n",
    "    host=Environment.Sandbox,\n",
    "    api_key={\"clientId\": os.getenv(\"PLAID_CLIENT_ID\"), \"secret\": os.getenv(\"PLAID_SECRET\")},\n",
    ")))\n",
    "\n",
    "# --- load config\n",
    "items   = json.loads(ITEMS_JSON.read_text())[\"items\"]\n",
    "cursors = json.loads(CURSORS_JSON.read_text()).get(\"transactions\", {})\n",
    "\n",
    "# --- load current Silver (if exists)\n",
    "if SILVER.exists():\n",
    "    silver_df = pd.read_csv(SILVER, dtype=str)\n",
    "    # cast amount/date later\n",
    "else:\n",
    "    silver_df = pd.DataFrame(columns=[\n",
    "        \"transaction_id\",\"item_id\",\"account_id\",\"date\",\"name\",\"merchant_name\",\n",
    "        \"amount\",\"pending\"\n",
    "    ])\n",
    "\n",
    "# --- helper: upsert/delete by transaction_id\n",
    "silver_df[\"transaction_id\"] = silver_df[\"transaction_id\"].astype(str)\n",
    "\n",
    "def upsert_rows(df, rows):\n",
    "    if not rows:\n",
    "        return df\n",
    "    add = pd.DataFrame(rows)\n",
    "    if add.empty:\n",
    "        return df\n",
    "    add[\"transaction_id\"] = add[\"transaction_id\"].astype(str)\n",
    "    # drop any existing ids then concat\n",
    "    df = df[~df[\"transaction_id\"].isin(add[\"transaction_id\"])]\n",
    "    df = pd.concat([df, add], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def delete_rows(df, removed_ids):\n",
    "    if not removed_ids:\n",
    "        return df\n",
    "    ids = pd.Series([r[\"transaction_id\"] for r in removed_ids], dtype=str)\n",
    "    return df[~df[\"transaction_id\"].isin(ids)]\n",
    "\n",
    "# --- run incremental sync for each item and update Silver in-memory\n",
    "next_cursors = {}\n",
    "for it in items:\n",
    "    item_id = it[\"item_id\"]; token = it[\"access_token\"]\n",
    "    cursor  = cursors.get(item_id, \"\")\n",
    "\n",
    "    latest_cursor = cursor\n",
    "    has_more = True\n",
    "    total_added = total_modified = total_removed = 0\n",
    "\n",
    "    while has_more:\n",
    "        req_kwargs = dict(access_token=token, count=500,\n",
    "                          options=TransactionsSyncRequestOptions(include_personal_finance_category=False))\n",
    "        if latest_cursor:\n",
    "            req_kwargs[\"cursor\"] = latest_cursor\n",
    "        resp = client.transactions_sync(TransactionsSyncRequest(**req_kwargs))\n",
    "\n",
    "        added    = resp[\"added\"]\n",
    "        modified = resp[\"modified\"]\n",
    "        removed  = resp[\"removed\"]\n",
    "\n",
    "        # keep posted only (pending == False)\n",
    "        added    = [t for t in added    if not t.get(\"pending\", False)]\n",
    "        modified = [t for t in modified if not t.get(\"pending\", False)]\n",
    "\n",
    "        # normalize to our Silver schema\n",
    "        def to_row(t):\n",
    "            return {\n",
    "                \"transaction_id\": t.get(\"transaction_id\"),\n",
    "                \"item_id\": item_id,\n",
    "                \"account_id\": t.get(\"account_id\"),\n",
    "                \"date\": t.get(\"date\"),\n",
    "                \"name\": t.get(\"name\"),\n",
    "                \"merchant_name\": t.get(\"merchant_name\"),\n",
    "                \"amount\": str(t.get(\"amount\")),   # keep as string for now; Power BI will parse\n",
    "                \"pending\": str(t.get(\"pending\", False)),\n",
    "            }\n",
    "\n",
    "        add_rows = [to_row(t) for t in added]\n",
    "        mod_rows = [to_row(t) for t in modified]\n",
    "\n",
    "        silver_df = upsert_rows(silver_df, add_rows)\n",
    "        silver_df = upsert_rows(silver_df, mod_rows)\n",
    "        silver_df = delete_rows(silver_df, removed)\n",
    "\n",
    "        latest_cursor = resp[\"next_cursor\"]\n",
    "        has_more = resp[\"has_more\"]\n",
    "\n",
    "        total_added    += len(added)\n",
    "        total_modified += len(modified)\n",
    "        total_removed  += len(removed)\n",
    "\n",
    "    next_cursors[item_id] = latest_cursor\n",
    "    print(f\"[{item_id}] added={total_added} modified={total_modified} removed={total_removed} | next_cursor set.\")\n",
    "\n",
    "# --- write Silver\n",
    "silver_df.to_csv(SILVER, index=False)\n",
    "print(\"✔ Silver written:\", SILVER, \"| rows:\", len(silver_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eafa196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Cursors updated: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\config\\plaid_cursors.json\n"
     ]
    }
   ],
   "source": [
    "# update cursors on disk only after Silver is safely written\n",
    "cursors.update(next_cursors)\n",
    "with CURSORS_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"transactions\": cursors}, f, indent=2)\n",
    "\n",
    "print(\"✔ Cursors updated:\", CURSORS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e67ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Silver rows: 386\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# repo + paths\n",
    "REPO = Path(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\").resolve()\n",
    "load_dotenv(REPO / \".env\")\n",
    "\n",
    "DATA = REPO / \"data\"\n",
    "INTERIM = DATA / \"interim\"\n",
    "PROCESSED = DATA / \"processed\"\n",
    "DOCS = REPO / \"docs\"\n",
    "CONFIG = REPO / \"config\"\n",
    "\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "DOCS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SILVER = INTERIM / \"transactions_canonical.csv\"\n",
    "ENRICHED = PROCESSED / \"transactions_enriched.csv\"\n",
    "UNKNOWN = DOCS / \"review_unknowns.csv\"\n",
    "YAML_PATH = CONFIG / \"categories.yaml\"\n",
    "\n",
    "assert SILVER.exists(), \"Missing data/interim/transactions_canonical.csv\"\n",
    "\n",
    "df = pd.read_csv(SILVER, dtype=str)\n",
    "print(\"Loaded Silver rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e74d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Silver rows: 386\n",
      "✔ Wrote: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\data\\processed\\transactions_enriched.csv | rows: 386\n",
      "✔ Logged unknowns: C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\\docs\\review_unknowns.csv | rows: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account</th>\n",
       "      <th>description</th>\n",
       "      <th>merchant_key</th>\n",
       "      <th>display_name_final</th>\n",
       "      <th>category_final</th>\n",
       "      <th>tags_final</th>\n",
       "      <th>confidence_final</th>\n",
       "      <th>source_final</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_necessity</th>\n",
       "      <th>is_non_spend_flow</th>\n",
       "      <th>month_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-30</td>\n",
       "      <td>ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB</td>\n",
       "      <td>Uber 072515 SF**POOL**</td>\n",
       "      <td>UBER</td>\n",
       "      <td>UBER</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>plaid</td>\n",
       "      <td>plaid</td>\n",
       "      <td>6.33</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB</td>\n",
       "      <td>Uber 063015 SF**POOL**</td>\n",
       "      <td>UBER</td>\n",
       "      <td>UBER</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>plaid</td>\n",
       "      <td>plaid</td>\n",
       "      <td>5.40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB</td>\n",
       "      <td>United Airlines</td>\n",
       "      <td>UNITED AIRLINES</td>\n",
       "      <td>UNITED AIRLINES</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>plaid</td>\n",
       "      <td>plaid</td>\n",
       "      <td>-500.00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>MCDONALD S</td>\n",
       "      <td>MCDONALD S</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>plaid</td>\n",
       "      <td>plaid</td>\n",
       "      <td>12.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>STARBUCKS</td>\n",
       "      <td>STARBUCKS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>plaid</td>\n",
       "      <td>plaid</td>\n",
       "      <td>4.33</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                account             description  \\\n",
       "0  2025-08-30  ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB  Uber 072515 SF**POOL**   \n",
       "1  2025-08-17  ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB  Uber 063015 SF**POOL**   \n",
       "2  2025-08-15  ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB         United Airlines   \n",
       "3  2025-08-14  ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB              McDonald's   \n",
       "4  2025-08-14  ozDabnNg3KhjmPE6KMbjfjgDx7DK1aioAo7yB               Starbucks   \n",
       "\n",
       "      merchant_key display_name_final category_final tags_final  \\\n",
       "0             UBER               UBER                             \n",
       "1             UBER               UBER                             \n",
       "2  UNITED AIRLINES    UNITED AIRLINES                             \n",
       "3       MCDONALD S         MCDONALD S                             \n",
       "4        STARBUCKS          STARBUCKS                             \n",
       "\n",
       "  confidence_final source_final  amount  is_necessity  is_non_spend_flow  \\\n",
       "0            plaid        plaid    6.33         False              False   \n",
       "1            plaid        plaid    5.40         False              False   \n",
       "2            plaid        plaid -500.00         False               True   \n",
       "3            plaid        plaid   12.00         False              False   \n",
       "4            plaid        plaid    4.33         False              False   \n",
       "\n",
       "  month_start  \n",
       "0  2025-08-01  \n",
       "1  2025-08-01  \n",
       "2  2025-08-01  \n",
       "3  2025-08-01  \n",
       "4  2025-08-01  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 — Silver → Gold (single, updated cell)\n",
    "\n",
    "import os, re, json, yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Paths & env ---\n",
    "REPO = Path(r\"C:\\Users\\kosis\\Downloads\\Automation\\spending-dashboard\").resolve()\n",
    "load_dotenv(REPO / \".env\")\n",
    "\n",
    "DATA = REPO / \"data\"\n",
    "INTERIM = DATA / \"interim\"\n",
    "PROCESSED = DATA / \"processed\"\n",
    "DOCS = REPO / \"docs\"\n",
    "CONFIG = REPO / \"config\"\n",
    "\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "DOCS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SILVER    = INTERIM / \"transactions_canonical.csv\"\n",
    "ENRICHED  = PROCESSED / \"transactions_enriched.csv\"\n",
    "UNKNOWN   = DOCS / \"review_unknowns.csv\"\n",
    "YAML_PATH = CONFIG / \"categories.yaml\"\n",
    "\n",
    "assert SILVER.exists(), \"Missing data/interim/transactions_canonical.csv\"\n",
    "\n",
    "# --- Load Silver ---\n",
    "df = pd.read_csv(SILVER, dtype=str)\n",
    "print(\"Loaded Silver rows:\", len(df))\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize_merchant_key(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.upper()\n",
    "    s = re.sub(r\"[^A-Z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# --- Load YAML map (optional) ---\n",
    "yaml_map = {\"merchants\": [], \"patterns\": []}\n",
    "if YAML_PATH.exists():\n",
    "    with open(YAML_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        yraw = yaml.safe_load(f) or {}\n",
    "        yaml_map[\"merchants\"] = yraw.get(\"merchants\", []) or []\n",
    "        yaml_map[\"patterns\"]  = yraw.get(\"patterns\", []) or []\n",
    "else:\n",
    "    print(\"Note: categories.yaml not found; unmatched merchants will be logged to review_unknowns.csv\")\n",
    "\n",
    "# Build exact match map\n",
    "exact_map = {}\n",
    "for m in yaml_map[\"merchants\"]:\n",
    "    key = normalize_merchant_key(m.get(\"match\", \"\"))\n",
    "    if key:\n",
    "        exact_map[key] = {\n",
    "            \"display_name_final\": m.get(\"display_name\") or key,\n",
    "            \"category_final\": m.get(\"category\") or \"\",\n",
    "            \"tags_final\": \",\".join(m.get(\"tags\", []) or []),\n",
    "            \"confidence_final\": \"yaml\",\n",
    "            \"source_final\": \"plaid\",\n",
    "        }\n",
    "\n",
    "# Compile regex rules\n",
    "regex_rules = []\n",
    "for p in yaml_map[\"patterns\"]:\n",
    "    rx = p.get(\"regex\")\n",
    "    if rx:\n",
    "        try:\n",
    "            regex_rules.append((\n",
    "                re.compile(rx, re.I),\n",
    "                {\n",
    "                    \"display_name_final\": p.get(\"display_name\") or \"\",\n",
    "                    \"category_final\": p.get(\"category\") or \"\",\n",
    "                    \"tags_final\": \",\".join(p.get(\"tags\", []) or []),\n",
    "                    \"confidence_final\": \"yaml\",\n",
    "                    \"source_final\": \"plaid\",\n",
    "                }\n",
    "            ))\n",
    "        except re.error as e:\n",
    "            print(\"Skipped bad regex:\", rx, \"|\", e)\n",
    "\n",
    "# --- Core field types ---\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")     # Plaid: outflows +, inflows -\n",
    "df[\"date\"]   = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# --- Derive description & merchant_key (prefer merchant_name fallback to name) ---\n",
    "desc = df[\"merchant_name\"].fillna(\"\").replace(\"\", pd.NA).fillna(df[\"name\"])\n",
    "df[\"description\"] = desc\n",
    "df[\"merchant_key\"] = desc.apply(normalize_merchant_key)\n",
    "\n",
    "# --- Enrichment scaffold ---\n",
    "enriched = pd.DataFrame(index=df.index)\n",
    "enriched[\"display_name_final\"] = \"\"\n",
    "enriched[\"category_final\"]     = \"\"\n",
    "enriched[\"tags_final\"]         = \"\"\n",
    "enriched[\"confidence_final\"]   = \"\"\n",
    "enriched[\"source_final\"]       = \"plaid\"\n",
    "\n",
    "# Exact matches\n",
    "mask_exact = df[\"merchant_key\"].isin(exact_map.keys())\n",
    "if mask_exact.any():\n",
    "    enriched.loc[mask_exact, [\"display_name_final\",\"category_final\",\"tags_final\",\"confidence_final\",\"source_final\"]] = \\\n",
    "        pd.DataFrame([exact_map[k] for k in df.loc[mask_exact, \"merchant_key\"]]).values\n",
    "\n",
    "# Regex matches for remaining blanks\n",
    "to_regex = enriched[\"display_name_final\"].eq(\"\")\n",
    "if to_regex.any() and regex_rules:\n",
    "    candidates = df.loc[to_regex, \"description\"].fillna(\"\")\n",
    "    matched_display, matched_category, matched_tags, matched_conf, matched_source = [], [], [], [], []\n",
    "    for i, text in candidates.items():\n",
    "        applied = False\n",
    "        for rx, mapping in regex_rules:\n",
    "            if rx.search(text or \"\"):\n",
    "                matched_display.append(mapping[\"display_name_final\"] or df.at[i, \"merchant_key\"])\n",
    "                matched_category.append(mapping[\"category_final\"])\n",
    "                matched_tags.append(mapping[\"tags_final\"])\n",
    "                matched_conf.append(\"yaml\")\n",
    "                matched_source.append(\"plaid\")\n",
    "                applied = True\n",
    "                break\n",
    "        if not applied:\n",
    "            matched_display.append(\"\")\n",
    "            matched_category.append(\"\")\n",
    "            matched_tags.append(\"\")\n",
    "            matched_conf.append(\"\")\n",
    "            matched_source.append(\"plaid\")\n",
    "    enriched.loc[to_regex, [\"display_name_final\",\"category_final\",\"tags_final\",\"confidence_final\",\"source_final\"]] = \\\n",
    "        pd.DataFrame({\n",
    "            \"display_name_final\": matched_display,\n",
    "            \"category_final\": matched_category,\n",
    "            \"tags_final\": matched_tags,\n",
    "            \"confidence_final\": matched_conf,\n",
    "            \"source_final\": matched_source\n",
    "        }).values\n",
    "\n",
    "# Fallback: use merchant_key if still blank\n",
    "still_blank = enriched[\"display_name_final\"].eq(\"\")\n",
    "enriched.loc[still_blank, \"display_name_final\"] = df.loc[still_blank, \"merchant_key\"]\n",
    "\n",
    "# --- Non-spend flow detection ---\n",
    "patterns_non_spend = [\n",
    "    r\"\\bPAYMENT\\b\", r\"\\bAUTOPAY\\b\", r\"\\bDIRECT\\s?PAY\\b\", r\"\\bCREDIT\\b\", r\"\\bREFUND\\b\",\n",
    "    r\"\\bTRANSFER\\b\", r\"\\bZELLE\\b\", r\"\\bVENMO\\b\", r\"\\bREVERSAL\\b\"\n",
    "]\n",
    "rx_non_spend = re.compile(\"|\".join(patterns_non_spend), re.I)\n",
    "is_non_spend = df[\"name\"].fillna(\"\").str.contains(rx_non_spend) | df[\"merchant_name\"].fillna(\"\").str.contains(rx_non_spend)\n",
    "# Also treat inflows (amount < 0) as non-spend by default\n",
    "is_non_spend = is_non_spend | (df[\"amount\"] < 0)\n",
    "\n",
    "# --- Month start (as Python date series) ---\n",
    "s  = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "ms = (s.values.astype(\"datetime64[M]\")).astype(\"datetime64[D]\")   # first day of month\n",
    "month_start = pd.Series(pd.DatetimeIndex(ms).date, index=df.index)\n",
    "\n",
    "# --- Assemble Gold ---\n",
    "gold = pd.DataFrame({\n",
    "    \"date\": df[\"date\"],\n",
    "    \"account\": df[\"account_id\"].astype(str),\n",
    "    \"description\": df[\"name\"].astype(str),\n",
    "    \"merchant_key\": df[\"merchant_key\"].astype(str),\n",
    "    \"display_name_final\": enriched[\"display_name_final\"].astype(str),\n",
    "    \"category_final\": enriched[\"category_final\"].astype(str),\n",
    "    \"tags_final\": enriched[\"tags_final\"].astype(str),\n",
    "    \"confidence_final\": enriched[\"confidence_final\"].replace(\"\", \"plaid\"),\n",
    "    \"source_final\": enriched[\"source_final\"],\n",
    "    \"amount\": df[\"amount\"],\n",
    "    \"is_necessity\": False,\n",
    "    \"is_non_spend_flow\": is_non_spend.astype(bool),\n",
    "    \"month_start\": month_start,\n",
    "})\n",
    "\n",
    "# --- Write outputs ---\n",
    "gold.to_csv(ENRICHED, index=False)\n",
    "\n",
    "unknown_mask = (gold[\"category_final\"] == \"\") & (~gold[\"is_non_spend_flow\"])\n",
    "unknowns = gold.loc[unknown_mask, [\"merchant_key\",\"description\"]].drop_duplicates().sort_values(\"merchant_key\")\n",
    "unknowns.to_csv(UNKNOWN, index=False)\n",
    "\n",
    "print(\"✔ Wrote:\", ENRICHED,  \"| rows:\", len(gold))\n",
    "print(\"✔ Logged unknowns:\", UNKNOWN, \"| rows:\", len(unknowns))\n",
    "\n",
    "# Preview\n",
    "gold.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Tools)",
   "language": "python",
   "name": "ai-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
